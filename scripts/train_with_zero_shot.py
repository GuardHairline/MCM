#!/usr/bin/env python3
"""
æ”¯æŒ0æ ·æœ¬æ£€æµ‹çš„æŒç»­å­¦ä¹ è®­ç»ƒè„šæœ¬

ä½¿ç”¨ä»»åŠ¡é…ç½®æ–‡ä»¶è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥åœ¨è®­ç»ƒç¬¬iä¸ªä»»åŠ¡æ—¶å¯¹ç¬¬i+1ã€i+2ç­‰ä»»åŠ¡è¿›è¡Œ0æ ·æœ¬æ£€æµ‹ã€‚
"""

import json
import argparse
import sys
import torch
import torch.multiprocessing as mp
import glob
import os
from pathlib import Path
from typing import Dict, List, Any

# è®¾ç½®æ–‡ä»¶ç³»ç»Ÿå…±äº«ç­–ç•¥ï¼Œè§£å†³"Too many open files"é—®é¢˜
mp.set_sharing_strategy('file_system')

# å¯¼å…¥è®­ç»ƒæ¨¡å—
from modules.train_refactored import train
from modules.parser import parse_train_args
from utils.logger import setup_logger


def load_task_config(config_file: str) -> Dict[str, Any]:
    """åŠ è½½ä»»åŠ¡é…ç½®æ–‡ä»¶"""
    with open(config_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def cleanup_experiment_files(config: Dict[str, Any], global_params: Dict[str, Any]):
    """
    æ¸…ç†æœ¬æ¬¡å®éªŒç”Ÿæˆçš„.ptæ–‡ä»¶
    
    åªåˆ é™¤å½“å‰å®éªŒçš„æ–‡ä»¶ï¼Œä¸å½±å“å…¶ä»–å®éªŒçš„æ–‡ä»¶
    æ ¹æ®é…ç½®æ–‡ä»¶åæ¥è¯†åˆ«ç›¸å…³æ–‡ä»¶
    
    Args:
        config: é…ç½®å­—å…¸
        global_params: å…¨å±€å‚æ•°å­—å…¸
    """
    try:
        print("="*60)
        print("ğŸ§¹ å¼€å§‹æ¸…ç†å®éªŒæ–‡ä»¶...")
        print("="*60)
        
        # ä»global_paramsä¸­æå–æ–‡ä»¶åæ¨¡å¼
        # ä¾‹å¦‚: checkpoints/twitter2015_none_t2m_hp1.pt
        model_path = global_params.get("output_model_path", "")
        if not model_path:
            print("âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹è·¯å¾„ï¼Œè·³è¿‡æ¸…ç†")
            return
        
        # æå–base_name: twitter2015_none_t2m_hp1
        model_file = Path(model_path)
        base_name = model_file.stem  # ä¸å«.ptæ‰©å±•å
        checkpoint_dir = model_file.parent
        
        print(f"ğŸ“ è¯†åˆ«æ¨¡å¼: {base_name}")
        print(f"ğŸ“ æ£€æŸ¥ç›®å½•: {checkpoint_dir}")
        
        # éœ€è¦åˆ é™¤çš„æ–‡ä»¶æ¨¡å¼
        patterns_to_delete = [
            f"{base_name}.pt",                      # ä¸»æ¨¡å‹æ–‡ä»¶
            f"{base_name}_*.pt",                    # å…¶ä»–ç›¸å…³æ¨¡å‹æ–‡ä»¶
            f"model_{base_name}*.pt",               # å¸¦modelå‰ç¼€çš„æ–‡ä»¶
            f"*{base_name}_task_heads.pt",          # ä»»åŠ¡å¤´æ–‡ä»¶
            f"label_embedding_{base_name}.pt",      # æ ‡ç­¾åµŒå…¥æ–‡ä»¶
        ]
        
        deleted_count = 0
        kept_count = 0
        
        # åœ¨checkpoint_dirä¸­æŸ¥æ‰¾å¹¶åˆ é™¤åŒ¹é…çš„æ–‡ä»¶
        for pattern in patterns_to_delete:
            full_pattern = os.path.join(checkpoint_dir, pattern)
            matching_files = glob.glob(full_pattern)
            
            for file_path in matching_files:
                file_name = os.path.basename(file_path)
                # ç¡®ä¿base_nameåœ¨æ–‡ä»¶åä¸­ï¼ˆé¢å¤–å®‰å…¨æ£€æŸ¥ï¼‰
                if base_name in file_name:
                    try:
                        os.remove(file_path)
                        print(f"  âœ“ åˆ é™¤: {file_name}")
                        deleted_count += 1
                    except Exception as e:
                        print(f"  âœ— åˆ é™¤å¤±è´¥: {file_name} ({e})")
                else:
                    kept_count += 1
        
        # æ¸…ç†EWCå‚æ•°
        ewc_dir = global_params.get("ewc_dir", "")
        if ewc_dir and os.path.exists(ewc_dir):
            ewc_pattern = os.path.join(ewc_dir, f"*{base_name}*.pt")
            for file_path in glob.glob(ewc_pattern):
                if base_name in os.path.basename(file_path):
                    try:
                        os.remove(file_path)
                        print(f"  âœ“ åˆ é™¤EWC: {os.path.basename(file_path)}")
                        deleted_count += 1
                    except Exception as e:
                        print(f"  âœ— åˆ é™¤EWCå¤±è´¥: {os.path.basename(file_path)} ({e})")
        
        # æ¸…ç†GEMè®°å¿†
        gem_dir = global_params.get("gem_mem_dir", "")
        if gem_dir and os.path.exists(gem_dir):
            gem_pattern = os.path.join(gem_dir, f"*{base_name}*.pt")
            for file_path in glob.glob(gem_pattern):
                if base_name in os.path.basename(file_path):
                    try:
                        os.remove(file_path)
                        print(f"  âœ“ åˆ é™¤GEM: {os.path.basename(file_path)}")
                        deleted_count += 1
                    except Exception as e:
                        print(f"  âœ— åˆ é™¤GEMå¤±è´¥: {os.path.basename(file_path)} ({e})")
        
        print(f"\nâœ… æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"âŒ æ¸…ç†è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print("="*60 + "\n")


def run_single_task(task_config: Dict[str, Any], global_params: Dict[str, Any], 
                   task_idx: int, total_tasks: int, pretrained_model_path: str = "", all_tasks: List[Dict[str, Any]] = []) -> str:
    """è¿è¡Œå•ä¸ªä»»åŠ¡"""
    
    print(f"Running task {task_idx + 1}/{total_tasks}: {task_config['task_name']} ({task_config['session_name']})")
    
    # åˆ›å»ºå‚æ•°å¯¹è±¡
    args = argparse.Namespace()
    
    # åŸºæœ¬å‚æ•°
    args.task_name = task_config["task_name"]
    args.session_name = task_config["session_name"]
    args.task_config_file = global_params.get("task_config_file", "")
    args.train_info_json = global_params["train_info_json"]
    args.output_model_path = global_params["output_model_path"]
    args.pretrained_model_path = pretrained_model_path
    
    # æ•°æ®å‚æ•°
    args.data_dir = global_params.get("data_dir", "data")
    args.dataset_name = global_params.get("dataset_name", "twitter2015")
    args.train_text_file = task_config["train_text_file"]
    args.test_text_file = task_config["test_text_file"]
    args.dev_text_file = task_config["dev_text_file"]
    args.image_dir = task_config["image_dir"]
    
    # æ¨¡å‹å‚æ•°
    args.text_model_name = task_config["text_model_name"]
    args.image_model_name = task_config["image_model_name"]
    args.fusion_strategy = task_config["fusion_strategy"]
    args.num_heads = task_config["num_heads"]
    args.mode = task_config["mode"]
    args.hidden_dim = task_config["hidden_dim"]
    args.dropout_prob = task_config["dropout_prob"]
    args.num_labels = task_config["num_labels"]
    
    # è®­ç»ƒå‚æ•°
    args.epochs = task_config["epochs"]
    args.batch_size = task_config["batch_size"]
    args.lr = task_config["lr"]
    args.weight_decay = task_config["weight_decay"]
    args.step_size = task_config["step_size"]
    args.gamma = task_config["gamma"]
    args.patience = task_config["patience"]
    args.num_workers = global_params.get("num_workers", 4)
    
    # æŒç»­å­¦ä¹ ç­–ç•¥å‚æ•°
    for key in ["ewc", "ewc_lambda", "replay", "memory_percentage", "replay_ratio", 
                "replay_frequency", "lwf", "lwf_T", "lwf_alpha", "lwf_decay",
                "si", "si_epsilon", "si_decay", "mas", "mas_eps", "gem", "gem_mem",
                "pnn", "tam_cl", "moe_adapters", "moe_num_experts", "moe_top_k",
                "ddas", "ddas_threshold", "clap4clip", "mymethod", "deqa"]:
        if key in task_config:
            setattr(args, key, task_config[key])
        else:
            setattr(args, key, 0)  # é»˜è®¤å…³é—­
    
    # DEQAç‰¹å®šå‚æ•°
    if task_config.get("deqa", 0):
        args.deqa_use_description = task_config.get("deqa_use_description", True)
        args.deqa_use_clip = task_config.get("deqa_use_clip", True)
        args.deqa_ensemble_method = task_config.get("deqa_ensemble_method", "weighted")
        args.deqa_freeze_old_experts = task_config.get("deqa_freeze_old_experts", True)
        args.deqa_distill_weight = task_config.get("deqa_distill_weight", 0.5)
        args.description_file = task_config.get("description_file", None)
    
    # æ ‡ç­¾åµŒå…¥å‚æ•°
    args.use_label_embedding = task_config.get("use_label_embedding", False)
    args.label_emb_dim = task_config.get("label_emb_dim", 128)
    args.use_similarity_reg = task_config.get("use_similarity_reg", True)
    args.similarity_weight = task_config.get("similarity_weight", 0.1)
    args.label_embedding_path = task_config.get("label_embedding_path", None)
    
    # æ¨¡å‹å¤´éƒ¨å‚æ•°
    args.triaffine = task_config.get("triaffine", 1)
    args.span_hidden = task_config.get("span_hidden", 256)
    
    # å›¾å¹³æ»‘å‚æ•°
    args.graph_smooth = task_config.get("graph_smooth", 1)
    args.graph_tau = task_config.get("graph_tau", 0.5)
    
    # ç›®å½•å‚æ•°
    args.ewc_dir = global_params["ewc_dir"]
    args.gem_mem_dir = global_params["gem_mem_dir"]
    
    # æ—¥å¿—å‚æ•°
    args.log_file = None
    
    print(f"Task parameters:")
    print(f"  Task: {args.task_name}")
    print(f"  Session: {args.session_name}")
    print(f"  Epochs: {args.epochs}")
    print(f"  Batch size: {args.batch_size}")
    print(f"  Learning rate: {args.lr}")
    print(f"  Strategy: {task_config.get('strategy', 'none')}")
    if args.use_label_embedding:
        print(f"  Label embedding: enabled")
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(args=args)
    
    try:
        # ç›´æ¥è°ƒç”¨è®­ç»ƒå‡½æ•°
        best_metrics = train(args, logger, all_tasks=all_tasks)
        print(f"Task {task_idx + 1} completed successfully")
        print(f"Best metrics: {best_metrics}")
        return global_params["output_model_path"]
    except Exception as e:
        print(f"Task {task_idx + 1} failed with error: {e}")
        raise


def main():
    parser = argparse.ArgumentParser(description="æŒç»­å­¦ä¹ è®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒ0æ ·æœ¬æ£€æµ‹ï¼‰")
    parser.add_argument("--config", type=str, required=True, default="scripts/task_config.json",
                       help="ä»»åŠ¡é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--start_task", type=int, default=0,
                       help="å¼€å§‹ä»»åŠ¡ç´¢å¼•ï¼ˆ0-basedï¼‰")
    parser.add_argument("--end_task", type=int, default=8,
                       help="ç»“æŸä»»åŠ¡ç´¢å¼•ï¼ˆ0-basedï¼Œä¸åŒ…å«ï¼‰")
    
    args = parser.parse_args()
    
    # åŠ è½½ä»»åŠ¡é…ç½®
    print(f"Loading task configuration from: {args.config}")
    config = load_task_config(args.config)
    
    tasks = config["tasks"]
    global_params = config["global_params"]
    global_params["task_config_file"] = args.config  # æ·»åŠ é…ç½®æ–‡ä»¶è·¯å¾„
    
    # ç¡®å®šä»»åŠ¡èŒƒå›´
    start_idx = args.start_task
    end_idx = args.end_task if args.end_task is not None else len(tasks)
    
    print(f"Total tasks: {len(tasks)}")
    print(f"Running tasks: {start_idx + 1} to {end_idx}")
    print(f"Environment: {config['env']}")
    print(f"Strategy: {config['strategy']}")
    print(f"Mode: {config['mode_suffix']}")
    print(f"Dataset: {config['dataset']}")
    print(f"Label embedding: {'Yes' if config.get('use_label_embedding', False) else 'No'}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    Path(global_params["train_info_json"]).parent.mkdir(parents=True, exist_ok=True)
    Path(global_params["output_model_path"]).parent.mkdir(parents=True, exist_ok=True)
    
    # æŒ‰é¡ºåºæ‰§è¡Œä»»åŠ¡
    pretrained_model_path = ""
    for i in range(start_idx, end_idx):
        task_config = tasks[i]
        
        # è¿è¡Œä»»åŠ¡
        model_path = run_single_task(task_config, global_params, i, len(tasks), pretrained_model_path, all_tasks=tasks)
        
        # æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        pretrained_model_path = model_path
        
        print(f"Completed task {i + 1}/{len(tasks)}: {task_config['task_name']}")
        print(f"Model saved to: {model_path}")
        print("-" * 50)
    
    print("All tasks completed successfully!")
    print(f"Final model: {pretrained_model_path}")
    print(f"Training info: {global_params['train_info_json']}")
    
    # ========== è‡ªåŠ¨ç»˜åˆ¶accçƒ­åŠ›å›¾ ==========
    from utils.plot import plot_acc_matrix_from_config
    plot_acc_matrix_from_config(
        config_file_path=args.config,
        train_info_file_path=global_params['train_info_json'],
        save_dir="checkpoints/acc_matrix"
    )
    
    # ========== æ¸…ç†å®éªŒæ–‡ä»¶ ==========
    cleanup_experiment_files(config, global_params)


if __name__ == "__main__":
    main() 