#!/usr/bin/env python3
"""
ç”ŸæˆCRFå’ŒSpan Loss Ablation Studyé…ç½®æ–‡ä»¶ - Kaggleå¤šè´¦å·ç‰ˆæœ¬

ç‰¹æ€§ï¼š
1. ä¸º6ä¸ªKaggleè´¦å·åˆ†é…ä¸åŒçš„å®éªŒç»„åˆ
2. æ¯ä¸ªè´¦å·2ä¸ªé…ç½®ï¼Œæ¯ä¸ªé…ç½®åŒ…å«text_only + multimodalä¸¤ä¸ªsession
3. è‡ªåŠ¨ç”Ÿæˆè´¦å·ä¸“å±è¿è¡Œè„šæœ¬
4. æ”¯æŒç»“æœæ±‡æ€»å’Œåˆ†æ

å®éªŒè®¾è®¡ï¼š
- 3ä¸ªä»»åŠ¡: MATE, MNER, MABSA
- 4ç§é…ç½®: baseline, crf_only, span_only, crf_and_span
- æ¯ä¸ªé…ç½®: text_only session â†’ multimodal sessionï¼ˆæŒç»­å­¦ä¹ åºåˆ—ï¼‰
- æ€»è®¡12ä¸ªé…ç½®ï¼Œ24ä¸ªè®­ç»ƒsession

è´¦å·åˆ†é…ç­–ç•¥ï¼š
- Account 1: MATE (baseline + crf_and_span)
- Account 2: MATE (crf_only + span_only)  
- Account 3: MNER (baseline + crf_and_span)
- Account 4: MNER (crf_only + span_only)
- Account 5: MABSA (baseline + crf_and_span)
- Account 6: MABSA (crf_only + span_only)

æ—¶é—´ä¼°ç®—ï¼ˆæ¯ä¸ªé…ç½®åŒ…å«2ä¸ªsessionï¼‰ï¼š
- Twitter2015: ~3-3.7å°æ—¶/é…ç½®ï¼ˆtext_only ~1.5h + multimodal ~1.5-2hï¼‰
- 2ä¸ªé…ç½® = 6-7.4å°æ—¶
- ç•™ä½™é‡ï¼š8å°æ—¶å†…å®Œæˆï¼ˆè¿œä½äº12å°æ—¶é™åˆ¶ï¼‰
"""

import json
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from scripts.generate_crf_test_configs import CRFTestConfigGenerator


class KaggleAblationStudyGenerator:
    """Kaggleå¤šè´¦å·Ablation Studyé…ç½®ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.base_generator = CRFTestConfigGenerator()
        
        # 3ä¸ªè´¦å·çš„å®éªŒåˆ†é…ï¼ˆç®€åŒ–ç‰ˆï¼šåªå¯¹æ¯” Baseline vs CRF Onlyï¼‰
        # ç§»é™¤ Span Lossï¼ˆæœ‰ä¸¥é‡é—®é¢˜ï¼‰
        self.account_assignments = {
            "account_1": {
                "name": "Account 1 - MATE (Baseline vs CRF)",
                "task": "mate",
                "ablations": ["baseline", "crf_only"],
                "description": "MATEä»»åŠ¡ï¼šå¯¹æ¯”baselineå’ŒCRFæ•ˆæœ"
            },
            "account_2": {
                "name": "Account 2 - MNER (Baseline vs CRF)",
                "task": "mner",
                "ablations": ["baseline", "crf_only"],
                "description": "MNERä»»åŠ¡ï¼šå¯¹æ¯”baselineå’ŒCRFæ•ˆæœ"
            },
            "account_3": {
                "name": "Account 3 - MABSA (Baseline vs CRF)",
                "task": "mabsa",
                "ablations": ["baseline", "crf_only"],
                "description": "MABSAä»»åŠ¡ï¼šå¯¹æ¯”baselineå’ŒCRFæ•ˆæœ"
            }
        }
        
        # æ—¶é—´ä¼°ç®—ï¼ˆåˆ†é’Ÿï¼‰- æ¯ä¸ªé…ç½®åŒ…å« text_only + multimodal ä¸¤ä¸ªsession
        self.time_estimates = {
            "mate": 180,     # 3å°æ—¶/é…ç½® (text_only ~1.5h + multimodal ~1.5h)
            "mner": 200,     # 3.3å°æ—¶/é…ç½® (text_only ~1.7h + multimodal ~1.6h)
            "mabsa": 220     # 3.7å°æ—¶/é…ç½® (text_only ~1.8h + multimodal ~1.9h)
        }
    
    def generate_account_configs(self,
                                 account_id: str,
                                 env: str = "server",
                                 dataset: str = "twitter2015",
                                 output_dir: str = "scripts/configs/kaggle_ablation"):
        """ä¸ºæŒ‡å®šè´¦å·ç”Ÿæˆé…ç½®"""
        
        if account_id not in self.account_assignments:
            raise ValueError(f"Unknown account: {account_id}. Available: {list(self.account_assignments.keys())}")
        
        assignment = self.account_assignments[account_id]
        task = assignment["task"]
        ablations = assignment["ablations"]
        
        output_path = Path(output_dir) / account_id
        output_path.mkdir(parents=True, exist_ok=True)
        
        configs = []
        
        print(f"\n{'='*80}")
        print(f"{assignment['name']}")
        print(f"{'='*80}")
        print(f"ä»»åŠ¡: {task.upper()}")
        print(f"é…ç½®: {', '.join(ablations)}")
        print(f"æè¿°: {assignment['description']}")
        print(f"é¢„è®¡æ—¶é—´: {len(ablations) * self.time_estimates[task]} åˆ†é’Ÿ")
        print(f"{'='*80}\n")
        
        for ablation_type in ablations:
            config_name = f"kaggle_{ablation_type}_{dataset}_{task}.json"
            config_file = output_path / config_name
            
            # ç”Ÿæˆé…ç½® - åŒ…å« text_only å’Œ multimodal ä¸¤ä¸ªsession
            # ä½¿ç”¨ generate_task_sequence_config ç›´æ¥ç”ŸæˆæŒç»­å­¦ä¹ åºåˆ—
            config = self.base_generator.base_generator.generate_task_sequence_config(
                env=env,
                dataset=dataset,
                task_sequence=[task, task],  # åŒä¸€ä¸ªä»»åŠ¡ä¸¤æ¬¡
                mode_sequence=["text_only", "multimodal"],  # å…ˆtext_onlyï¼Œå†multimodal
                strategy="none",            # æ— æŒç»­å­¦ä¹ ç­–ç•¥ï¼ˆåªæ˜¯é¡ºåºè®­ç»ƒï¼‰
                use_label_embedding=False,
                seq_suffix=f"_{ablation_type}",
                **self.base_generator.recommended_hyperparams
            )
            
            # æ·»åŠ CRFå’ŒSpan Lossé…ç½®åˆ°æ¯ä¸ªsession
            ablation_config = self.base_generator.ablation_configs[ablation_type]
            for task_config in config["tasks"]:
                # åªå¯¹åºåˆ—ä»»åŠ¡å¯ç”¨CRF
                if task_config["task_name"] in ["mate", "mner", "mabsa"]:
                    task_config.update({
                        "use_crf": ablation_config["use_crf"],
                        "use_span_loss": ablation_config["use_span_loss"],
                        "boundary_weight": ablation_config["boundary_weight"],
                        "span_f1_weight": ablation_config["span_f1_weight"],
                        "transition_weight": ablation_config["transition_weight"]
                    })
                    
                    # ç¡®ä¿num_labelsæ­£ç¡®
                    if task_config["task_name"] == "mate":
                        task_config["num_labels"] = 3
                    elif task_config["task_name"] == "mner":
                        task_config["num_labels"] = 9
                    elif task_config["task_name"] == "mabsa":
                        task_config["num_labels"] = 7
            
            # æ·»åŠ æ¶ˆèå®éªŒå…ƒä¿¡æ¯
            config["ablation_info"] = {
                "purpose": "Ablation study for CRF and Span Loss",
                "ablation_type": ablation_type,
                "configuration": ablation_config["description"],
                "mode_sequence": ["text_only", "multimodal"],
                "expected_improvement": self.base_generator._get_expected_improvement(ablation_type)
            }
            
            # Kaggleç‰¹æ®Šé…ç½®
            config["kaggle_mode"] = True
            config["kaggle_output_path"] = "/kaggle/working"
            
            # ä¿å­˜é…ç½®
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            configs.append({
                "file": config_name,
                "path": config_file.as_posix(),
                "task": task,
                "ablation_type": ablation_type,
                "dataset": dataset
            })
            
            print(f"  âœ“ Generated: {config_name}")
        
        # ç”Ÿæˆè´¦å·ç´¢å¼•æ–‡ä»¶
        index_file = output_path / f"{account_id}_index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump({
                "account_id": account_id,
                "account_name": assignment["name"],
                "task": task,
                "ablations": ablations,
                "description": assignment["description"],
                "estimated_time_minutes": len(ablations) * self.time_estimates[task],
                "total_configs": len(configs),
                "configs": configs
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\n  âœ“ Index file: {index_file}\n")
        
        return configs, output_path
    
    def generate_all_accounts(self,
                             env: str = "server",
                             dataset: str = "twitter2015",
                             output_dir: str = "scripts/configs/kaggle_ablation"):
        """ä¸ºæ‰€æœ‰6ä¸ªè´¦å·ç”Ÿæˆé…ç½®"""
        
        all_configs = {}
        
        for account_id in self.account_assignments.keys():
            configs, account_dir = self.generate_account_configs(
                account_id=account_id,
                env=env,
                dataset=dataset,
                output_dir=output_dir
            )
            all_configs[account_id] = {
                "configs": configs,
                "directory": str(account_dir)
            }
            
            # ä¸ºæ¯ä¸ªè´¦å·ç”Ÿæˆè¿è¡Œè„šæœ¬
            self._generate_account_runner(account_id, configs, account_dir)
        
        # ç”Ÿæˆæ€»ç´¢å¼•æ–‡ä»¶
        master_index = Path(output_dir) / "master_index.json"
        with open(master_index, 'w', encoding='utf-8') as f:
            json.dump({
                "description": "6è´¦å·Ablation Studyæ€»ç´¢å¼•",
                "total_accounts": len(self.account_assignments),
                "total_configs": sum(len(v["configs"]) for v in all_configs.values()),
                "accounts": {
                    acc_id: {
                        "name": self.account_assignments[acc_id]["name"],
                        "task": self.account_assignments[acc_id]["task"],
                        "ablations": self.account_assignments[acc_id]["ablations"],
                        "configs_count": len(all_configs[acc_id]["configs"]),
                        "directory": all_configs[acc_id]["directory"]
                    }
                    for acc_id in self.account_assignments.keys()
                }
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\n{'='*80}")
        print(f"âœ… æ‰€æœ‰è´¦å·é…ç½®ç”Ÿæˆå®Œæˆ")
        print(f"{'='*80}")
        print(f"æ€»è´¦å·æ•°: {len(self.account_assignments)}")
        print(f"æ€»é…ç½®æ•°: {sum(len(v['configs']) for v in all_configs.values())}")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print(f"æ€»ç´¢å¼•æ–‡ä»¶: {master_index}")
        print(f"{'='*80}\n")
        
        # ç”Ÿæˆéƒ¨ç½²æŒ‡å—
        self._generate_deployment_guide(Path(output_dir))
        
        # ç”Ÿæˆç»“æœåˆ†æè„šæœ¬
        self._generate_analysis_script(Path(output_dir), all_configs)
        
        return all_configs
    
    def _generate_account_runner(self, account_id: str, configs: list, output_dir: Path):
        """ä¸ºå•ä¸ªè´¦å·ç”ŸæˆKaggleè¿è¡Œè„šæœ¬"""
        
        assignment = self.account_assignments[account_id]
        runner_path = output_dir / f"run_{account_id}.py"
        
        with open(runner_path, 'w', encoding='utf-8', newline='\n') as f:
            f.write(f'''#!/usr/bin/env python3
"""
Kaggleè¿è¡Œè„šæœ¬ - {assignment["name"]}

æ­¤è„šæœ¬åœ¨Kaggle Notebookä¸­è¿è¡Œ
ä»»åŠ¡: {assignment["task"].upper()}
é…ç½®: {", ".join(assignment["ablations"])}
é¢„è®¡æ—¶é—´: {len(configs) * self.time_estimates[assignment["task"]]} åˆ†é’Ÿ

ä½¿ç”¨è¯´æ˜:
1. åœ¨Kaggle Notebookä¸­åˆ›å»ºæ–°çš„Code
2. è®¾ç½®åŠ é€Ÿå™¨ä¸º GPU P100
3. æ·»åŠ æ•°æ®é›†: mcm-project (åŒ…å«ä»£ç å’Œæ•°æ®)
4. å¤åˆ¶æ­¤è„šæœ¬å†…å®¹åˆ°Notebook
5. ç‚¹å‡» Run All
"""

import os
import sys
import json
import subprocess
import time
from pathlib import Path
import shutil

# ============================================================================
# Kaggleç¯å¢ƒé…ç½®
# ============================================================================

KAGGLE_INPUT = "/kaggle/input"
KAGGLE_WORKING = "/kaggle/working"
PROJECT_DATASET = "mcm-project"  # ä½ çš„Kaggleæ•°æ®é›†åç§°

print("="*80)
print("{assignment['name']}")
print("="*80)
print(f"ä»»åŠ¡: {assignment['task'].upper()}")
print(f"é…ç½®æ•°: {len(configs)}")
print(f"é¢„è®¡æ—¶é—´: {len(configs) * self.time_estimates[assignment['task']]} åˆ†é’Ÿ")
print("="*80 + "\\n")

# ============================================================================
# Step 1: é¡¹ç›®è®¾ç½®
# ============================================================================

print("\\n" + "="*80)
print("Step 1: è®¾ç½®é¡¹ç›®")
print("="*80)

# æ£€æŸ¥æ•°æ®é›†
dataset_path = Path(KAGGLE_INPUT) / PROJECT_DATASET
if not dataset_path.exists():
    print(f"âŒ æ•°æ®é›†æœªæ‰¾åˆ°: {{dataset_path}}")
    print("è¯·åœ¨Notebookè®¾ç½®ä¸­æ·»åŠ  '{{PROJECT_DATASET}}' æ•°æ®é›†")
    sys.exit(1)

print(f"âœ“ æ•°æ®é›†è·¯å¾„: {{dataset_path}}")

# å¤åˆ¶é¡¹ç›®åˆ°å·¥ä½œç›®å½•
project_dir = Path(KAGGLE_WORKING) / "MCM"
if project_dir.exists():
    print(f"æ¸…ç†æ—§é¡¹ç›®: {{project_dir}}")
    shutil.rmtree(project_dir)

print(f"å¤åˆ¶é¡¹ç›®åˆ°: {{project_dir}}")
shutil.copytree(dataset_path, project_dir)

# åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•
os.chdir(project_dir)
sys.path.insert(0, str(project_dir))

print(f"âœ“ å½“å‰å·¥ä½œç›®å½•: {{os.getcwd()}}")
print(f"âœ“ Pythonè·¯å¾„å·²æ›´æ–°")

# ============================================================================
# Step 2: æ£€æŸ¥ä¾èµ–
# ============================================================================

print("\\n" + "="*80)
print("Step 2: æ£€æŸ¥ä¾èµ–")
print("="*80)

try:
    import torch
    print(f"âœ“ PyTorch: {{torch.__version__}}")
    print(f"âœ“ CUDA available: {{torch.cuda.is_available()}}")
    if torch.cuda.is_available():
        print(f"âœ“ GPU: {{torch.cuda.get_device_name(0)}}")
except ImportError:
    print("âŒ PyTorchæœªå®‰è£…")
    sys.exit(1)

# å®‰è£…pytorch-crf (å¦‚æœéœ€è¦)
try:
    from torchcrf import CRF
    print("âœ“ torchcrfå·²å®‰è£…")
except ImportError:
    print("å®‰è£…torchcrf...")
    subprocess.run([sys.executable, "-m", "pip", "install", "pytorch-crf", "-q"], check=True)
    print("âœ“ torchcrfå®‰è£…å®Œæˆ")

# ============================================================================
# Step 3: è¿è¡Œå®éªŒ
# ============================================================================

print("\\n" + "="*80)
print("Step 3: è¿è¡Œå®éªŒ")
print("="*80)

# é…ç½®æ–‡ä»¶åˆ—è¡¨
configs = {json.dumps([{"file": c["file"], "ablation": c["ablation_type"]} for c in configs], indent=2)}

results = []
start_time = time.time()

for i, config_info in enumerate(configs, 1):
    config_file = Path("scripts/configs/kaggle_ablation/{account_id}") / config_info["file"]
    ablation_type = config_info["ablation"]
    
    print(f"\\n{{'-'*80}}")
    print(f"å®éªŒ {{i}}/{{len(configs)}}: {{ablation_type}}")
    print(f"é…ç½®: {{config_file}}")
    print(f"{{'-'*80}}")
    
    exp_start = time.time()
    
    try:
        # è¿è¡Œè®­ç»ƒ
        cmd = [
            sys.executable, "-m", "scripts.train_with_zero_shot",
            "--config", str(config_file)
        ]
        
        result = subprocess.run(
            cmd,
            cwd=project_dir,
            env={{**os.environ, "PYTHONPATH": str(project_dir)}},
            capture_output=True,
            text=True
        )
        
        exp_time = time.time() - exp_start
        
        if result.returncode == 0:
            print(f"âœ… å®éªŒ {{i}} å®Œæˆ ({{exp_time/60:.1f}} åˆ†é’Ÿ)")
            status = "success"
        else:
            print(f"âŒ å®éªŒ {{i}} å¤±è´¥")
            print(f"é”™è¯¯: {{result.stderr[-500:]}}")
            status = "failed"
        
        results.append({{
            "experiment_id": i,
            "ablation_type": ablation_type,
            "status": status,
            "time_minutes": exp_time / 60,
            "config_file": str(config_file)
        }})
        
    except Exception as e:
        print(f"âŒ å®éªŒ {{i}} å¼‚å¸¸: {{e}}")
        results.append({{
            "experiment_id": i,
            "ablation_type": ablation_type,
            "status": "error",
            "error": str(e)
        }})
    
    # ä¿å­˜ä¸­é—´ç»“æœ
    with open(Path(KAGGLE_WORKING) / "{account_id}_results.json", 'w') as f:
        json.dump(results, f, indent=2)

# ============================================================================
# Step 4: åˆ†æç»“æœ
# ============================================================================

print("\\n" + "="*80)
print("Step 4: åˆ†æç»“æœ")
print("="*80)

# è¯»å–æ‰€æœ‰ç”Ÿæˆçš„metrics JSONæ–‡ä»¶
metrics_dir = Path(KAGGLE_WORKING) / "checkpoints"
all_metrics = []

if metrics_dir.exists():
    for json_file in metrics_dir.rglob("*_metrics.json"):
        try:
            with open(json_file, 'r') as f:
                metrics = json.load(f)
                # ä»æ–‡ä»¶åæ¨æ–­ablationç±»å‹
                file_name = json_file.stem
                if "baseline" in str(json_file):
                    ablation = "baseline"
                elif "crf_only" in str(json_file):
                    ablation = "crf_only"
                else:
                    ablation = "unknown"
                
                metrics["ablation_type"] = ablation
                metrics["file_path"] = str(json_file)
                all_metrics.append(metrics)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å– {{json_file}}: {{e}}")

print(f"âœ“ æ‰¾åˆ° {{len(all_metrics)}} ä¸ªç»“æœæ–‡ä»¶")

# ç”Ÿæˆå¯¹æ¯”åˆ†æ
if len(all_metrics) >= 2:
    print("\\n" + "-"*80)
    print("ç»“æœå¯¹æ¯”:")
    print("-"*80)
    
    # æŒ‰ablationç±»å‹åˆ†ç»„
    baseline_results = [m for m in all_metrics if m["ablation_type"] == "baseline"]
    crf_results = [m for m in all_metrics if m["ablation_type"] == "crf_only"]
    
    if baseline_results and crf_results:
        # å–æœ€æ–°çš„ç»“æœï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼‰
        baseline = baseline_results[-1]
        crf = crf_results[-1]
        
        print(f"\\n{{assignment['task'].upper()}} ä»»åŠ¡:")
        print(f"  Baseline:")
        print(f"    - Token Acc: {{baseline.get('token_accuracy', 'N/A'):.4f if isinstance(baseline.get('token_accuracy'), (int, float)) else 'N/A'}}")
        print(f"    - Chunk F1:  {{baseline.get('chunk_f1', 'N/A'):.4f if isinstance(baseline.get('chunk_f1'), (int, float)) else 'N/A'}}")
        
        print(f"  CRF Only:")
        print(f"    - Token Acc: {{crf.get('token_accuracy', 'N/A'):.4f if isinstance(crf.get('token_accuracy'), (int, float)) else 'N/A'}}")
        print(f"    - Chunk F1:  {{crf.get('chunk_f1', 'N/A'):.4f if isinstance(crf.get('chunk_f1'), (int, float)) else 'N/A'}}")
        
        # è®¡ç®—æå‡
        if isinstance(baseline.get('chunk_f1'), (int, float)) and isinstance(crf.get('chunk_f1'), (int, float)):
            improvement = crf['chunk_f1'] - baseline['chunk_f1']
            improvement_pct = (improvement / baseline['chunk_f1']) * 100 if baseline['chunk_f1'] > 0 else 0
            print(f"  Improvement:")
            print(f"    - Chunk F1: +{{improvement:.4f}} ({{improvement_pct:+.1f}}%)")
else:
    print("âš ï¸ ç»“æœä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆå¯¹æ¯”åˆ†æ")

# ============================================================================
# Step 5: ä¿å­˜ç»“æœ
# ============================================================================

total_time = time.time() - start_time

print("\\n" + "="*80)
print("Step 5: ä¿å­˜ç»“æœ")
print("="*80)

# ä¿å­˜æ‰§è¡Œæ‘˜è¦
final_results = {{
    "account_id": "{account_id}",
    "account_name": "{assignment['name']}",
    "task": "{assignment['task']}",
    "ablations": {assignment['ablations']},
    "total_experiments": len(results),
    "successful": sum(1 for r in results if r['status'] == 'success'),
    "failed": sum(1 for r in results if r['status'] in ['failed', 'error']),
    "total_time_minutes": total_time / 60,
    "experiments": results,
    "metrics": all_metrics
}}

output_file = Path(KAGGLE_WORKING) / "{account_id}_final_results.json"
with open(output_file, 'w') as f:
    json.dump(final_results, f, indent=2)

print(f"âœ“ ç»“æœå·²ä¿å­˜: {{output_file}}")

# æ‰“åŒ…æ‰€æœ‰ç»“æœ
print("\\næ‰“åŒ…ç»“æœæ–‡ä»¶...")
import zipfile

# æ‰“åŒ…1: metricså’Œæ‘˜è¦JSON
zip_path = Path(KAGGLE_WORKING) / "{account_id}_results.zip"
with zipfile.ZipFile(zip_path, 'w') as zipf:
    # æ·»åŠ æ‘˜è¦JSON
    zipf.write(output_file, arcname=output_file.name)
    
    # æ·»åŠ æ‰€æœ‰metrics JSON
    for metrics in all_metrics:
        file_path = Path(metrics["file_path"])
        if file_path.exists():
            zipf.write(file_path, arcname=f"metrics/{{file_path.name}}")

print(f"âœ“ ç»“æœå·²æ‰“åŒ…: {{zip_path}}")

# æ‰“åŒ…2: å•ç‹¬å‹ç¼©æ‰€æœ‰æ¨¡å‹æ–‡ä»¶ï¼ˆ.ptï¼‰
print("\\nå‹ç¼©æ¨¡å‹æ–‡ä»¶...")
models_zip_path = Path(KAGGLE_WORKING) / "{account_id}_models.zip"
pt_files_count = 0

with zipfile.ZipFile(models_zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:
    # éå†å·¥ä½œç›®å½•æŸ¥æ‰¾æ‰€æœ‰ .pt æ–‡ä»¶
    for pt_file in Path(KAGGLE_WORKING).rglob("*.pt"):
        # æ·»åŠ åˆ°å‹ç¼©åŒ…ï¼Œä¿ç•™ç›¸å¯¹è·¯å¾„ç»“æ„
        arcname = pt_file.relative_to(KAGGLE_WORKING)
        zipf.write(pt_file, arcname=str(arcname))
        pt_files_count += 1
        print(f"  å‹ç¼©: {{arcname}}")

print(f"âœ“ æ¨¡å‹æ–‡ä»¶å·²å‹ç¼©: {{models_zip_path}}")
print(f"  å…± {{pt_files_count}} ä¸ªæ¨¡å‹æ–‡ä»¶")

print("\\n" + "="*80)
print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
print("="*80)
print(f"æ€»å®éªŒæ•°: {{len(results)}}")
print(f"æˆåŠŸ: {{sum(1 for r in results if r['status'] == 'success')}}")
print(f"å¤±è´¥: {{sum(1 for r in results if r['status'] in ['failed', 'error'])}}")
print(f"æ€»æ—¶é—´: {{total_time/60:.1f}} åˆ†é’Ÿ")
print("\\nè¯·ä¸‹è½½ä»¥ä¸‹æ–‡ä»¶:")
print(f"  1. {{output_file.name}} - JSONæ ¼å¼çš„è¯¦ç»†ç»“æœ")
print(f"  2. {{zip_path.name}} - æ‰“åŒ…çš„æŒ‡æ ‡å’Œç»“æœæ–‡ä»¶")
print(f"  3. {{models_zip_path.name}} - æ‰“åŒ…çš„æ‰€æœ‰æ¨¡å‹æ–‡ä»¶ï¼ˆ.ptï¼‰")
print("="*80)
''')
        
        print(f"  âœ“ Runner script: {runner_path}")
    
    def _generate_deployment_guide(self, output_dir: Path):
        """ç”Ÿæˆéƒ¨ç½²æŒ‡å—"""
        
        guide_path = output_dir / "KAGGLE_DEPLOYMENT_GUIDE.md"
        
        with open(guide_path, 'w', encoding='utf-8') as f:
            f.write("""# Kaggle 6è´¦å·Ablation Studyéƒ¨ç½²æŒ‡å—

## ğŸ“‹ å®éªŒè®¾è®¡

### æ€»ä½“æ–¹æ¡ˆ

- **æ€»é…ç½®æ•°**: 12ä¸ª (3ä»»åŠ¡ Ã— 4 ablation)
- **è´¦å·æ•°**: 6ä¸ª
- **æ¯è´¦å·é…ç½®æ•°**: 2ä¸ª
- **æ¯è´¦å·é¢„è®¡æ—¶é—´**: 3-4å°æ—¶
- **Kaggleé™åˆ¶**: 12å°æ—¶ï¼ˆå……è¶³ä½™é‡ï¼‰

### è´¦å·åˆ†é…

| è´¦å· | ä»»åŠ¡ | Ablationé…ç½® | é¢„è®¡æ—¶é—´ |
|------|------|--------------|----------|
| **Account 1** | MATE | baseline + crf_and_span | ~3å°æ—¶ |
| **Account 2** | MATE | crf_only + span_only | ~3å°æ—¶ |
| **Account 3** | MNER | baseline + crf_and_span | ~3.3å°æ—¶ |
| **Account 4** | MNER | crf_only + span_only | ~3.3å°æ—¶ |
| **Account 5** | MABSA | baseline + crf_and_span | ~3.6å°æ—¶ |
| **Account 6** | MABSA | crf_only + span_only | ~3.6å°æ—¶ |

## ğŸš€ éƒ¨ç½²æ­¥éª¤

### 1. å‡†å¤‡Kaggleæ•°æ®é›†

åœ¨æœ¬åœ°æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

```bash
# 1. æ‰“åŒ…é¡¹ç›®
cd /path/to/MCM
zip -r mcm-project.zip . -x "*.git*" "*__pycache__*" "*.pyc" "*checkpoint*" "*logs*"

# 2. ä¸Šä¼ åˆ°Kaggle
# åœ¨Kaggleç½‘ç«™ä¸Š:
# - ç‚¹å‡» "Datasets" -> "New Dataset"
# - ä¸Šä¼  mcm-project.zip
# - è®¾ç½®åç§°ä¸º "mcm-project"
# - è®¾ç½®ä¸º Private
# - ç‚¹å‡» "Create"
```

### 2. ä¸ºæ¯ä¸ªè´¦å·åˆ›å»ºNotebook

å¯¹äºæ¯ä¸ªè´¦å·ï¼ˆ1-6ï¼‰ï¼Œé‡å¤ä»¥ä¸‹æ­¥éª¤ï¼š

#### Step 1: åˆ›å»ºNotebook

1. ç™»å½•å¯¹åº”çš„Kaggleè´¦å·
2. ç‚¹å‡» "Code" -> "New Notebook"
3. Notebookè®¾ç½®ï¼š
   - **åŠ é€Ÿå™¨**: GPU P100
   - **Internet**: On
   - **æŒä¹…åŒ–**: Off (èŠ‚çœé…é¢)

#### Step 2: æ·»åŠ æ•°æ®é›†

1. ç‚¹å‡»å³ä¾§ "Add Data"
2. æœç´¢ "mcm-project"
3. æ·»åŠ ä½ çš„æ•°æ®é›†

#### Step 3: å¤åˆ¶è¿è¡Œè„šæœ¬

1. æ‰“å¼€å¯¹åº”è´¦å·çš„è¿è¡Œè„šæœ¬:
   - Account 1: `account_1/run_account_1.py`
   - Account 2: `account_2/run_account_2.py`
   - ... (ä»¥æ­¤ç±»æ¨)

2. å¤åˆ¶å…¨éƒ¨å†…å®¹åˆ°Notebook

3. ä¿®æ”¹æ•°æ®é›†åç§°ï¼ˆå¦‚æœéœ€è¦ï¼‰:
   ```python
   PROJECT_DATASET = "your-username/mcm-project"  # ä¿®æ”¹ä¸ºä½ çš„æ•°æ®é›†è·¯å¾„
   ```

#### Step 4: è¿è¡Œ

1. ç‚¹å‡» "Save Version"
2. é€‰æ‹© "Save & Run All (Commit)"
3. ç­‰å¾…å®Œæˆï¼ˆ3-4å°æ—¶ï¼‰

### 3. åŒæ—¶è¿è¡Œæ‰€æœ‰è´¦å·

âœ¨ **å…³é”®**: 6ä¸ªè´¦å·å¯ä»¥åŒæ—¶è¿è¡Œï¼Œäº’ä¸å¹²æ‰°ï¼

```
Account 1  â†’  [MATE baseline + full]     â†’  3å°æ—¶  â†’  å®Œæˆ
Account 2  â†’  [MATE crf + span]          â†’  3å°æ—¶  â†’  å®Œæˆ
Account 3  â†’  [MNER baseline + full]     â†’  3.3å°æ—¶ â†’  å®Œæˆ
Account 4  â†’  [MNER crf + span]          â†’  3.3å°æ—¶ â†’  å®Œæˆ
Account 5  â†’  [MABSA baseline + full]    â†’  3.6å°æ—¶ â†’  å®Œæˆ
Account 6  â†’  [MABSA crf + span]         â†’  3.6å°æ—¶ â†’  å®Œæˆ

æ€»æ—¶é—´: ~3.6å°æ—¶ (å¹¶è¡Œ)
```

## ğŸ“Š æ”¶é›†ç»“æœ

### æ¯ä¸ªè´¦å·å®Œæˆå

1. è¿›å…¥å¯¹åº”çš„Notebook
2. ç‚¹å‡» "Output"
3. ä¸‹è½½æ–‡ä»¶:
   - `account_X_final_results.json` (å¿…éœ€)
   - `checkpoints/` ç›®å½•ä¸‹çš„æ¨¡å‹æ–‡ä»¶ (å¯é€‰)

### æ–‡ä»¶ç»„ç»‡

```
results/
â”œâ”€â”€ account_1_final_results.json
â”œâ”€â”€ account_2_final_results.json
â”œâ”€â”€ account_3_final_results.json
â”œâ”€â”€ account_4_final_results.json
â”œâ”€â”€ account_5_final_results.json
â””â”€â”€ account_6_final_results.json
```

## ğŸ” ç»“æœåˆ†æ

ä¸‹è½½æ‰€æœ‰ç»“æœåï¼Œè¿è¡Œåˆ†æè„šæœ¬ï¼š

```bash
cd scripts/configs/kaggle_ablation

# å°†ä¸‹è½½çš„JSONæ–‡ä»¶æ”¾åˆ°results/ç›®å½•

python analyze_results.py
```

è¿™å°†ç”Ÿæˆï¼š
- `ablation_study_summary.json` - æ€»ç»“
- `ablation_study_report.md` - è¯¦ç»†æŠ¥å‘Š
- `ablation_comparison.png` - å¯¹æ¯”å›¾è¡¨

## âš ï¸ æ³¨æ„äº‹é¡¹

### Kaggleé™åˆ¶

- **GPUæ—¶é—´**: æ¯å‘¨30å°æ—¶ï¼ˆæ¯è´¦å·ï¼‰
- **è¿è¡Œæ—¶é—´**: å•æ¬¡æœ€å¤š12å°æ—¶
- **å¹¶è¡Œ**: æ¯è´¦å·æœ€å¤š1ä¸ªactive session

### æœ€ä½³å®è·µ

1. **ç›‘æ§è¿›åº¦**: å®šæœŸæ£€æŸ¥Outputæ—¥å¿—
2. **åŠæ—¶ä¿å­˜**: å®éªŒå®Œæˆåç«‹å³ä¸‹è½½ç»“æœ
3. **å¤‡ä»½**: ä¿å­˜æ‰€æœ‰JSONæ–‡ä»¶
4. **ç½‘ç»œç¨³å®š**: ç¡®ä¿ä¸Šä¼ æ•°æ®é›†æ—¶ç½‘ç»œç¨³å®š

### æ•…éšœå¤„ç†

**é—®é¢˜1: æ•°æ®é›†æœªæ‰¾åˆ°**
```
âŒ æ•°æ®é›†æœªæ‰¾åˆ°: /kaggle/input/mcm-project
```
è§£å†³: æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®æ·»åŠ ï¼Œåç§°æ˜¯å¦åŒ¹é…

**é—®é¢˜2: GPUä¸å¯ç”¨**
```
âŒ CUDA available: False
```
è§£å†³: æ£€æŸ¥Notebookè®¾ç½®ï¼Œç¡®ä¿é€‰æ‹©äº†GPU P100åŠ é€Ÿå™¨

**é—®é¢˜3: æ—¶é—´è¶…é™**
```
Session timeout after 12 hours
```
è§£å†³: å‡å°‘epochsæˆ–batch_sizeï¼ˆä½†æˆ‘ä»¬çš„é…ç½®åº”è¯¥ä¸ä¼šè¶…æ—¶ï¼‰

## ğŸ“ˆ é¢„æœŸç»“æœ

### Chunk F1æå‡

| é…ç½® | MATE | MNER | MABSA |
|------|------|------|-------|
| **Baseline** | ~32% | ~30% | ~35% |
| **CRF only** | ~68% (+36%) | ~65% (+35%) | ~70% (+35%) |
| **Span only** | ~65% (+33%) | ~62% (+32%) | ~67% (+32%) |
| **CRF + Span** | ~76% (+44%) | ~74% (+44%) | ~78% (+43%) |

## ğŸ‰ å®Œæˆæ£€æŸ¥æ¸…å•

- [ ] 6ä¸ªè´¦å·çš„Notebookéƒ½å·²åˆ›å»º
- [ ] æ‰€æœ‰Notebookéƒ½æ·»åŠ äº†mcm-projectæ•°æ®é›†
- [ ] æ‰€æœ‰Notebookéƒ½è®¾ç½®äº†GPU P100
- [ ] æ‰€æœ‰è¿è¡Œè„šæœ¬å·²æ­£ç¡®å¤åˆ¶
- [ ] 6ä¸ªNotebookåŒæ—¶è¿è¡Œ
- [ ] æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¸‹è½½
- [ ] ç»“æœåˆ†æè„šæœ¬å·²è¿è¡Œ
- [ ] åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿå¼€å§‹ä½ çš„Ablation Studyä¹‹æ—…ï¼** ğŸš€
""")
        
        print(f"  âœ“ Deployment guide: {guide_path}")
    
    def _generate_analysis_script(self, output_dir: Path, all_configs: dict):
        """ç”Ÿæˆç»“æœåˆ†æè„šæœ¬"""
        
        script_path = output_dir / "analyze_results.py"
        
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write('''#!/usr/bin/env python3
"""
Ablation Studyç»“æœåˆ†æè„šæœ¬

åˆ†æ6ä¸ªè´¦å·çš„å®éªŒç»“æœï¼Œç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""

import json
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def load_all_results(results_dir="results"):
    """åŠ è½½æ‰€æœ‰è´¦å·çš„ç»“æœ"""
    results_path = Path(results_dir)
    all_results = {}
    
    for i in range(1, 7):
        result_file = results_path / f"account_{i}_final_results.json"
        if result_file.exists():
            with open(result_file, 'r') as f:
                all_results[f"account_{i}"] = json.load(f)
            print(f"âœ“ Loaded: {result_file}")
        else:
            print(f"âš ï¸  Missing: {result_file}")
    
    return all_results

def extract_metrics(all_results):
    """æå–æ‰€æœ‰æŒ‡æ ‡"""
    data = []
    
    for account_id, account_data in all_results.items():
        task = account_data["task"]
        
        for exp in account_data["experiments"]:
            if exp["status"] == "success":
                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è¾“å‡ºæå–metrics
                # å‡è®¾metricsä¿å­˜åœ¨å•ç‹¬çš„æ–‡ä»¶ä¸­
                data.append({
                    "account": account_id,
                    "task": task,
                    "ablation": exp["ablation_type"],
                    "time_minutes": exp["time_minutes"],
                    # TODO: æ·»åŠ ä»ç»“æœæ–‡ä»¶ä¸­è¯»å–çš„metrics
                })
    
    return pd.DataFrame(data)

def generate_report(df, output_dir="results"):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    output_path = Path(output_dir)
    
    # ç”Ÿæˆæ€»ç»“
    summary = {
        "total_experiments": len(df),
        "successful_experiments": len(df[df["time_minutes"].notna()]),
        "total_time_hours": df["time_minutes"].sum() / 60,
        "tasks": df["task"].unique().tolist(),
        "ablations": df["ablation"].unique().tolist()
    }
    
    with open(output_path / "ablation_study_summary.json", 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"âœ“ Summary saved: {output_path / 'ablation_study_summary.json'}")
    
    # ç”ŸæˆmarkdownæŠ¥å‘Š
    with open(output_path / "ablation_study_report.md", 'w') as f:
        f.write("# Ablation Study Results\\n\\n")
        f.write(f"## Summary\\n\\n")
        f.write(f"- Total Experiments: {summary['total_experiments']}\\n")
        f.write(f"- Successful: {summary['successful_experiments']}\\n")
        f.write(f"- Total Time: {summary['total_time_hours']:.1f} hours\\n")
        f.write(f"\\n## Results by Task\\n\\n")
        
        for task in summary['tasks']:
            task_df = df[df['task'] == task]
            f.write(f"### {task.upper()}\\n\\n")
            f.write(task_df.to_markdown(index=False))
            f.write("\\n\\n")
    
    print(f"âœ“ Report saved: {output_path / 'ablation_study_report.md'}")

def main():
    print("="*80)
    print("Ablation Studyç»“æœåˆ†æ")
    print("="*80)
    
    # åŠ è½½ç»“æœ
    all_results = load_all_results()
    
    if not all_results:
        print("\\nâŒ æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶")
        print("è¯·ç¡®ä¿å°†æ‰€æœ‰ account_X_final_results.json æ”¾åœ¨ results/ ç›®å½•")
        return
    
    print(f"\\nâœ“ åŠ è½½äº† {len(all_results)} ä¸ªè´¦å·çš„ç»“æœ\\n")
    
    # æå–æŒ‡æ ‡
    df = extract_metrics(all_results)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(df)
    
    print("\\nâœ… åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
''')
        
        print(f"  âœ“ Analysis script: {script_path}")


def main():
    parser = argparse.ArgumentParser(
        description="ç”ŸæˆKaggle 6è´¦å·Ablation Studyé…ç½®"
    )
    parser.add_argument("--env", type=str, default="server",
                       choices=["local", "server"],
                       help="ç¯å¢ƒç±»å‹")
    parser.add_argument("--dataset", type=str, default="twitter2015",
                       choices=["twitter2015", "twitter2017"],
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--output_dir", type=str,
                       default="scripts/configs/kaggle_ablation",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--account", type=str, default="all",
                       help="æŒ‡å®šè´¦å·ID (account_1 åˆ° account_6) æˆ– 'all'")
    
    args = parser.parse_args()
    
    generator = KaggleAblationStudyGenerator()
    
    if args.account == "all":
        # ä¸ºæ‰€æœ‰è´¦å·ç”Ÿæˆé…ç½®
        generator.generate_all_accounts(
            env=args.env,
            dataset=args.dataset,
            output_dir=args.output_dir
        )
    else:
        # åªä¸ºæŒ‡å®šè´¦å·ç”Ÿæˆé…ç½®
        generator.generate_account_configs(
            account_id=args.account,
            env=args.env,
            dataset=args.dataset,
            output_dir=args.output_dir
        )
    
    print("\n" + "="*80)
    print("ä¸‹ä¸€æ­¥:")
    print("="*80)
    print("1. æŸ¥çœ‹éƒ¨ç½²æŒ‡å—: scripts/configs/kaggle_ablation/KAGGLE_DEPLOYMENT_GUIDE.md")
    print("2. å‡†å¤‡Kaggleæ•°æ®é›†")
    print("3. åœ¨6ä¸ªè´¦å·ä¸ŠåŒæ—¶è¿è¡Œ")
    print("4. ä¸‹è½½ç»“æœJSONæ–‡ä»¶")
    print("5. è¿è¡Œç»“æœåˆ†æ: python scripts/configs/kaggle_ablation/analyze_results.py")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()

