#!/usr/bin/env python3
"""
ç”ŸæˆMATEã€MNERã€MABSAè¶…å‚æ•°æœç´¢é…ç½®æ–‡ä»¶ - Kaggleç‰ˆæœ¬

é’ˆå¯¹Kaggleç¯å¢ƒçš„ç‰¹æ®Šä¼˜åŒ–:
1. è¾“å‡ºç›®å½•: /kaggle/working (Kaggleå¯å†™ç›®å½•)
2. æ•°æ®é›†è·¯å¾„: /kaggle/input/dataset-name/ (Kaggleæ•°æ®é›†æŒ‚è½½ç‚¹)
3. GPUç­–ç•¥: ç‹¬äº«P100ï¼Œä¸éœ€è¦ç­‰å¾…ï¼Œæ”¯æŒä¸²è¡Œè¿è¡Œ
4. æ—¶é—´é™åˆ¶: è€ƒè™‘Kaggle 9-12å°æ—¶è¿è¡Œé™åˆ¶
5. å®šæœŸä¿å­˜: æ¯ä¸ªå®éªŒå®Œæˆåç«‹å³ä¿å­˜ç»“æœ
"""

import json
import argparse
from pathlib import Path
from typing import List, Tuple
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from scripts.generate_task_config import TaskConfigGenerator


class KaggleTaskHyperparameterSearchGenerator:
    """Kaggleç¯å¢ƒä¸‹çš„å¤šä»»åŠ¡è¶…å‚æ•°æœç´¢é…ç½®ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.base_generator = TaskConfigGenerator()
        
        # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
        self.hyperparameter_grid = {
            "lr": [5e-5, 1e-5, 5e-6],
            "step_size": [5, 10, 15],
            "gamma": [0.3, 0.5, 0.7]
        }
        
        # ä»»åŠ¡åˆ—è¡¨
        self.tasks = ["mate", "mner", "mabsa"]
        
        # ç­–ç•¥åˆ—è¡¨
        self.strategies = ["none"]
        
    def get_hyperparameter_combinations(self) -> List[Tuple[float, int, float]]:
        """ç”Ÿæˆåˆç†çš„è¶…å‚æ•°ç»„åˆ"""
        combinations = []
        
        # ç­–ç•¥1: å›ºå®šgammaï¼Œå˜åŒ–lrå’Œstep_size
        for lr in self.hyperparameter_grid["lr"]:
            for step_size in self.hyperparameter_grid["step_size"]:
                combinations.append((lr, step_size, 0.5))
        
        # ç­–ç•¥2: å›ºå®šlrå’Œstep_sizeï¼Œå˜åŒ–gamma
        for gamma in self.hyperparameter_grid["gamma"]:
            if gamma != 0.5:
                combinations.append((1e-5, 10, gamma))
        
        # ç­–ç•¥3: ç‰¹æ®Šç»„åˆ
        special_combinations = [
            (5e-5, 5, 0.7),
            (5e-6, 15, 0.3),
            (1e-5, 10, 0.5),
        ]
        
        for combo in special_combinations:
            if combo not in combinations:
                combinations.append(combo)
        
        return combinations
    
    def generate_single_config(self, 
                              env: str,
                              dataset: str,
                              task_name: str,
                              strategy: str,
                              lr: float,
                              step_size: int,
                              gamma: float,
                              seq_suffix: str = "",
                              kaggle_output_path: str = "/kaggle/working") -> dict:
        """ç”Ÿæˆå•ä¸ªé…ç½®æ–‡ä»¶ - Kaggleä¼˜åŒ–ç‰ˆæœ¬
        
        æ³¨æ„ï¼š
        - æ•°æ®é›†è·¯å¾„ä½¿ç”¨é¡¹ç›®å†…ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚ data/twitter2015_images/ï¼‰
        - é¡¹ç›®ä¼šè¢«å¤åˆ¶åˆ° /kaggle/working/MCMï¼Œæ•°æ®é›†ä¼šéšä¹‹ç§»åŠ¨
        - åªéœ€è¦ä¿®æ”¹checkpointè¾“å‡ºè·¯å¾„åˆ° /kaggle/working/checkpoints
        """
        
        task_sequence = [task_name, task_name]
        mode_sequence = ["text_only", "multimodal"]
        
        # ä½¿ç”¨åŸºç¡€ç”Ÿæˆå™¨ç”Ÿæˆé…ç½®
        config = self.base_generator.generate_task_sequence_config(
            env=env,
            dataset=dataset,
            task_sequence=task_sequence,
            mode_sequence=mode_sequence,
            strategy=strategy,
            use_label_embedding=False,
            seq_suffix=seq_suffix,
            lr=lr,
            step_size=step_size,
            gamma=gamma,
            epochs=20,
            patience=999
        )
        
        # Kaggleç‰¹æ®Šé…ç½® - åªä¿®æ”¹è¾“å‡ºè·¯å¾„ï¼Œæ•°æ®è·¯å¾„ä¿æŒç›¸å¯¹è·¯å¾„
        config["kaggle_mode"] = True
        config["kaggle_output_path"] = kaggle_output_path
        
        # æ·»åŠ è¶…å‚æ•°ä¿¡æ¯
        config["hyperparameters"] = {
            "lr": lr,
            "step_size": step_size,
            "gamma": gamma
        }
        
        return config
    
    def generate_all_configs(self, 
                            env: str = "server",
                            dataset: str = "twitter2015",
                            output_dir: str = "scripts/configs/kaggle_hyperparam_search",
                            kaggle_output_path: str = "/kaggle/working"):
        """ç”Ÿæˆæ‰€æœ‰é…ç½®æ–‡ä»¶"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        hyperparams = self.get_hyperparameter_combinations()
        
        print(f"ç”ŸæˆKaggleé…ç½®æ–‡ä»¶åˆ°: {output_path}")
        print(f"ä»»åŠ¡åˆ—è¡¨: {self.tasks}")
        print(f"ç­–ç•¥: {self.strategies}")
        print(f"è¶…å‚æ•°ç»„åˆæ•°é‡: {len(hyperparams)}")
        print(f"æ€»é…ç½®æ–‡ä»¶æ•°: {len(self.tasks) * len(self.strategies) * len(hyperparams)}")
        print(f"Kaggleè¾“å‡ºè·¯å¾„: {kaggle_output_path}")
        print()
        
        configs_generated = []
        
        for task_name in self.tasks:
            print(f"\n{'='*60}")
            print(f"ç”Ÿæˆä»»åŠ¡: {task_name.upper()}")
            print(f"{'='*60}\n")
            
            for strategy in self.strategies:
                for i, (lr, step_size, gamma) in enumerate(hyperparams):
                    lr_str = f"{lr:.0e}".replace("-", "").replace("+", "")
                    config_name = f"kaggle_{dataset}_{task_name}_{strategy}_lr{lr_str}_ss{step_size}_g{gamma:.1f}.json"
                    config_file = output_path / config_name
                    
                    seq_suffix = f"hp{i+1}"
                    
                    config = self.generate_single_config(
                        env=env,
                        dataset=dataset,
                        task_name=task_name,
                        strategy=strategy,
                        lr=lr,
                        step_size=step_size,
                        gamma=gamma,
                        seq_suffix=seq_suffix,
                        kaggle_output_path=kaggle_output_path
                    )
                    
                    with open(config_file, 'w', encoding='utf-8') as f:
                        json.dump(config, f, indent=2, ensure_ascii=False)
                    
                    configs_generated.append({
                        "file": config_file.as_posix(),
                        "task": task_name,
                        "strategy": strategy,
                        "lr": lr,
                        "step_size": step_size,
                        "gamma": gamma
                    })
                    
                    print(f"âœ“ {config_name}")
        
        # ç”Ÿæˆç´¢å¼•æ–‡ä»¶
        index_file = output_path / "config_index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump({
                "total_configs": len(configs_generated),
                "tasks": self.tasks,
                "strategies": self.strategies,
                "hyperparameter_grid": self.hyperparameter_grid,
                "kaggle_optimized": True,
                "configs": configs_generated
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nç´¢å¼•æ–‡ä»¶å·²ç”Ÿæˆ: {index_file}")
        print(f"\næ€»å…±ç”Ÿæˆ {len(configs_generated)} ä¸ªé…ç½®æ–‡ä»¶")
        
        return configs_generated


def main():
    parser = argparse.ArgumentParser(description="ç”ŸæˆKaggleç¯å¢ƒçš„è¶…å‚æ•°æœç´¢é…ç½®")
    parser.add_argument("--env", type=str, default="server",
                       choices=["local", "server"],
                       help="ç¯å¢ƒç±»å‹")
    parser.add_argument("--dataset", type=str, default="twitter2015",
                       choices=["twitter2015", "twitter2017", "mix"],
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--output_dir", type=str, 
                       default="scripts/configs/kaggle_hyperparam_search",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--kaggle_dataset_name", type=str,
                       default="mcm-project",
                       help="Kaggleæ•°æ®é›†åç§°")
    parser.add_argument("--max_experiments_per_session", type=int,
                       default=5,
                       help="æ¯ä¸ªKaggleä¼šè¯æœ€å¤šè¿è¡Œçš„å®éªŒæ•°ï¼ˆè€ƒè™‘æ—¶é—´é™åˆ¶ï¼‰")
    
    args = parser.parse_args()
    
    # Kaggleè¾“å‡ºè·¯å¾„
    kaggle_output_path = "/kaggle/working"
    
    # ç”Ÿæˆé…ç½®
    generator = KaggleTaskHyperparameterSearchGenerator()
    configs = generator.generate_all_configs(
        env=args.env,
        dataset=args.dataset,
        output_dir=args.output_dir,
        kaggle_output_path=kaggle_output_path
    )
    
    # ç”ŸæˆKaggle Notebookè„šæœ¬
    notebook_path = Path(args.output_dir) / "kaggle_runner.py"
    _generate_kaggle_runner(notebook_path, configs, args.output_dir, args.max_experiments_per_session)
    
    # ç”Ÿæˆéƒ¨ç½²è¯´æ˜
    readme_path = Path(args.output_dir) / "KAGGLE_DEPLOYMENT.md"
    _generate_deployment_guide(readme_path, args.kaggle_dataset_name, len(configs), args.max_experiments_per_session)
    
    # ç”Ÿæˆé¡¹ç›®å‡†å¤‡è„šæœ¬
    prep_script_path = Path(args.output_dir) / "prepare_for_kaggle.sh"
    _generate_preparation_script(prep_script_path)
    
    # ç”Ÿæˆç»“æœåˆ†æè„šæœ¬ï¼ˆKaggleç‰ˆï¼‰
    analysis_script_path = Path(args.output_dir) / "analyze_kaggle_results.py"
    _generate_kaggle_analysis_script(analysis_script_path, configs)
    
    print(f"\nâœ“ Kaggleè¿è¡Œè„šæœ¬å·²ç”Ÿæˆ: {notebook_path}")
    print(f"âœ“ éƒ¨ç½²è¯´æ˜å·²ç”Ÿæˆ: {readme_path}")
    print(f"âœ“ å‡†å¤‡è„šæœ¬å·²ç”Ÿæˆ: {prep_script_path}")
    print(f"âœ“ ç»“æœåˆ†æè„šæœ¬å·²ç”Ÿæˆ: {analysis_script_path}")
    print(f"\nè¯·æŸ¥çœ‹ {readme_path} äº†è§£è¯¦ç»†éƒ¨ç½²æ­¥éª¤")


def _generate_kaggle_runner(script_path: Path, configs: list, output_dir: str, max_experiments: int):
    """ç”ŸæˆKaggle Notebookè¿è¡Œè„šæœ¬"""
    
    with open(script_path, 'w', encoding='utf-8', newline='\n') as f:
        f.write(f'''#!/usr/bin/env python3
"""
Kaggle Notebookè¿è¡Œè„šæœ¬ - MATEã€MNERã€MABSAè¶…å‚æ•°æœç´¢

æ­¤è„šæœ¬è®¾è®¡åœ¨Kaggle Notebookä¸­è¿è¡Œï¼Œé’ˆå¯¹Kaggleç¯å¢ƒä¼˜åŒ–ï¼š
1. ç‹¬äº«P100 GPUï¼Œæ— éœ€ç­‰å¾…
2. è‡ªåŠ¨è®¾ç½®é¡¹ç›®è·¯å¾„
3. å®‰è£…ä¾èµ–
4. ä¸²è¡Œè¿è¡Œå®éªŒï¼ˆé¿å…æ˜¾å­˜é—®é¢˜ï¼‰
5. å®šæœŸä¿å­˜ç»“æœåˆ° /kaggle/working
6. æ”¯æŒæ–­ç‚¹ç»­è·‘ï¼ˆè€ƒè™‘9-12å°æ—¶é™åˆ¶ï¼‰

ä½¿ç”¨æ–¹æ³•:
1. åœ¨Kaggle Notebookä¸­è¿è¡Œæ­¤è„šæœ¬
2. è®¾ç½®åŠ é€Ÿå™¨ä¸º GPU P100
3. æ·»åŠ MCMé¡¹ç›®æ•°æ®é›†
4. è¿è¡Œå…¨éƒ¨æˆ–æŒ‡å®šèŒƒå›´çš„å®éªŒ

å‚æ•°:
    --start_exp: èµ·å§‹å®éªŒIDï¼ˆé»˜è®¤1ï¼‰
    --end_exp: ç»“æŸå®éªŒIDï¼ˆé»˜è®¤{max_experiments}ï¼‰
    --config_dir: é…ç½®æ–‡ä»¶ç›®å½•ï¼ˆé»˜è®¤ä»æ•°æ®é›†è¯»å–ï¼‰
"""

import os
import sys
import json
import subprocess
import time
from pathlib import Path
import shutil

# Kaggleç¯å¢ƒé…ç½®
KAGGLE_INPUT = "/kaggle/input"
KAGGLE_WORKING = "/kaggle/working"
CODE_DATASET = "mcm-code"      # ä»£ç æ•°æ®é›†åç§°
DATA_DATASET = "mcm-data"      # æ•°æ®æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä½¿ç”¨åˆ†ç¦»æ–¹æ¡ˆï¼‰
PROJECT_NAME = "mcm-project"   # å®Œæ•´é¡¹ç›®æ•°æ®é›†åç§°ï¼ˆå‘åå…¼å®¹ï¼‰

# é¢œè‰²è¾“å‡º
class Colors:
    GREEN = '\\033[0;32m'
    YELLOW = '\\033[1;33m'
    RED = '\\033[0;31m'
    BLUE = '\\033[0;34m'
    CYAN = '\\033[0;36m'
    NC = '\\033[0m'

def print_info(msg):
    print(f"{{Colors.GREEN}}[INFO]{{Colors.NC}} {{msg}}")

def print_warning(msg):
    print(f"{{Colors.YELLOW}}[WARNING]{{Colors.NC}} {{msg}}")

def print_error(msg):
    print(f"{{Colors.RED}}[ERROR]{{Colors.NC}} {{msg}}")

def print_separator():
    print(f"{{Colors.BLUE}}{'='*80}{{Colors.NC}}")

def setup_environment():
    """è®¾ç½®Kaggleç¯å¢ƒ"""
    global KAGGLE_INPUT, KAGGLE_WORKING  # å¿…é¡»åœ¨å‡½æ•°å¼€å¤´å£°æ˜
    
    print_separator()
    print_info("è®¾ç½®Kaggleç¯å¢ƒ...")
    print_separator()
    
    # 1. æ£€æŸ¥æ˜¯å¦åœ¨Kaggleç¯å¢ƒ
    if not os.path.exists(KAGGLE_INPUT):
        print_warning("æœªæ£€æµ‹åˆ°Kaggleç¯å¢ƒï¼Œä½¿ç”¨æœ¬åœ°è·¯å¾„")
        KAGGLE_INPUT = "."
        KAGGLE_WORKING = "./output"
        os.makedirs(KAGGLE_WORKING, exist_ok=True)
    
    # 2. æŸ¥æ‰¾é¡¹ç›®è·¯å¾„ï¼ˆæ”¯æŒä¸¤ç§æ¨¡å¼ï¼‰
    code_path = None
    data_path = None
    use_split_mode = False
    
    # æ¨¡å¼1: åˆ†ç¦»æ¨¡å¼ï¼ˆä»£ç å’Œæ•°æ®åˆ†å¼€ï¼‰
    possible_code_paths = [
        Path(KAGGLE_INPUT) / CODE_DATASET,
        Path(KAGGLE_INPUT) / CODE_DATASET / "MCM",
    ]
    
    for path in possible_code_paths:
        if path.exists():
            code_path = path
            use_split_mode = True
            print_info(f"âœ“ æ£€æµ‹åˆ°åˆ†ç¦»æ¨¡å¼ - ä»£ç è·¯å¾„: {{path}}")
            break
    
    # å¦‚æœä½¿ç”¨åˆ†ç¦»æ¨¡å¼ï¼ŒæŸ¥æ‰¾æ•°æ®è·¯å¾„
    if use_split_mode:
        possible_data_paths = [
            Path(KAGGLE_INPUT) / DATA_DATASET,
            Path(KAGGLE_INPUT) / DATA_DATASET / "data",
        ]
        
        for path in possible_data_paths:
            if path.exists():
                data_path = path
                print_info(f"âœ“ æ£€æµ‹åˆ°åˆ†ç¦»æ¨¡å¼ - æ•°æ®è·¯å¾„: {{path}}")
                break
        
        if data_path is None:
            print_warning(f"æœªæ‰¾åˆ°æ•°æ®æ•°æ®é›† '{{DATA_DATASET}}'ï¼Œå°†åªä½¿ç”¨ä»£ç æ•°æ®é›†")
    
    # æ¨¡å¼2: å®Œæ•´æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
    if not use_split_mode:
        possible_full_paths = [
            Path(KAGGLE_INPUT) / PROJECT_NAME,
            Path(KAGGLE_INPUT) / PROJECT_NAME / "MCM",
            Path(KAGGLE_INPUT) / "MCM",
        ]
        
        for path in possible_full_paths:
            if path.exists():
                code_path = path
                print_info(f"âœ“ æ£€æµ‹åˆ°å®Œæ•´æ¨¡å¼ - é¡¹ç›®è·¯å¾„: {{path}}")
                break
    
    if code_path is None:
        print_error("æœªæ‰¾åˆ°é¡¹ç›®è·¯å¾„ï¼Œå°è¯•åˆ—å‡ºå¯ç”¨æ•°æ®é›†...")
        if os.path.exists(KAGGLE_INPUT):
            print(f"å¯ç”¨æ•°æ®é›†: {{os.listdir(KAGGLE_INPUT)}}")
        raise FileNotFoundError("æ— æ³•æ‰¾åˆ°MCMé¡¹ç›®ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†é…ç½®")
    
    # 3. å¤åˆ¶é¡¹ç›®åˆ°æ ¹ç›®å½•
    work_project_path = Path("/MCM")
    
    if not work_project_path.exists():
        print_info(f"å¤åˆ¶ä»£ç åˆ°é¡¹ç›®ç›®å½•: {{work_project_path}}")
        shutil.copytree(code_path, work_project_path, dirs_exist_ok=True)
    else:
        print_info(f"é¡¹ç›®ç›®å½•å·²å­˜åœ¨: {{work_project_path}}")
    
    # 4. å¦‚æœæ˜¯åˆ†ç¦»æ¨¡å¼ï¼Œé“¾æ¥æˆ–å¤åˆ¶æ•°æ®ç›®å½•
    if use_split_mode and data_path:
        target_data_dir = work_project_path / "data"
        target_model_dir = work_project_path / "downloaded_model"
        
        # å¤„ç†dataç›®å½•
        if (data_path / "data").exists():
            source_data = data_path / "data"
        else:
            source_data = data_path
        
        if not target_data_dir.exists():
            print_info(f"é“¾æ¥æ•°æ®ç›®å½•: {{source_data}} -> {{target_data_dir}}")
            try:
                # å°è¯•åˆ›å»ºç¬¦å·é“¾æ¥ï¼ˆæ›´å¿«ï¼‰
                os.symlink(source_data, target_data_dir)
            except (OSError, NotImplementedError):
                # å¦‚æœä¸æ”¯æŒç¬¦å·é“¾æ¥ï¼Œåˆ™å¤åˆ¶
                print_warning("ä¸æ”¯æŒç¬¦å·é“¾æ¥ï¼Œå¤åˆ¶æ•°æ®ç›®å½•ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰...")
                shutil.copytree(source_data, target_data_dir, dirs_exist_ok=True)
        
        # å¤„ç†downloaded_modelç›®å½•  
        source_model = data_path / "downloaded_model"
        if source_model.exists() and not target_model_dir.exists():
            print_info(f"é“¾æ¥æ¨¡å‹ç›®å½•: {{source_model}} -> {{target_model_dir}}")
            try:
                os.symlink(source_model, target_model_dir)
            except (OSError, NotImplementedError):
                print_warning("ä¸æ”¯æŒç¬¦å·é“¾æ¥ï¼Œå¤åˆ¶æ¨¡å‹ç›®å½•...")
                shutil.copytree(source_model, target_model_dir, dirs_exist_ok=True)
    
    # 5. åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½• /MCM
    os.chdir(work_project_path)
    print_info(f"å½“å‰å·¥ä½œç›®å½•: {{os.getcwd()}}")
    
    # 6. æ·»åŠ åˆ°Pythonè·¯å¾„
    sys.path.insert(0, str(work_project_path))
    print_info(f"å·²æ·»åŠ åˆ°Pythonè·¯å¾„: {{work_project_path}}")
    
    # 7. å®‰è£…ä¾èµ–ï¼ˆKaggleä¼˜åŒ–ï¼‰
    # ä¼˜å…ˆä½¿ç”¨Kaggleä¼˜åŒ–çš„requirementsæ–‡ä»¶
    kaggle_req = work_project_path / "requirements_kaggle.txt"
    regular_req = work_project_path / "requirements.txt"
    
    if kaggle_req.exists():
        print_info("æ£€æµ‹åˆ°Kaggleä¼˜åŒ–çš„ä¾èµ–æ–‡ä»¶ï¼Œä½¿ç”¨ requirements_kaggle.txt")
        requirements_file = kaggle_req
    elif regular_req.exists():
        print_info("ä½¿ç”¨æ ‡å‡†ä¾èµ–æ–‡ä»¶ requirements.txt")
        print_warning("âš ï¸  å¯èƒ½ä¼šæœ‰ç‰ˆæœ¬å†²çªè­¦å‘Šï¼ˆé€šå¸¸å¯ä»¥å¿½ç•¥ï¼‰")
        requirements_file = regular_req
    else:
        print_warning("æœªæ‰¾åˆ°ä¾èµ–æ–‡ä»¶ï¼Œå®‰è£…æœ€å°ä¾èµ–é›†")
        # å®‰è£…å¿…éœ€çš„åŒ…
        minimal_packages = ["pytorch_crf", "sentencepiece", "protobuf==3.20.3"]
        for pkg in minimal_packages:
            try:
                subprocess.run([sys.executable, "-m", "pip", "install", "-q", pkg], check=False)
            except:
                pass
        requirements_file = None
    
    if requirements_file:
        print_info(f"å®‰è£…ä¾èµ–: {{requirements_file.name}}")
        try:
            # ä½¿ç”¨--no-depsé¿å…è‡ªåŠ¨è§£æå†²çª
            subprocess.run(
                [sys.executable, "-m", "pip", "install", "-q", "-r", str(requirements_file)],
                check=False  # ä¸å› è­¦å‘Šè€Œå¤±è´¥
            )
            print_info("âœ“ ä¾èµ–å®‰è£…å®Œæˆï¼ˆå¿½ç•¥ç‰ˆæœ¬å†²çªè­¦å‘Šï¼‰")
        except subprocess.CalledProcessError as e:
            print_warning(f"ä¾èµ–å®‰è£…æœ‰è­¦å‘Š: {{e}}")
            print_warning("ç»§ç»­è¿è¡Œï¼ŒKaggleé¢„è£…åŒ…é€šå¸¸å¯ç”¨")
    
    # 8. åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(KAGGLE_WORKING) / "checkpoints"
    output_dir.mkdir(parents=True, exist_ok=True)
    print_info(f"è¾“å‡ºç›®å½•: {{output_dir}}")
    
    # 9. æ£€æŸ¥GPU
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print_info(f"âœ“ GPUå¯ç”¨: {{gpu_name}} ({{gpu_memory:.1f}} GB)")
        else:
            print_warning("æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
    except ImportError:
        print_warning("PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥GPU")
    
    print_separator()
    return work_project_path

def load_configs(config_dir: Path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    index_file = config_dir / "config_index.json"
    
    if not index_file.exists():
        raise FileNotFoundError(f"é…ç½®ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {{index_file}}")
    
    with open(index_file, 'r', encoding='utf-8') as f:
        index_data = json.load(f)
    
    return index_data["configs"]

def update_config_paths(config_file: Path, kaggle_working: str):
    """æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„ä¸ºKaggleè·¯å¾„"""
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # éœ€è¦æ›´æ–°çš„è·¯å¾„å­—æ®µï¼ˆåŒ…æ‹¬æ‰€æœ‰å¯èƒ½åŒ…å«è¾“å‡ºè·¯å¾„çš„å­—æ®µï¼‰
    path_keys = [
        "checkpoint_path", "save_path", "output_dir",
        "train_info_json", "output_model_path", "pretrained_model_path",
        "ewc_dir", "label_embedding_path", "label_emb_path"
    ]
    
    # æ›´æ–°æ‰€æœ‰è·¯å¾„å­—æ®µ
    def update_path(obj):
        if isinstance(obj, dict):
            for key, value in obj.items():
                if key in path_keys and isinstance(value, str):
                    # å°†checkpointsè·¯å¾„æ›¿æ¢ä¸ºKaggleå·¥ä½œç›®å½•
                    if "checkpoints" in value:
                        new_value = value.replace("checkpoints", f"{{kaggle_working}}/checkpoints")
                        obj[key] = new_value
                        print_info(f"  æ›´æ–°è·¯å¾„: {{key}}")
                        print_info(f"    ä»: {{value}}")
                        print_info(f"    åˆ°: {{new_value}}")
                elif isinstance(value, (dict, list)):
                    update_path(value)
        elif isinstance(obj, list):
            for item in obj:
                update_path(item)
    
    print_info("æ­£åœ¨æ›´æ–°é…ç½®æ–‡ä»¶è·¯å¾„...")
    update_path(config)
    
    # ä¿å­˜æ›´æ–°åçš„é…ç½®
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    return config

def run_experiment(exp_id: int, config_file: Path, task: str, strategy: str, 
                   lr: float, step_size: int, gamma: float):
    """è¿è¡Œå•ä¸ªå®éªŒ"""
    print_separator()
    print_info(f"è¿è¡Œå®éªŒ #{{exp_id}}")
    print_info(f"  ä»»åŠ¡: {{task}}")
    print_info(f"  ç­–ç•¥: {{strategy}}")
    print_info(f"  è¶…å‚æ•°: lr={{lr}}, step_size={{step_size}}, gamma={{gamma}}")
    print_info(f"  é…ç½®æ–‡ä»¶: {{config_file}}")
    print_separator()
    
    # æ›´æ–°é…ç½®æ–‡ä»¶è·¯å¾„
    config = update_config_paths(config_file, KAGGLE_WORKING)
    
    # è¿è¡Œè®­ç»ƒè„šæœ¬
    start_time = time.time()
    
    try:
        # ä½¿ç”¨subprocessè¿è¡Œè®­ç»ƒè„šæœ¬
        cmd = [
            sys.executable, "-m", "scripts.train_with_zero_shot",
            "--config", str(config_file),
            "--start_task", "0",
            "--end_task", "2"
        ]
        
        print_info(f"æ‰§è¡Œå‘½ä»¤: {{' '.join(cmd)}}")
        
        result = subprocess.run(cmd, check=True, capture_output=False, text=True)
        
        elapsed_time = time.time() - start_time
        print_info(f"âœ“ å®éªŒ #{{exp_id}} å®Œæˆ (è€—æ—¶: {{elapsed_time/60:.1f}} åˆ†é’Ÿ)")
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        output_dir = Path(KAGGLE_WORKING) / "checkpoints"
        if output_dir.exists():
            files = list(output_dir.glob("**/*"))
            files = [f for f in files if f.is_file()]
            print_info(f"  å·²ä¿å­˜ {{len(files)}} ä¸ªæ–‡ä»¶åˆ° {{output_dir}}")
            
            # æ˜¾ç¤ºæœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯æœ¬æ¬¡å®éªŒç”Ÿæˆçš„ï¼‰
            recent_files = sorted(files, key=lambda x: x.stat().st_mtime, reverse=True)[:5]
            if recent_files:
                print_info(f"  æœ€è¿‘ç”Ÿæˆçš„æ–‡ä»¶:")
                for f in recent_files:
                    size_mb = f.stat().st_size / (1024 * 1024)
                    rel_path = f.relative_to(output_dir)
                    print_info(f"    - {{rel_path}} ({{size_mb:.2f}} MB)")
        else:
            print_warning(f"  âš ï¸  è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {{output_dir}}")
            print_warning(f"  æ–‡ä»¶å¯èƒ½è¢«ä¿å­˜åˆ°äº†å…¶ä»–ä½ç½®ï¼Œè¯·æ£€æŸ¥é…ç½®")
        
        return True
        
    except subprocess.CalledProcessError as e:
        elapsed_time = time.time() - start_time
        print_error(f"âœ— å®éªŒ #{{exp_id}} å¤±è´¥ (è€—æ—¶: {{elapsed_time/60:.1f}} åˆ†é’Ÿ)")
        print_error(f"é”™è¯¯ä¿¡æ¯: {{e}}")
        return False
    except Exception as e:
        print_error(f"âœ— å®éªŒ #{{exp_id}} å‘ç”ŸæœªçŸ¥é”™è¯¯: {{e}}")
        return False

def save_progress(completed_experiments: list, output_file: Path):
    """ä¿å­˜è¿›åº¦"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({{
            "completed_experiments": completed_experiments,
            "total_completed": len(completed_experiments),
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }}, f, indent=2)
    
    print_info(f"è¿›åº¦å·²ä¿å­˜: {{output_file}}")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Kaggleè¶…å‚æ•°æœç´¢å®éªŒè¿è¡Œå™¨")
    parser.add_argument("--start_exp", type=int, default=1, help="èµ·å§‹å®éªŒID")
    parser.add_argument("--end_exp", type=int, default={max_experiments}, help="ç»“æŸå®éªŒID")
    parser.add_argument("--config_dir", type=str, default=None, help="é…ç½®æ–‡ä»¶ç›®å½•")
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒ
    project_path = setup_environment()
    
    # ç¡®å®šé…ç½®æ–‡ä»¶ç›®å½•
    if args.config_dir:
        config_dir = Path(args.config_dir)
    else:
        config_dir = project_path / "scripts/configs/kaggle_hyperparam_search"
    
    if not config_dir.exists():
        print_error(f"é…ç½®ç›®å½•ä¸å­˜åœ¨: {{config_dir}}")
        print_info("å°è¯•æŸ¥æ‰¾é…ç½®æ–‡ä»¶...")
        # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
        alt_paths = [
            project_path / "configs/kaggle_hyperparam_search",
            Path(KAGGLE_INPUT) / PROJECT_NAME / "scripts/configs/kaggle_hyperparam_search",
        ]
        for alt_config_dir in alt_paths:
            if alt_config_dir.exists():
                config_dir = alt_config_dir
                print_info(f"æ‰¾åˆ°é…ç½®ç›®å½•: {{config_dir}}")
                break
        else:
            raise FileNotFoundError(f"æ— æ³•æ‰¾åˆ°é…ç½®ç›®å½•")
    
    print_info(f"é…ç½®ç›®å½•: {{config_dir}}")
    
    # åŠ è½½é…ç½®
    configs = load_configs(config_dir)
    print_info(f"åŠ è½½äº† {{len(configs)}} ä¸ªé…ç½®")
    
    # å‡†å¤‡è¿è¡Œå®éªŒ
    completed_experiments = []
    failed_experiments = []
    
    progress_file = Path(KAGGLE_WORKING) / "experiment_progress.json"
    
    # å¦‚æœå­˜åœ¨è¿›åº¦æ–‡ä»¶ï¼ŒåŠ è½½ä¹‹å‰çš„è¿›åº¦
    if progress_file.exists():
        with open(progress_file, 'r', encoding='utf-8') as f:
            progress_data = json.load(f)
            completed_experiments = progress_data.get("completed_experiments", [])
        print_info(f"ä»è¿›åº¦æ–‡ä»¶æ¢å¤ï¼Œå·²å®Œæˆ {{len(completed_experiments)}} ä¸ªå®éªŒ")
    
    # è¿è¡Œå®éªŒ
    total_time_start = time.time()
    
    for i, config_info in enumerate(configs, 1):
        # æ£€æŸ¥å®éªŒèŒƒå›´
        if i < args.start_exp or i > args.end_exp:
            continue
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        if i in completed_experiments:
            print_info(f"å®éªŒ #{{i}} å·²å®Œæˆï¼Œè·³è¿‡")
            continue
        
        # è¿è¡Œå®éªŒ
        config_file = Path(config_info["file"])
        # å¦‚æœconfig_fileæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not config_file.is_absolute():
            config_file = project_path / config_file
        
        success = run_experiment(
            exp_id=i,
            config_file=config_file,
            task=config_info["task"],
            strategy=config_info["strategy"],
            lr=config_info["lr"],
            step_size=config_info["step_size"],
            gamma=config_info["gamma"]
        )
        
        if success:
            completed_experiments.append(i)
        else:
            failed_experiments.append(i)
        
        # ä¿å­˜è¿›åº¦
        save_progress(completed_experiments, progress_file)
        
        # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
        elapsed = time.time() - total_time_start
        remaining = args.end_exp - i
        avg_time_per_exp = elapsed / len(completed_experiments) if completed_experiments else 0
        estimated_remaining = remaining * avg_time_per_exp if avg_time_per_exp > 0 else 0
        
        print_info(f"æ€»ä½“è¿›åº¦: {{len(completed_experiments)}}/{{args.end_exp - args.start_exp + 1}} å®Œæˆ")
        print_info(f"å·²ç”¨æ—¶: {{elapsed/3600:.1f}} å°æ—¶")
        if estimated_remaining > 0:
            print_info(f"é¢„è®¡å‰©ä½™: {{estimated_remaining/3600:.1f}} å°æ—¶")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    print_separator()
    print_info("å®éªŒå®Œæˆï¼")
    print_info(f"æˆåŠŸ: {{len(completed_experiments)}} ä¸ª")
    print_info(f"å¤±è´¥: {{len(failed_experiments)}} ä¸ª")
    print_info(f"æ€»è€—æ—¶: {{(time.time() - total_time_start)/3600:.1f}} å°æ—¶")
    
    if failed_experiments:
        print_warning(f"å¤±è´¥çš„å®éªŒID: {{failed_experiments}}")
    
    print_info(f"ç»“æœä¿å­˜åœ¨: {{KAGGLE_WORKING}}/checkpoints")
    print_separator()
    
    # è‡ªåŠ¨æ‰“åŒ…ç»“æœ
    print_separator()
    print_info("æ­£åœ¨æ£€æŸ¥å¹¶æ‰“åŒ…å®éªŒç»“æœ...")
    output_dir = Path(KAGGLE_WORKING) / "checkpoints"
    
    # è¯¦ç»†æ£€æŸ¥è¾“å‡ºç›®å½•
    print_info(f"æ£€æŸ¥è¾“å‡ºç›®å½•: {{output_dir}}")
    
    if not output_dir.exists():
        print_error(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {{output_dir}}")
        print_info("å°è¯•æ£€æŸ¥å…¶ä»–å¯èƒ½çš„ä½ç½®...")
        
        # æ£€æŸ¥é¡¹ç›®ç›®å½•ä¸‹çš„checkpoints
        project_checkpoints = Path("/MCM/checkpoints")
        if project_checkpoints.exists():
            files = list(project_checkpoints.glob("**/*"))
            files = [f for f in files if f.is_file()]
            print_warning(f"å‘ç°æ–‡ä»¶è¢«ä¿å­˜åˆ°äº†é¡¹ç›®ç›®å½•: {{project_checkpoints}}")
            print_warning(f"  å…± {{len(files)}} ä¸ªæ–‡ä»¶")
            if files:
                print_info("  æ–‡ä»¶åˆ—è¡¨:")
                for f in files[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                    print_info(f"    - {{f.relative_to(project_checkpoints)}}")
                print_error("âŒ è·¯å¾„é…ç½®æœ‰é—®é¢˜ï¼æ–‡ä»¶åº”è¯¥ä¿å­˜åˆ° /kaggle/working/checkpoints")
                print_error("   ä½†å®é™…ä¿å­˜åˆ°äº† /MCM/checkpoints")
    else:
        files = list(output_dir.glob("**/*"))
        files = [f for f in files if f.is_file()]
        print_info(f"âœ“ è¾“å‡ºç›®å½•å­˜åœ¨ï¼Œå…± {{len(files)}} ä¸ªæ–‡ä»¶")
        
        if files:
            print_info("æ–‡ä»¶åˆ—è¡¨:")
            for f in files[:20]:  # æ˜¾ç¤ºå‰20ä¸ª
                size_mb = f.stat().st_size / (1024 * 1024)
                rel_path = f.relative_to(output_dir)
                print_info(f"  - {{rel_path}} ({{size_mb:.2f}} MB)")
            if len(files) > 20:
                print_info(f"  ... è¿˜æœ‰ {{len(files) - 20}} ä¸ªæ–‡ä»¶")
    
    if output_dir.exists() and any(output_dir.iterdir()):
        try:
            print_info("å¼€å§‹æ‰“åŒ…...")
            archive_path = Path(KAGGLE_WORKING) / "experiment_results"
            shutil.make_archive(str(archive_path), 'zip', output_dir)
            
            archive_file = Path(f"{{archive_path}}.zip")
            if archive_file.exists():
                size_mb = archive_file.stat().st_size / (1024 * 1024)
                print_info(f"âœ“ ç»“æœå·²æ‰“åŒ…: {{archive_file}}")
                print_info(f"  æ–‡ä»¶å¤§å°: {{size_mb:.1f}} MB")
                print_info(f"  è¯·åœ¨å³ä¾§ 'Output' æ ‡ç­¾é¡µä¸‹è½½ experiment_results.zip")
            else:
                print_error("æ‰“åŒ…å¤±è´¥ï¼šæœªç”Ÿæˆzipæ–‡ä»¶")
        except Exception as e:
            print_error(f"æ‰“åŒ…å¤±è´¥: {{e}}")
    else:
        print_warning("è¾“å‡ºç›®å½•ä¸ºç©ºï¼Œæ²¡æœ‰ç»“æœéœ€è¦æ‰“åŒ…")
    
    print_separator()
    print_info("=" * 80)
    print_info("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
    print_info("=" * 80)
    print_info("")
    print_info("ğŸ“¦ ç»“æœå·²æ‰“åŒ…ï¼Œè¯·ä¸‹è½½ experiment_results.zip")
    print_info("")
    print_warning("âš ï¸  ä¸ºèŠ‚çœGPUé…é¢ï¼Œè¯·ç«‹å³æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š")
    print_warning("   1. åœ¨å³ä¾§ 'Output' æ ‡ç­¾ä¸‹è½½ experiment_results.zip")
    print_warning("   2. ç‚¹å‡»å³ä¸Šè§’ 'Stop Session' æŒ‰é’®åœæ­¢Notebook")
    print_warning("   3. æˆ–è€…ç­‰å¾…æ­¤è„šæœ¬è‡ªåŠ¨é€€å‡ºåæ‰‹åŠ¨åœæ­¢")
    print_info("")
    print_separator()
    
    # ç­‰å¾…å‡ ç§’è®©ç”¨æˆ·çœ‹åˆ°æ¶ˆæ¯
    print_info("ç­‰å¾…10ç§’åè‡ªåŠ¨é€€å‡º...")
    for i in range(10, 0, -1):
        print(f"  {{i}}...", end='\\r')
        time.sleep(1)
    
    print_info("âœ“ è„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼Œè¯·æ‰‹åŠ¨åœæ­¢Sessionä»¥é‡Šæ”¾GPUèµ„æº")
    print_separator()

if __name__ == "__main__":
    main()
''')
    
    script_path.chmod(0o755)


def _generate_deployment_guide(readme_path: Path, dataset_name: str, total_configs: int, max_per_session: int):
    """ç”ŸæˆKaggleéƒ¨ç½²æŒ‡å—"""
    
    with open(readme_path, 'w', encoding='utf-8', newline='\n') as f:
        f.write(f'''# Kaggleéƒ¨ç½²æŒ‡å— - MCMé¡¹ç›®è¶…å‚æ•°æœç´¢

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨Kaggleä¸Šè¿è¡ŒMCMé¡¹ç›®çš„è¶…å‚æ•°æœç´¢å®éªŒã€‚

## ğŸ“‹ ç›®å½•

1. [å‰æœŸå‡†å¤‡](#å‰æœŸå‡†å¤‡)
2. [é¡¹ç›®æ‰“åŒ…ä¸Šä¼ ](#é¡¹ç›®æ‰“åŒ…ä¸Šä¼ )
3. [åˆ›å»ºKaggle Notebook](#åˆ›å»ºkaggle-notebook)
4. [è¿è¡Œå®éªŒ](#è¿è¡Œå®éªŒ)
5. [ç»“æœä¸‹è½½](#ç»“æœä¸‹è½½)
6. [æ³¨æ„äº‹é¡¹](#æ³¨æ„äº‹é¡¹)
7. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

## ğŸ”§ å‰æœŸå‡†å¤‡

### 1. æ£€æŸ¥é¡¹ç›®ç»“æ„

ç¡®ä¿ä½ çš„é¡¹ç›®åŒ…å«ä»¥ä¸‹å…³é”®æ–‡ä»¶ï¼š
- `requirements.txt` - Pythonä¾èµ–
- `scripts/train_with_zero_shot.py` - è®­ç»ƒè„šæœ¬
- `scripts/configs/kaggle_hyperparam_search/` - é…ç½®æ–‡ä»¶
- æ‰€æœ‰å¿…è¦çš„ä»£ç æ–‡ä»¶ï¼ˆ`models/`, `datasets/`, `continual/` ç­‰ï¼‰

### 2. å‡†å¤‡æ•°æ®é›†

ç¡®ä¿ä»¥ä¸‹æ•°æ®åœ¨é¡¹ç›®ä¸­ï¼š
- `data/twitter2015_images/` - Twitter2015å›¾ç‰‡æ•°æ®
- `data/MNER/` - MNERæ•°æ®é›†
- `data/MNRE/` - MNREæ•°æ®é›†  
- `data/MASC/` - MASCæ•°æ®é›†
- `data/MABSA/` - MABSAæ•°æ®é›†ï¼ˆå¦‚æœæœ‰ï¼‰
- `downloaded_model/` - é¢„è®­ç»ƒæ¨¡å‹ï¼ˆDeBERTa, ViTç­‰ï¼‰

### 3. æ¸…ç†ä¸å¿…è¦æ–‡ä»¶

ä¸ºäº†å‡å°ä¸Šä¼ å¤§å°ï¼Œåˆ é™¤ä»¥ä¸‹å†…å®¹ï¼š
```bash
bash scripts/configs/kaggle_hyperparam_search/prepare_for_kaggle.sh
```

è¿™ä¼šè‡ªåŠ¨ï¼š
- åˆ é™¤ `__pycache__/` å’Œ `.pyc` æ–‡ä»¶
- åˆ é™¤å·²æœ‰çš„ `checkpoints/` ï¼ˆç»“æœä¼šåœ¨Kaggleä¸Šé‡æ–°ç”Ÿæˆï¼‰
- åˆ é™¤ `.git/` ç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
- å‹ç¼©é¡¹ç›®ä¸º `MCM_kaggle.zip`

---

## ğŸ“¦ é¡¹ç›®æ‰“åŒ…ä¸Šä¼ 

### æ–¹æ³•1ï¼šä½¿ç”¨å‡†å¤‡è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# è¿è¡Œå‡†å¤‡è„šæœ¬
cd scripts/configs/kaggle_hyperparam_search
bash prepare_for_kaggle.sh

# è„šæœ¬ä¼šç”Ÿæˆ MCM_kaggle.zip
```

### æ–¹æ³•2ï¼šæ‰‹åŠ¨æ‰“åŒ…

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•
cd /path/to/MCM

# æ¸…ç†ç¼“å­˜
find . -type d -name "__pycache__" -exec rm -rf {{}} + 2>/dev/null || true
find . -type f -name "*.pyc" -delete

# æ‰“åŒ…ï¼ˆæ’é™¤ä¸å¿…è¦æ–‡ä»¶ï¼‰
zip -r MCM_kaggle.zip . \\
    -x "*.git*" \\
    -x "*__pycache__*" \\
    -x "*.pyc" \\
    -x "*checkpoints/*" \\
    -x "*.zip"
```

### ä¸Šä¼ åˆ°Kaggleæ•°æ®é›†

1. è®¿é—® [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)
2. ç‚¹å‡» **"New Dataset"**
3. ä¸Šä¼  `MCM_kaggle.zip`
4. è®¾ç½®æ•°æ®é›†åç§°ï¼š`{dataset_name}` ï¼ˆæˆ–ä½ å–œæ¬¢çš„åç§°ï¼‰
5. é€‰æ‹© **Private** ï¼ˆç§æœ‰æ•°æ®é›†ï¼‰
6. ç‚¹å‡» **"Create"**

âš ï¸ **æ³¨æ„**ï¼šKaggleæ•°æ®é›†ä¸Šä¼ åä¼šè‡ªåŠ¨è§£å‹ï¼Œæ‰€ä»¥ä½ çš„é¡¹ç›®æ–‡ä»¶ä¼šåœ¨ `/kaggle/input/{dataset_name}/MCM/` æˆ– `/kaggle/input/{dataset_name}/` ä¸‹ã€‚

---

## ğŸ““ åˆ›å»ºKaggle Notebook

### 1. åˆ›å»ºæ–°Notebook

1. è®¿é—® [https://www.kaggle.com/code](https://www.kaggle.com/code)
2. ç‚¹å‡» **"New Notebook"**
3. é€‰æ‹© **Python**
4. è®¾ç½®Notebookæ ‡é¢˜ï¼š`MCM Hyperparameter Search`

### 2. é…ç½®Notebookè®¾ç½®

ç‚¹å‡»å³ä¾§è®¾ç½®é¢æ¿ï¼š

**åŠ é€Ÿå™¨ (Accelerator)**ï¼š
- é€‰æ‹© **GPU P100** ï¼ˆæ¨èï¼‰
- æˆ– **GPU T4** ï¼ˆå¦‚æœP100ä¸å¯ç”¨ï¼‰
- âš ï¸ ä¸è¦é€‰æ‹© TPU

**æŒä¹…åŒ– (Persistence)**ï¼š
- å¦‚æœå¯ç”¨ï¼Œå¼€å¯ **"Enable GPU"** å’Œ **"Internet"**

**æ•°æ®é›† (Data)**ï¼š
- ç‚¹å‡» **"Add Data"**
- æœç´¢å¹¶æ·»åŠ ä½ ä¸Šä¼ çš„æ•°æ®é›†ï¼š`{dataset_name}`
- æ•°æ®é›†ä¼šæŒ‚è½½åˆ° `/kaggle/input/{dataset_name}/`

### 3. Notebookä»£ç 

åœ¨ç¬¬ä¸€ä¸ªCellä¸­ç²˜è´´ä»¥ä¸‹ä»£ç ï¼š

```python
# Cell 1: ç¯å¢ƒè®¾ç½®å’Œé¡¹ç›®å¤åˆ¶
import os
import sys
import shutil
from pathlib import Path

# æ£€æŸ¥é¡¹ç›®è·¯å¾„
print("æ£€æŸ¥æ•°æ®é›†è·¯å¾„...")
print("å¯ç”¨æ•°æ®é›†:", os.listdir("/kaggle/input"))

# æ‰¾åˆ°é¡¹ç›®è·¯å¾„
dataset_name = "{dataset_name}"
possible_paths = [
    f"/kaggle/input/{{dataset_name}}/MCM",
    f"/kaggle/input/{{dataset_name}}",
]

project_source = None
for path in possible_paths:
    if os.path.exists(path):
        project_source = Path(path)
        print(f"âœ“ æ‰¾åˆ°é¡¹ç›®: {{path}}")
        break

if project_source is None:
    raise FileNotFoundError("æœªæ‰¾åˆ°MCMé¡¹ç›®ï¼")

# å¤åˆ¶åˆ°å·¥ä½œç›®å½•
work_dir = Path("/kaggle/working/MCM")
if not work_dir.exists():
    print("å¤åˆ¶é¡¹ç›®åˆ°å·¥ä½œç›®å½•...")
    shutil.copytree(project_source, work_dir)
    print("âœ“ å¤åˆ¶å®Œæˆ")

# åˆ‡æ¢å·¥ä½œç›®å½•
os.chdir(work_dir)
sys.path.insert(0, str(work_dir))
print(f"å½“å‰å·¥ä½œç›®å½•: {{os.getcwd()}}")
```

```python
# Cell 2: å®‰è£…ä¾èµ–
!pip install -q transformers datasets torch torchvision pillow numpy pandas scikit-learn matplotlib seaborn tqdm

print("âœ“ ä¾èµ–å®‰è£…å®Œæˆ")
```

```python
# Cell 3: æ£€æŸ¥GPU
import torch

if torch.cuda.is_available():
    print(f"âœ“ GPU: {{torch.cuda.get_device_name(0)}}")
    print(f"  æ˜¾å­˜: {{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}} GB")
else:
    print("âš ï¸ æœªæ£€æµ‹åˆ°GPU")
```

```python
# Cell 4: è¿è¡Œå®éªŒ
# ä½¿ç”¨kaggle_runner.pyè„šæœ¬

# ä»æ•°æ®é›†ä¸­å¤åˆ¶è¿è¡Œè„šæœ¬
runner_script = work_dir / "scripts/configs/kaggle_hyperparam_search/kaggle_runner.py"

if not runner_script.exists():
    print(f"é”™è¯¯: è¿è¡Œè„šæœ¬ä¸å­˜åœ¨: {{runner_script}}")
else:
    # è¿è¡Œå‰5ä¸ªå®éªŒï¼ˆæ ¹æ®æ—¶é—´è°ƒæ•´ï¼‰
    !python {{str(runner_script)}} --start_exp 1 --end_exp 5
```

### 4. è°ƒæ•´å®éªŒèŒƒå›´

æ ¹æ®Kaggleæ—¶é—´é™åˆ¶è°ƒæ•´å®éªŒæ•°é‡ï¼š

| GPUç±»å‹ | å¯ç”¨æ—¶é—´ | å»ºè®®å®éªŒæ•° |
|---------|----------|-----------|
| P100    | 9å°æ—¶    | 3-5ä¸ªå®éªŒ  |
| T4      | 9å°æ—¶    | 2-3ä¸ªå®éªŒ  |

**ä¼°ç®—**ï¼šæ¯ä¸ªå®éªŒçº¦1.5-2å°æ—¶ï¼ˆå–å†³äºä»»åŠ¡å’Œæ•°æ®é›†å¤§å°ï¼‰

---

## ğŸš€ è¿è¡Œå®éªŒ

### æ–¹å¼1ï¼šè¿è¡Œå…¨éƒ¨Cellï¼ˆæ¨èï¼‰

ç‚¹å‡» **"Run All"** æŒ‰é’®

### æ–¹å¼2ï¼šé€ä¸ªCellè¿è¡Œ

ä¾æ¬¡ç‚¹å‡»æ¯ä¸ªCellçš„è¿è¡ŒæŒ‰é’®

### ç›‘æ§è¿›åº¦

- è§‚å¯Ÿè¾“å‡ºæ—¥å¿—
- æ£€æŸ¥ `/kaggle/working/checkpoints/` ç›®å½•
- æŸ¥çœ‹ `experiment_progress.json` äº†è§£è¿›åº¦

### åˆ†æ‰¹è¿è¡Œç­–ç•¥

ç”±äºKaggleæœ‰9-12å°æ—¶æ—¶é—´é™åˆ¶ï¼Œå»ºè®®åˆ†æ‰¹è¿è¡Œï¼š

**ç¬¬1æ‰¹**ï¼ˆå®éªŒ1-5ï¼‰ï¼š
```python
!python kaggle_runner.py --start_exp 1 --end_exp 5
```

**ç¬¬2æ‰¹**ï¼ˆå®éªŒ6-10ï¼‰ï¼š
```python
!python kaggle_runner.py --start_exp 6 --end_exp 10
```

æ¯æ‰¹è¿è¡Œå®Œæˆåï¼š
1. ä¸‹è½½ `/kaggle/working/checkpoints/` åˆ°æœ¬åœ°
2. åˆ›å»ºæ–°çš„Notebookç»§ç»­ä¸‹ä¸€æ‰¹

---

## ğŸ’¾ ç»“æœä¸‹è½½

### ä¸‹è½½æ£€æŸ¥ç‚¹æ–‡ä»¶

åœ¨Notebookçš„æœ€åä¸€ä¸ªCellä¸­ï¼š

```python
# æ‰“åŒ…ç»“æœ
import shutil

output_dir = Path("/kaggle/working/checkpoints")
if output_dir.exists():
    shutil.make_archive("/kaggle/working/results", 'zip', output_dir)
    print("âœ“ ç»“æœå·²æ‰“åŒ…: /kaggle/working/results.zip")
    print(f"  å¤§å°: {{(Path('/kaggle/working/results.zip').stat().st_size / 1e6):.1f}} MB")
```

ç„¶åç‚¹å‡»å³ä¾§ **Output** æ ‡ç­¾é¡µï¼Œä¸‹è½½ `results.zip`

### ä¸‹è½½å•ä¸ªæ–‡ä»¶

ä¹Ÿå¯ä»¥åœ¨Notebookä¸­ç›´æ¥æŸ¥çœ‹å’Œä¸‹è½½å•ä¸ªæ–‡ä»¶ï¼š

```python
# åˆ—å‡ºæ‰€æœ‰ç»“æœæ–‡ä»¶
!ls -lh /kaggle/working/checkpoints/
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### Kaggleé™åˆ¶

1. **è¿è¡Œæ—¶é—´**: 9-12å°æ—¶åä¼šè‡ªåŠ¨ç»ˆæ­¢
   - è§£å†³ï¼šåˆ†æ‰¹è¿è¡Œï¼Œæ¯æ‰¹3-5ä¸ªå®éªŒ
   
2. **ç£ç›˜ç©ºé—´**: ~20GB
   - è§£å†³ï¼šå®šæœŸåˆ é™¤ä¸­é—´ç»“æœï¼Œåªä¿ç•™æœ€ç»ˆæ¨¡å‹

3. **GPUæ˜¾å­˜**: P100çº¦16GB
   - è§£å†³ï¼šå¦‚æœOOMï¼Œå‡å°batch_size

4. **ç½‘ç»œé™åˆ¶**: æŸäº›å¤–éƒ¨èµ„æºå¯èƒ½æ— æ³•è®¿é—®
   - è§£å†³ï¼šå°†é¢„è®­ç»ƒæ¨¡å‹åŒ…å«åœ¨æ•°æ®é›†ä¸­

### è·¯å¾„é—®é¢˜

- Kaggleæ•°æ®é›†æ˜¯**åªè¯»**çš„ï¼ˆ`/kaggle/input/`ï¼‰
- æ‰€æœ‰è¾“å‡ºå¿…é¡»å†™åˆ° `/kaggle/working/`
- é¡¹ç›®ä»£ç å±‚çº§ä¸èƒ½è¶…è¿‡5å±‚ï¼ˆå·²é€šè¿‡å¤åˆ¶åˆ°å·¥ä½œç›®å½•è§£å†³ï¼‰

### æ¨¡å‹ä¿å­˜

é…ç½®æ–‡ä»¶å·²è‡ªåŠ¨å°†checkpointè·¯å¾„è®¾ç½®ä¸ºï¼š
```
/kaggle/working/checkpoints/
```

ä¸éœ€è¦æ‰‹åŠ¨ä¿®æ”¹ã€‚

---

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜1: ModuleNotFoundError: No module named 'scripts'

**åŸå› **: å·¥ä½œç›®å½•ä¸æ­£ç¡®

**è§£å†³**:
```python
import os, sys
os.chdir("/kaggle/working/MCM")
sys.path.insert(0, "/kaggle/working/MCM")
```

### é—®é¢˜2: FileNotFoundError: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨

**åŸå› **: æ•°æ®é›†æœªæ­£ç¡®æŒ‚è½½

**è§£å†³**:
```python
# æ£€æŸ¥æ•°æ®é›†
!ls -la /kaggle/input/
!ls -la /kaggle/input/{dataset_name}/
```

### é—®é¢˜3: CUDA out of memory

**åŸå› **: GPUæ˜¾å­˜ä¸è¶³

**è§£å†³**:
1. ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„ `batch_size`
2. æˆ–åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ ï¼š
```python
torch.cuda.empty_cache()
```

### é—®é¢˜4: è¿è¡Œæ—¶é—´è¶…è¿‡é™åˆ¶

**åŸå› **: Kaggle 9å°æ—¶é™åˆ¶

**è§£å†³**:
- å‡å°‘æ¯æ‰¹å®éªŒæ•°é‡
- ä½¿ç”¨ `--start_exp` å’Œ `--end_exp` å‚æ•°åˆ†æ‰¹è¿è¡Œ

### é—®é¢˜5: æ— æ³•ä¿å­˜ç»“æœ

**åŸå› **: å†™å…¥åªè¯»ç›®å½•

**è§£å†³**:
ç¡®ä¿æ‰€æœ‰è¾“å‡ºè·¯å¾„éƒ½åœ¨ `/kaggle/working/` ä¸‹

---

## ğŸ“Š ç»“æœåˆ†æ

ä¸‹è½½ç»“æœåï¼Œåœ¨æœ¬åœ°è¿è¡Œåˆ†æè„šæœ¬ï¼š

```bash
# è§£å‹ç»“æœ
unzip results.zip -d ./kaggle_results

# è¿è¡Œåˆ†æ
python scripts/configs/kaggle_hyperparam_search/analyze_kaggle_results.py \\
    --results_dir ./kaggle_results
```

---

## ğŸ“ è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜ï¼Ÿ

1. æ£€æŸ¥Kaggle Notebookçš„è¾“å‡ºæ—¥å¿—
2. æŸ¥çœ‹ `/kaggle/working/experiment_progress.json`
3. æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®ä¸Šä¼ 
4. ç¡®è®¤GPUæ˜¯å¦å¯ç”¨

---

## å®éªŒé…ç½®æ€»ç»“

- **æ€»å®éªŒæ•°**: {total_configs}
- **ä»»åŠ¡**: MATE, MNER, MABSA
- **æ¯ä¸ªä»»åŠ¡**: text_only â†’ multimodal
- **è¶…å‚æ•°**: lr, step_size, gamma
- **æ¯æ‰¹å»ºè®®æ•°**: {max_per_session}
- **é¢„è®¡æ€»æ—¶é—´**: çº¦ {total_configs * 1.5 / max_per_session:.0f} ä¸ªKaggleä¼šè¯

---

Good luck! ğŸš€
''')


def _generate_preparation_script(script_path: Path):
    """ç”Ÿæˆé¡¹ç›®å‡†å¤‡è„šæœ¬"""
    
    with open(script_path, 'w', encoding='utf-8', newline='\n') as f:
        f.write('''#!/bin/bash
#===============================================================================
# Kaggleé¡¹ç›®å‡†å¤‡è„šæœ¬
#
# åŠŸèƒ½:
#   1. æ¸…ç†ä¸å¿…è¦çš„æ–‡ä»¶ï¼ˆç¼“å­˜ã€checkpointsç­‰ï¼‰
#   2. å‹ç¼©é¡¹ç›®ä¸º MCM_kaggle.zip
#   3. å‡†å¤‡ä¸Šä¼ åˆ°Kaggleæ•°æ®é›†
#
# ä½¿ç”¨æ–¹æ³•:
#   cd MCM  # è¿›å…¥é¡¹ç›®æ ¹ç›®å½•
#   bash scripts/configs/kaggle_hyperparam_search/prepare_for_kaggle.sh
#===============================================================================

set -e

# é¢œè‰²å®šä¹‰
GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
RED='\\033[0;31m'
BLUE='\\033[0;34m'
NC='\\033[0m'

echo -e "${BLUE}===============================================================================${NC}"
echo -e "${BLUE}Kaggleé¡¹ç›®å‡†å¤‡è„šæœ¬${NC}"
echo -e "${BLUE}===============================================================================${NC}"
echo ""

# æ£€æŸ¥æ˜¯å¦åœ¨é¡¹ç›®æ ¹ç›®å½•
if [ ! -f "requirements.txt" ]; then
    echo -e "${RED}é”™è¯¯: è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬${NC}"
    echo -e "${YELLOW}å½“å‰ç›®å½•: $(pwd)${NC}"
    exit 1
fi

echo -e "${GREEN}âœ“ å½“å‰ç›®å½•: $(pwd)${NC}"
echo ""

# 1. æ¸…ç†Pythonç¼“å­˜
echo -e "${BLUE}[1/5] æ¸…ç†Pythonç¼“å­˜...${NC}"
find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
find . -type f -name "*.pyc" -delete 2>/dev/null || true
find . -type d -name "*.egg-info" -exec rm -rf {} + 2>/dev/null || true
echo -e "${GREEN}âœ“ Pythonç¼“å­˜å·²æ¸…ç†${NC}"
echo ""

# 2. æ¸…ç†checkpointsï¼ˆå¯é€‰ï¼ŒèŠ‚çœç©ºé—´ï¼‰
echo -e "${BLUE}[2/5] æ¸…ç†checkpointsç›®å½•...${NC}"
read -p "æ˜¯å¦åˆ é™¤checkpointsç›®å½•ï¼Ÿ(y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    rm -rf checkpoints/*
    echo -e "${GREEN}âœ“ checkpointså·²æ¸…ç†${NC}"
else
    echo -e "${YELLOW}âŠ˜ ä¿ç•™checkpoints${NC}"
fi
echo ""

# 3. æ¸…ç†æ—¥å¿—æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
echo -e "${BLUE}[3/5] æ¸…ç†æ—¥å¿—æ–‡ä»¶...${NC}"
read -p "æ˜¯å¦åˆ é™¤logç›®å½•ï¼Ÿ(y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    rm -rf log/*
    echo -e "${GREEN}âœ“ æ—¥å¿—å·²æ¸…ç†${NC}"
else
    echo -e "${YELLOW}âŠ˜ ä¿ç•™æ—¥å¿—${NC}"
fi
echo ""

# 4. æ¸…ç†.gitï¼ˆå¯é€‰ï¼Œå¤§å¹…å‡å°ä½“ç§¯ï¼‰
echo -e "${BLUE}[4/5] æ¸…ç†.gitç›®å½•...${NC}"
read -p "æ˜¯å¦åˆ é™¤.gitç›®å½•ï¼Ÿ(è¿™ä¼šåˆ é™¤Gitå†å²ï¼) (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    rm -rf .git
    echo -e "${GREEN}âœ“ .gitå·²æ¸…ç†${NC}"
else
    echo -e "${YELLOW}âŠ˜ ä¿ç•™.git${NC}"
fi
echo ""

# 5. å‹ç¼©é¡¹ç›®
echo -e "${BLUE}[5/5] å‹ç¼©é¡¹ç›®...${NC}"

OUTPUT_ZIP="MCM_kaggle.zip"

# åˆ é™¤æ—§çš„å‹ç¼©åŒ…
if [ -f "$OUTPUT_ZIP" ]; then
    rm "$OUTPUT_ZIP"
fi

# å‹ç¼©ï¼ˆæ’é™¤ä¸å¿…è¦çš„æ–‡ä»¶ï¼‰
zip -r "$OUTPUT_ZIP" . \\
    -x "*.git*" \\
    -x "*__pycache__*" \\
    -x "*.pyc" \\
    -x "*checkpoints/*" \\
    -x "*log/*" \\
    -x "*.zip" \\
    -x "*test_outputs/*" \\
    -x "*.ipynb_checkpoints*" \\
    -q

FILE_SIZE=$(du -h "$OUTPUT_ZIP" | cut -f1)

echo -e "${GREEN}âœ“ é¡¹ç›®å·²å‹ç¼©: $OUTPUT_ZIP (å¤§å°: $FILE_SIZE)${NC}"
echo ""

# æ˜¾ç¤ºåç»­æ­¥éª¤
echo -e "${BLUE}===============================================================================${NC}"
echo -e "${BLUE}å‡†å¤‡å®Œæˆï¼${NC}"
echo -e "${BLUE}===============================================================================${NC}"
echo ""
echo -e "${GREEN}ä¸‹ä¸€æ­¥:${NC}"
echo -e "  1. è®¿é—® https://www.kaggle.com/datasets"
echo -e "  2. ç‚¹å‡» 'New Dataset'"
echo -e "  3. ä¸Šä¼  $OUTPUT_ZIP"
echo -e "  4. è®¾ç½®æ•°æ®é›†åç§°ï¼ˆä¾‹å¦‚: mcm-projectï¼‰"
echo -e "  5. é€‰æ‹© Privateï¼ˆç§æœ‰ï¼‰"
echo -e "  6. ç‚¹å‡» Create"
echo ""
echo -e "${YELLOW}æ³¨æ„:${NC}"
echo -e "  - Kaggleä¼šè‡ªåŠ¨è§£å‹zipæ–‡ä»¶"
echo -e "  - ä½ çš„é¡¹ç›®ä¼šåœ¨ /kaggle/input/<æ•°æ®é›†åç§°>/ ä¸‹"
echo -e "  - ä¸Šä¼ å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå–å†³äºæ–‡ä»¶å¤§å°"
echo ""
echo -e "${BLUE}===============================================================================${NC}"
''')
    
    script_path.chmod(0o755)


def _generate_kaggle_analysis_script(script_path: Path, configs: list):
    """ç”ŸæˆKaggleç»“æœåˆ†æè„šæœ¬"""
    
    with open(script_path, 'w', encoding='utf-8', newline='\n') as f:
        f.write('''#!/usr/bin/env python3
"""
Kaggleè¶…å‚æ•°æœç´¢ç»“æœåˆ†æè„šæœ¬

ä»Kaggleä¸‹è½½çš„ç»“æœç›®å½•ä¸­æå–å’Œåˆ†æå®éªŒç»“æœ
"""

import json
import pandas as pd
from pathlib import Path
import numpy as np
import argparse


def calculate_metrics(acc_matrix):
    """è®¡ç®—æŒç»­å­¦ä¹ æŒ‡æ ‡"""
    n = len(acc_matrix)
    if n == 0:
        return {}
    
    # AA (Average Accuracy)
    aa = np.mean([acc_matrix[n-1][j] for j in range(n)])
    
    # AIA (Average Incremental Accuracy)
    aia_values = []
    for i in range(n):
        avg_acc = np.mean([acc_matrix[i][j] for j in range(i+1)])
        aia_values.append(avg_acc)
    aia = np.mean(aia_values)
    
    # FM (Forgetting Measure)
    fm_values = []
    for j in range(n-1):
        max_acc = max([acc_matrix[i][j] for i in range(j, n)])
        final_acc = acc_matrix[n-1][j]
        fm_values.append(max_acc - final_acc)
    fm = np.mean(fm_values) if fm_values else 0.0
    
    # BWT (Backward Transfer)
    bwt_values = []
    for j in range(n-1):
        final_acc = acc_matrix[n-1][j]
        acc_after_j = acc_matrix[j][j]
        bwt_values.append(final_acc - acc_after_j)
    bwt = np.mean(bwt_values) if bwt_values else 0.0
    
    # FWT (Forward Transfer)
    fwt_values = []
    for j in range(1, n):
        acc_before_j = acc_matrix[j-1][j] if j > 0 else 0.0
        fwt_values.append(acc_before_j)
    fwt = np.mean(fwt_values) if fwt_values else 0.0
    
    return {
        "AA": aa,
        "AIA": aia,
        "FM": fm,
        "BWT": bwt,
        "FWT": fwt
    }


def find_train_info_files(results_dir: Path):
    """æŸ¥æ‰¾æ‰€æœ‰train_infoæ–‡ä»¶"""
    return list(results_dir.glob("**/train_info_*.json"))


def extract_hyperparams_from_filename(filename: str):
    """ä»æ–‡ä»¶åæå–è¶…å‚æ•°ä¿¡æ¯"""
    # æ–‡ä»¶åæ ¼å¼: train_info_twitter2015_none_t2m_hpX.json
    # æˆ‘ä»¬éœ€è¦ä»é…ç½®æ–‡ä»¶æˆ–å…¶ä»–åœ°æ–¹è·å–è¶…å‚æ•°
    return None


def analyze_results(results_dir: Path, output_dir: Path):
    """åˆ†æKaggleç»“æœ"""
    
    print(f"åˆ†æç›®å½•: {results_dir}")
    print()
    
    # æŸ¥æ‰¾æ‰€æœ‰train_infoæ–‡ä»¶
    train_info_files = find_train_info_files(results_dir)
    
    if not train_info_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•train_infoæ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(train_info_files)} ä¸ªç»“æœæ–‡ä»¶")
    print()
    
    all_results = []
    
    for train_info_path in train_info_files:
        print(f"å¤„ç†: {train_info_path.name}", end=" ... ")
        
        try:
            with open(train_info_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            acc_matrix = np.array(data.get("accuracy_matrix", []))
            
            if len(acc_matrix) == 0:
                print("âŒ æ— å‡†ç¡®ç‡çŸ©é˜µ")
                continue
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = calculate_metrics(acc_matrix)
            
            # ä»æ–‡ä»¶åæå–ä¿¡æ¯
            # æ ¼å¼: train_info_<dataset>_<strategy>_<mode>_<seq>.json
            parts = train_info_path.stem.replace("train_info_", "").split("_")
            
            result = {
                "file": train_info_path.name,
                "dataset": parts[0] if len(parts) > 0 else "unknown",
                "strategy": parts[1] if len(parts) > 1 else "unknown",
                "mode": parts[2] if len(parts) > 2 else "unknown",
                "seq": parts[3] if len(parts) > 3 else "unknown",
                **metrics,
                "acc_matrix": acc_matrix.tolist()
            }
            
            # æ·»åŠ ä»»åŠ¡å‡†ç¡®ç‡
            n = len(acc_matrix)
            for i in range(n):
                result[f"Task{i+1}_AfterT{i+1}"] = acc_matrix[i][i]
            for j in range(n):
                result[f"Task{j+1}_Final"] = acc_matrix[n-1][j]
            
            all_results.append(result)
            print("âœ“")
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
    
    if not all_results:
        print("\\næ²¡æœ‰æˆåŠŸæå–çš„ç»“æœ")
        return
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(all_results)
    
    # ä¿å­˜ç»“æœ
    output_dir.mkdir(parents=True, exist_ok=True)
    
    results_csv = output_dir / "kaggle_results_summary.csv"
    df.to_csv(results_csv, index=False, encoding='utf-8')
    print(f"\\nâœ“ ç»“æœå·²ä¿å­˜: {results_csv}")
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print("\\n" + "="*80)
    print("ç»“æœç»Ÿè®¡")
    print("="*80)
    print(f"\\nå¹³å‡ AA: {df['AA'].mean():.4f} Â± {df['AA'].std():.4f}")
    print(f"å¹³å‡ AIA: {df['AIA'].mean():.4f} Â± {df['AIA'].std():.4f}")
    print(f"å¹³å‡ FM: {df['FM'].mean():.4f} Â± {df['FM'].std():.4f}")
    print(f"å¹³å‡ BWT: {df['BWT'].mean():.4f} Â± {df['BWT'].std():.4f}")
    print(f"å¹³å‡ FWT: {df['FWT'].mean():.4f} Â± {df['FWT'].std():.4f}")
    
    # æ˜¾ç¤ºæœ€ä½³ç»“æœ
    print("\\n" + "="*80)
    print("æœ€ä½³ç»“æœ (æŒ‰AAæ’åº)")
    print("="*80)
    
    df_sorted = df.sort_values("AA", ascending=False)
    print("\\nTop 5:")
    for idx, row in df_sorted.head(5).iterrows():
        print(f"  {row['file']}: AA={row['AA']:.4f}, FM={row['FM']:.4f}")
    
    print("\\nåˆ†æå®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(description="åˆ†æKaggleå®éªŒç»“æœ")
    parser.add_argument("--results_dir", type=str, required=True,
                       help="Kaggleç»“æœç›®å½•ï¼ˆè§£å‹åçš„checkpointsç›®å½•ï¼‰")
    parser.add_argument("--output_dir", type=str, default="./kaggle_analysis",
                       help="åˆ†æç»“æœè¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    results_dir = Path(args.results_dir)
    output_dir = Path(args.output_dir)
    
    if not results_dir.exists():
        print(f"é”™è¯¯: ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        return
    
    analyze_results(results_dir, output_dir)


if __name__ == "__main__":
    main()
''')
    
    script_path.chmod(0o755)


if __name__ == "__main__":
    main()

