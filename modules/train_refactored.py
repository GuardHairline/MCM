# modules/train_refactored.py
import os
import torch
import torch.multiprocessing as mp
from torch.utils.data import DataLoader

# è®¾ç½®æ–‡ä»¶ç³»ç»Ÿå…±äº«ç­–ç•¥ï¼Œè§£å†³"Too many open files"é—®é¢˜
mp.set_sharing_strategy('file_system')

os.environ["TOKENIZERS_PARALLELISM"] = "false"
import warnings
warnings.filterwarnings(
    "ignore",
    message=".*byte fallback option which is not implemented in the fast tokenizers.*",
    category=UserWarning,
    module="transformers.convert_slow_tokenizer"
)
warnings.filterwarnings(
    "ignore",
    message=".*TypedStorage is deprecated.*",
    category=UserWarning,
    module="torch._utils"
)

from datasets.get_dataset import get_dataset
from modules.evaluate import evaluate_single_task, evaluate_all_learned_tasks
from .train_utils import (
    load_train_info, create_model, create_continual_learning_components,
    create_session_info, save_train_info, create_optimizer, create_ddas_optimizer,
    create_scheduler
)
from .training_loop_fixed import train_model, update_continual_learning_components
from .parser import parse_train_args
from continual.label_embedding import (
    build_global_label_mapping, create_label_groups, get_label_text_mapping, generate_label_embeddings, GlobalLabelEmbedding
)
from continual.label_embedding_manager import LabelEmbeddingManager
from continual.moe_adapters.freeze_topk_experts import freeze_topk_experts
from continual.metrics import ContinualMetrics
from utils.logger import setup_logger
from utils.ensureFileExists import ensure_directory_exists
from visualize.feature_clustering import visualize_task_after_training, visualize_all_tasks_evolution
from visualize.feature_clustering_enhanced import visualize_task_enhanced
from visualize.training_curves import plot_training_curves
import json
import argparse


def train(args, logger, all_tasks=[]):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æå–é…ç½®æ–‡ä»¶åï¼ˆç”¨äºå¯è§†åŒ–æ–‡ä»¶å‘½åï¼Œé¿å…ä¸åŒé…ç½®äº’ç›¸è¦†ç›–ï¼‰
    config_name = None
    if hasattr(args, 'task_config_file') and args.task_config_file:
        from pathlib import Path
        config_name = Path(args.task_config_file).stem  # æå–æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰
        logger.info(f"é…ç½®æ–‡ä»¶å: {config_name}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    ensure_directory_exists(args.train_info_json)
    ensure_directory_exists(args.ewc_dir)
    ensure_directory_exists(args.output_model_path)
    
    logger.info(f"=== Start training for new task: {args.task_name} ===")
    
    # ========== 1) åŠ è½½è®­ç»ƒä¿¡æ¯ ==========
    train_info = load_train_info(args.train_info_json)
    old_sessions_count = len(train_info["sessions"])
    args.old_sessions_count = old_sessions_count
    logger.info(f"Previously learned sessions: {old_sessions_count}")
    
    # ========== 1.5) åŠ è½½ä»»åŠ¡é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰ç”¨äº0æ ·æœ¬æ£€æµ‹ ==========
    task_config = None
    future_tasks = []
    if hasattr(args, 'task_config_file') and args.task_config_file:
        logger.info(f"Loading task configuration from: {args.task_config_file}")
        with open(args.task_config_file, 'r', encoding='utf-8') as f:
            task_config = json.load(f)
        
        # æ‰¾åˆ°å½“å‰ä»»åŠ¡åœ¨åºåˆ—ä¸­çš„ä½ç½®
        current_task_idx = None
        for i, task in enumerate(task_config['tasks']):
            if task['task_name'] == args.task_name and task['session_name'] == args.session_name:
                current_task_idx = i
                break
        
        if current_task_idx is not None:
            # è·å–åç»­ä»»åŠ¡ä¿¡æ¯ç”¨äº0æ ·æœ¬æ£€æµ‹
            future_tasks = task_config['tasks'][current_task_idx + 1:]
            logger.info(f"Found {len(future_tasks)} future tasks for zero-shot evaluation")
            for i, task in enumerate(future_tasks):
                logger.info(f"  Future task {i+1}: {task['task_name']} ({task['session_name']})")
    
    # ========== 2) åˆå§‹åŒ–æ ‡ç­¾åµŒå…¥ç®¡ç†å™¨ ==========
    label_embedding_manager = None
    if args.use_label_embedding:
        logger.info("Initializing label embedding manager")
        # è‡ªåŠ¨ç”Ÿæˆlabel embeddingï¼ˆå¦‚ä¸å­˜åœ¨ï¼‰
        if not args.label_embedding_path or not os.path.exists(args.label_embedding_path):
            logger.info("No existing label embedding found, generating with deberta-v3-base")
            label2idx = build_global_label_mapping()
            label_texts = get_label_text_mapping()
            pretrained_embeddings = generate_label_embeddings(
                label_texts, emb_dim=args.label_emb_dim, device="cuda" if torch.cuda.is_available() else "cpu"
            )
            label_groups = create_label_groups()
            gle = GlobalLabelEmbedding(
                label2idx=label2idx,
                emb_dim=args.label_emb_dim,
                label_groups=label_groups,
                use_similarity_regularization=args.use_similarity_reg,
                similarity_weight=args.similarity_weight,
                pretrained_embeddings=pretrained_embeddings
            )
            gle.export(args.label_embedding_path)
            logger.info(f"Label embedding generated and saved to {args.label_embedding_path}")
        # æ­£å¸¸åŠ è½½
        label_embedding_manager = LabelEmbeddingManager(
            emb_dim=args.label_emb_dim,
            use_similarity_regularization=args.use_similarity_reg,
            similarity_weight=args.similarity_weight
        )
        label_embedding_manager.create_or_load_embedding(args.label_embedding_path, device)
        label_embedding_manager.print_label_mapping()

        # å†»ç»“æ—§ä»»åŠ¡æ ‡ç­¾
        emb_obj = label_embedding_manager.get_embedding()
        if emb_obj is not None:
            emb_obj.freeze_seen_labels(args.task_name, args.num_labels)
    
    # ========== 3) åˆ›å»ºæ¨¡å‹ ==========
    logger.info("Creating model")
    full_model = create_model(args, device, label_embedding_manager, logger)

    # æ³¨å†Œå½“å‰ä»»åŠ¡çš„å¤´ï¼ˆç¡®ä¿ä¼˜åŒ–å™¨/åˆ‡æ¢å¯ç”¨ï¼‰
    current_head_key = getattr(args, 'head_key', args.session_name)
    # è‹¥å…±äº«å¤´å·²å­˜åœ¨ï¼ˆé€šè¿‡ head_keyï¼‰ï¼Œä¸é‡å¤æ³¨å†Œ
    if not (full_model.head_manager.has_head(args.session_name) or full_model.head_manager.has_head(current_head_key)):
        full_model.add_task_head(args.session_name, args.task_name, full_model.head, args)
    # è®¾ç½®å½“å‰æ´»åŠ¨å¤´
    try:
        full_model.set_active_head(args.session_name, strict=False)
    except Exception:
        pass
    
    # ========== 3.5) åªä¸ºå†å²ä»»åŠ¡åˆ›å»ºæ¨¡å‹å¤´ï¼ˆå»¶è¿Ÿåˆ›å»ºæ¨¡å¼ï¼‰ ==========
    # æ³¨æ„ï¼šä¸å†ä¸ºæœªæ¥ä»»åŠ¡é¢„åˆ›å»ºheadï¼Œåªåœ¨éœ€è¦æ—¶åˆ›å»º
    if all_tasks is not None and not args.tam_cl:
        # åªä¸ºå·²ç»å­¦ä¹ è¿‡çš„ä»»åŠ¡åŠ è½½headï¼ˆä»train_infoä¸­è·å–ï¼‰
        learned_sessions = set(s['session_name'] for s in train_info.get('sessions', []))
        logger.info(f"Loading heads for {len(learned_sessions)} previously learned sessions")
        
        for task in all_tasks:
            session_name = task['session_name']
            task_name = task['task_name']
            
            # åªå¤„ç†å†å²ä»»åŠ¡
            if session_name not in learned_sessions:
                continue
            
            # å¦‚æœheadå·²å­˜åœ¨ï¼Œè·³è¿‡
            if full_model.head_manager.has_head(session_name):
                logger.debug(f"Head for {session_name} already exists, skipping")
                continue
            
            try:
                task_args = argparse.Namespace(**task)
                if not hasattr(task_args, 'head_key'):
                    task_args.head_key = session_name
                use_label_embedding = getattr(task_args, 'use_label_embedding', False)
                
                # ä½¿ç”¨TaskHeadManageråˆ›å»ºhead
                logger.info(f"Creating head for historical task: {session_name} ({task_name})")
                head_key = getattr(task_args, 'head_key', session_name)
                head = full_model.head_manager.create_and_register_head(
                    session_name, task_name, task_args, use_label_embedding, head_key=head_key
                )
                
                if head is None:
                    logger.warning(f"Failed to create head for {session_name}")
                    
            except Exception as e:
                logger.warning(f"Error creating head for {session_name}: {e}")
                continue
        
        logger.info(f"Historical task heads loaded: {full_model.head_manager.get_head_count()}")
    elif args.tam_cl:
        logger.info("TAM-CL: Using task-specific adapters instead of separate heads")
    
    # ========== 3.6) MoE-Adapters: ä¸ºæ–°ä»»åŠ¡æ·»åŠ ä¸“å®¶ ==========
    if args.moe_adapters:
        logger.info("MoE-Adapters: Adding new expert for current task")
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ªä»»åŠ¡
        is_first_task = len(train_info.get('sessions', [])) == 0
        
        if is_first_task:
            logger.info("  First task: Expert already created during model initialization")
        else:
            logger.info(f"  Task {len(train_info['sessions']) + 1}: Calling start_new_task() to add new expert")
            # è°ƒç”¨MoeAdapterWrapperçš„start_new_taskæ–¹æ³•
            if hasattr(full_model.base_model, 'start_new_task'):
                full_model.base_model.start_new_task()
                logger.info("  âœ“ New expert added and old experts frozen")
            else:
                logger.warning("  âœ— base_model does not have start_new_task method!")
    
    # ========== 4) åˆ›å»ºæŒç»­å­¦ä¹ ç»„ä»¶ ==========
    logger.info("Creating continual learning components")
    ewc, fisher_selector, replay_memory, lwf, si, mas, gem, pnn = create_continual_learning_components(
        args, full_model, train_info, device, logger
    )
    
    # ========== 5) åŠ è½½æ•°æ® ==========
    logger.info("Loading datasets")
    train_dataset = get_dataset(args.task_name, "train", args)
    val_dataset = get_dataset(args.task_name, "dev", args)
    test_dataset = get_dataset(args.task_name, "test", args)
    
    # åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸­å‡å°‘workeræ•°é‡ä»¥é¿å…æ–‡ä»¶æè¿°ç¬¦é—®é¢˜
    num_workers = min(args.num_workers, 2) if os.environ.get('SERVER_ENV') else args.num_workers
    
    train_loader = DataLoader(
        train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=num_workers
    )
    val_loader = DataLoader(
        val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=num_workers
    )
    test_loader = DataLoader(
        test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=num_workers
    )
    # ========== 5.5) å¦‚æœä½¿ç”¨ GEMï¼Œæ³¨å†Œå½“å‰ä»»åŠ¡çš„è®°å¿†æ ·æœ¬ ==========
    if gem is not None:
        gem.register_task(args.task_name, train_dataset)
    # ========== 6) åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ==========
    optimizer = create_optimizer(full_model, args)
    total_training_steps = len(train_loader) * args.epochs if len(train_loader) > 0 else args.epochs
    scheduler = create_scheduler(optimizer, args, total_training_steps)
    
    # ========== 7) è®­ç»ƒæ¨¡å‹ ==========
    logger.info("Starting training")
    train_result = train_model(
        model=full_model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        args=args,
        ewc=ewc,
        fisher_selector=fisher_selector,
        replay_memory=replay_memory,
        lwf=lwf,
        si=si,
        mas=mas,
        gem=gem,
        label_embedding_manager=label_embedding_manager,
        logger=logger
    )
    # train_result æ˜¯ dictï¼ŒåŒ…å«æ‰€æœ‰éœ€è¦çš„å†…å®¹
    
    # ========== 8) ä¿å­˜æ ‡ç­¾åµŒå…¥ ==========
    if label_embedding_manager and args.label_embedding_path:
        label_embedding_manager.save_embedding(args.label_embedding_path)
    
    # ========== 9) æ›´æ–°æŒç»­å­¦ä¹ ç»„ä»¶ ==========
    session_info = create_session_info(args)
    session_info = update_continual_learning_components(
        full_model, train_loader, device, args,
        ewc, fisher_selector, si, mas, gem, session_info, logger
    )
    if args.moe_adapters and hasattr(full_model.base_model, 'text_adapters'):
    # å‡è®¾éœ€è¦å†»ç»“æ¯å±‚ä¸€ä¸ªä¸“å®¶ï¼Œå¯ç”¨ args.freeze_topk_experts å‚æ•°é…ç½®
        freeze_topk = getattr(args, 'freeze_topk_experts', 1)
        freeze_topk_experts(full_model, freeze_topk)
    # ========== 10) è¯„ä¼°å’Œæ›´æ–°è®­ç»ƒä¿¡æ¯ ==========
    logger.info("Evaluating model")
    # è¯„ä¼°å½“å‰ä»»åŠ¡ï¼ˆä½¿ç”¨DEVé›†ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼ŒTESTé›†ä»…ç”¨äºè®°å½•ï¼‰
    current_dev_metrics = train_result["final_dev_metrics"]
    current_test_metrics = train_result["final_test_metrics"]
    logger.info(f"Current task DEV metrics: {current_dev_metrics['acc']:.4f}")
    logger.info(f"Current task TEST metrics (reference only): {current_test_metrics['acc']:.4f}")
    
    # ========== 10.5) 0æ ·æœ¬æ£€æµ‹åç»­ä»»åŠ¡ï¼ˆä½¿ç”¨DEVé›†ï¼Œä¸ä½¿ç”¨TESTé›†ï¼‰ ==========
    zero_shot_metrics = {}
    if future_tasks:
        logger.info("Performing zero-shot evaluation on future tasks (using DEV set)...")
        logger.info("âš ï¸  IMPORTANT: Creating temporary task heads with random weights for zero-shot evaluation")
        logger.info("   (Different tasks have different label spaces, cannot use current task's head!)")
        
        # âœ… ä¿®å¤å®Œæˆï¼šDEQAç°åœ¨ä¸æ¡†æ¶å®Œå…¨å…¼å®¹
        # DEQAä½¿ç”¨ï¼šDEQAä¸“å®¶èåˆç‰¹å¾ + TaskHeadè¾“å‡ºlogits
        # æ™®é€šæ¨¡å‹ä½¿ç”¨ï¼šBaseModelç‰¹å¾ + TaskHeadè¾“å‡ºlogits
        # ä¸¤è€…éƒ½ä½¿ç”¨ç›¸åŒçš„head_manageræœºåˆ¶ï¼
        
        from models.deqa_expert_model import DEQAMultimodalModel
        is_deqa = isinstance(full_model, DEQAMultimodalModel)
        if is_deqa:
            logger.info("   (DEQAæ¨¡å‹: ä½¿ç”¨DEQAä¸“å®¶èåˆç‰¹å¾ + ä¸´æ—¶éšæœºhead)")
        
        for future_task in future_tasks:
            session_name = future_task['session_name']
            task_name = future_task['task_name']
            logger.info(f"Zero-shot evaluation on: {session_name} (task: {task_name})")
            
            try:
                # åˆ›å»ºæœªæ¥ä»»åŠ¡çš„å‚æ•°å¯¹è±¡
                future_args = argparse.Namespace(**future_task)
                if not hasattr(future_args, 'head_key'):
                    future_args.head_key = session_name
                future_args.task_name = task_name
                future_args.session_name = session_name
                
                # ğŸ”‘ å…³é”®ï¼šä¸ºæœªæ¥ä»»åŠ¡ä¸´æ—¶åˆ›å»ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„head
                # åŸå› ï¼šä¸åŒä»»åŠ¡çš„æ ‡ç­¾ç©ºé—´ä¸åŒï¼
                # ä¾‹å¦‚ï¼šMASCçš„0=NEGï¼Œä½†MATEçš„0=Oï¼Œå«ä¹‰å®Œå…¨ä¸åŒ
                logger.info(f"  Step 1: Creating temporary random head for {session_name}")
                logger.info(f"          Task: {task_name}, Labels: {future_args.num_labels}")
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨è¿™ä¸ªhead
                head_exists = full_model.head_manager.has_head(session_name)
                
                if not head_exists:
                    # åˆ›å»ºä¸´æ—¶headï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
                    # âœ“ å¯¹äºDEQAï¼šåˆ›å»ºDEQAä¸“å®¶é›†æˆ + ä¸´æ—¶head
                    # âœ“ å¯¹äºæ™®é€šæ¨¡å‹ï¼šä»…åˆ›å»ºä¸´æ—¶head
                    use_label_embedding = getattr(future_args, 'use_label_embedding', False)
                    
                    if is_deqa:
                        # DEQAéœ€è¦å…ˆæ·»åŠ ä»»åŠ¡ï¼ˆåˆ›å»ºä¸“å®¶ï¼‰
                        full_model.add_task(task_name, session_name, future_args.num_labels, future_args)
                    else:
                        # æ™®é€šæ¨¡å‹åªéœ€åˆ›å»ºhead
                        head_key = getattr(future_args, 'head_key', session_name)
                        temp_head = full_model.head_manager.create_and_register_head(
                            session_name, task_name, future_args, use_label_embedding, head_key=head_key
                        )
                        if temp_head is None:
                            logger.warning(f"  âœ— Failed to create temporary head for {session_name}")
                            zero_shot_metrics[session_name] = {"acc": 0.0, "micro_prec": 0.0, "micro_recall": 0.0, "micro_f1": 0.0}
                            continue
                    
                    logger.info(f"  âœ“ Temporary head created (random weights)")
                else:
                    logger.info(f"  âœ“ Head already exists for {session_name}")
                
                # è®¾ç½®æ´»åŠ¨headä¸ºæœªæ¥ä»»åŠ¡çš„head
                logger.info(f"  Step 2: Setting active head to {session_name}")
                full_model.set_active_head(session_name, strict=True)
                
                # 0æ ·æœ¬è¯„ä¼°ï¼ˆä½¿ç”¨DEVé›†ï¼Œä¸æ˜¯TESTé›†ï¼‰
                # æ­¤æ—¶ï¼š
                # - æ™®é€šæ¨¡å‹: è®­ç»ƒå¥½çš„base_model + éšæœºhead âœ“
                # - DEQA: è®­ç»ƒå¥½çš„DEQAä¸“å®¶èåˆ + éšæœºhead âœ“
                logger.info(f"  Step 3: Evaluating with trained features + random head")
                try:
                    zero_shot_acc = evaluate_single_task(full_model, task_name, "dev", device, future_args)
                    zero_shot_metrics[session_name] = zero_shot_acc
                    logger.info(f"  âœ“ Zero-shot DEV accuracy on {session_name}: {zero_shot_acc['acc']:.4f}")
                except Exception as e:
                    logger.warning(f"  âœ— Failed zero-shot evaluation on {session_name}: {e}")
                    zero_shot_metrics[session_name] = {"acc": 0.0, "micro_prec": 0.0, "micro_recall": 0.0, "micro_f1": 0.0}
                    logger.info(f"  Zero-shot DEV accuracy on {session_name}: 0.0000 (fallback)")
                
                # ğŸ”‘ é‡è¦ï¼šè¯„ä¼°å®Œååˆ é™¤ä¸´æ—¶headï¼ˆèŠ‚çœå†…å­˜ï¼‰
                if not head_exists:
                    logger.info(f"  Step 4: Removing temporary head to save memory")
                    full_model.head_manager.remove_head(session_name)
                    if is_deqa:
                        # DEQAè¿˜éœ€è¦åˆ é™¤ä¸“å®¶
                        del full_model.deqa_cl.task_ensembles[session_name]
                    logger.info(f"  âœ“ Temporary components removed")
                
            except Exception as e:
                logger.warning(f"  âœ— Error in zero-shot evaluation for {session_name}: {str(e)}")
                import traceback
                logger.debug(traceback.format_exc())
                zero_shot_metrics[session_name] = None
        
        # # å°†0æ ·æœ¬æŒ‡æ ‡æ·»åŠ åˆ°session_info
        # session_info["zero_shot_metrics"] = zero_shot_metrics
    
    # ========== 11) æ›´æ–°è®­ç»ƒä¿¡æ¯ ==========
    session_info["details"].update({
        "epoch_losses": train_result["epoch_losses"],
        "dev_metrics_history": train_result["dev_metrics_history"],
        "dev_losses": train_result.get("dev_losses", []),  # éªŒè¯losså†å²
        "best_metric_summary": train_result.get("best_metric_summary", {}),  # âœ¨ æœ€ä½³devæŒ‡æ ‡æ‘˜è¦ï¼ˆå«æœ€ä½³epochï¼‰
        "final_dev_metrics": train_result["final_dev_metrics"],  # ç”¨äºæ¨¡å‹é€‰æ‹©å’Œearly stopping
        "final_test_metrics": train_result["final_test_metrics"],  # ä»…ç”¨äºæœ€ç»ˆæŠ¥å‘Š
        "dev_used_for_decisions": True,  # æ ‡è®°ä½¿ç”¨DEVé›†è¿›è¡Œè®­ç»ƒå†³ç­–
        "test_for_reference_only": True,  # æ ‡è®°TESTé›†ä»…ä¾›æœ€ç»ˆå‚è€ƒ
        "zero_shot_metrics": zero_shot_metrics if zero_shot_metrics else {}  # 0æ ·æœ¬æ£€æµ‹ç»“æœï¼ˆåŸºäºDEVï¼‰
    })
    
    # âœ¨ åœ¨session_infoçš„é¡¶å±‚ä¹Ÿè®°å½•æœ€ä½³æŒ‡æ ‡ï¼Œæ–¹ä¾¿è®¿é—®
    if "best_metric_summary" in train_result:
        session_info["best_dev_epoch"] = train_result["best_metric_summary"].get("best_epoch", 0)
        session_info["best_dev_metric"] = train_result["best_metric_summary"].get("best_dev_metric", 0.0)
        session_info["best_dev_metric_type"] = train_result["best_metric_summary"].get("metric_type", "unknown")
    
    # è·å–å½“å‰ä»»åŠ¡çš„ç´¢å¼•ï¼ˆåŸºäºå·²å­¦ä¹ çš„ä»»åŠ¡æ•°é‡ï¼‰
    task_idx = len(train_info["tasks"])
    
    # âœ¨ æ›´æ–°å‡†ç¡®ç‡çŸ©é˜µï¼ˆæ”¯æŒä¸‰ç§æŒ‡æ ‡ï¼‰
    cm = ContinualMetrics()
    cm.acc_matrix = train_info.get("acc_matrix", [])
    cm.chunk_f1_matrix = train_info.get("chunk_f1_matrix", [])
    cm.token_micro_f1_no_o_matrix = train_info.get("token_micro_f1_no_o_matrix", [])
    
    # âœ¨ æ„å»ºä¸‰ç§æ€§èƒ½åˆ—è¡¨ï¼šåŒ…å«æ‰€æœ‰å·²å­¦ä¹ ä»»åŠ¡çš„å‡†ç¡®ç‡
    performance_list = []  # é»˜è®¤æŒ‡æ ‡ï¼ˆaccï¼‰
    chunk_f1_list = []  # åºåˆ—ä»»åŠ¡æŒ‡æ ‡1
    token_micro_f1_no_o_list = []  # åºåˆ—ä»»åŠ¡æŒ‡æ ‡2
    
    # è¾…åŠ©å‡½æ•°ï¼šä»metricsä¸­æå–æŒ‡å®šæŒ‡æ ‡
    def extract_metrics_for_all_tasks(full_model, sessions, device, train_info, metric_name='acc'):
        """è¯„ä¼°æ‰€æœ‰å†å²ä»»åŠ¡å¹¶æå–æŒ‡å®šæŒ‡æ ‡"""
        metrics_list = []
        for session in sessions:
            session_args = argparse.Namespace(**session["args"])
            task_metrics = evaluate_single_task(full_model, session["task_name"], "test", device, session_args)
            metrics_list.append(task_metrics.get(metric_name, 0.0))
        return metrics_list
    
    # å¦‚æœæœ‰ä¹‹å‰å­¦ä¹ çš„ä»»åŠ¡ï¼Œéœ€è¦è¯„ä¼°æ‰€æœ‰ä»»åŠ¡ï¼ˆä½¿ç”¨TESTé›†è¿›è¡Œæœ€ç»ˆè¯„ä¼°ï¼‰
    if old_sessions_count > 0:
        logger.info(f"Previous sessions: {[s.get('session_name', 'unknown') for s in train_info['sessions']]}")
        
        # è¯„ä¼°æ‰€æœ‰å†å²ä»»åŠ¡ï¼Œè·å–ä¸‰ç§æŒ‡æ ‡
        all_acc_metrics = extract_metrics_for_all_tasks(full_model, train_info["sessions"], device, train_info, 'acc')
        all_chunk_f1_metrics = extract_metrics_for_all_tasks(full_model, train_info["sessions"], device, train_info, 'chunk_f1')
        all_token_micro_f1_no_o_metrics = extract_metrics_for_all_tasks(full_model, train_info["sessions"], device, train_info, 'token_micro_f1_no_o')
        
        logger.info(f"All historical tasks TEST metrics (acc): {all_acc_metrics}")
        logger.info(f"All historical tasks TEST metrics (chunk_f1): {all_chunk_f1_metrics}")
        logger.info(f"All historical tasks TEST metrics (token_micro_f1_no_o): {all_token_micro_f1_no_o_metrics}")
        
        # æ·»åŠ å½“å‰ä»»åŠ¡çš„æŒ‡æ ‡
        performance_list = all_acc_metrics + [current_test_metrics["acc"]]
        chunk_f1_list = all_chunk_f1_metrics + [current_test_metrics.get("chunk_f1", current_test_metrics["acc"])]
        token_micro_f1_no_o_list = all_token_micro_f1_no_o_metrics + [current_test_metrics.get("token_micro_f1_no_o", current_test_metrics["acc"])]
        
        logger.info(f"Current task TEST metrics - acc: {current_test_metrics['acc']:.4f}, "
                   f"chunk_f1: {current_test_metrics.get('chunk_f1', current_test_metrics['acc']):.4f}, "
                   f"token_micro_f1_no_o: {current_test_metrics.get('token_micro_f1_no_o', current_test_metrics['acc']):.4f}")
        logger.info(f"Final performance list (acc): {performance_list}")
        logger.info(f"Final performance list (chunk_f1): {chunk_f1_list}")
        logger.info(f"Final performance list (token_micro_f1_no_o): {token_micro_f1_no_o_list}")
    else:
        # ç¬¬ä¸€ä¸ªä»»åŠ¡ï¼Œåªæœ‰å½“å‰ä»»åŠ¡çš„å‡†ç¡®ç‡ï¼ˆä½¿ç”¨TESTé›†æŒ‡æ ‡ï¼‰
        performance_list = [current_test_metrics["acc"]]
        chunk_f1_list = [current_test_metrics.get("chunk_f1", current_test_metrics["acc"])]
        token_micro_f1_no_o_list = [current_test_metrics.get("token_micro_f1_no_o", current_test_metrics["acc"])]
        
        logger.info(f"First task performance - acc: {performance_list[0]:.4f}, "
                   f"chunk_f1: {chunk_f1_list[0]:.4f}, "
                   f"token_micro_f1_no_o: {token_micro_f1_no_o_list[0]:.4f}")
    
    # âœ¨ å¤„ç†0æ ·æœ¬æŒ‡æ ‡ï¼ˆåˆ†åˆ«æå–ä¸‰ç§æŒ‡æ ‡ï¼‰
    zero_shot_chunk_f1_metrics = {}
    zero_shot_token_micro_f1_no_o_metrics = {}
    if zero_shot_metrics:
        for session_name, metrics in zero_shot_metrics.items():
            if metrics:
                zero_shot_chunk_f1_metrics[session_name] = {'chunk_f1': metrics.get('chunk_f1', metrics.get('acc', 0.0))}
                zero_shot_token_micro_f1_no_o_metrics[session_name] = {'token_micro_f1_no_o': metrics.get('token_micro_f1_no_o', metrics.get('acc', 0.0))}
    
    # âœ¨ å°†ä¸‰ç§æŒ‡æ ‡ä¼ é€’ç»™å‡†ç¡®ç‡çŸ©é˜µ
    cm.update_acc_matrix(
        task_idx, 
        performance_list, 
        zero_shot_metrics,
        chunk_f1_list,
        token_micro_f1_no_o_list,
        zero_shot_chunk_f1_metrics,
        zero_shot_token_micro_f1_no_o_metrics
    )
    train_info["acc_matrix"] = cm.acc_matrix
    train_info["chunk_f1_matrix"] = cm.chunk_f1_matrix
    train_info["token_micro_f1_no_o_matrix"] = cm.token_micro_f1_no_o_matrix
    
    # æ·»åŠ å½“å‰ä»»åŠ¡åˆ°è®­ç»ƒä¿¡æ¯ä¸­
    train_info["sessions"].append(session_info)
    train_info["tasks"].append(args.task_name)
    
    # ========== 12) è®¡ç®—æŒç»­å­¦ä¹ æŒ‡æ ‡ ==========
    # è‹¥æ˜¯ç¬¬ä¸€ä¸ªä»»åŠ¡, ä¸ç®—æŒç»­å­¦ä¹ æŒ‡æ ‡
    if len(train_info["sessions"]) <= 1:
        logger.info("[Info] This is the first task, skip any CL metrics.")
        final_metrics = {}
        final_metrics_chunk_f1 = {}
        final_metrics_token_micro_f1_no_o = {}
    else:
        k = len(train_info["sessions"])  # æ€»ä»»åŠ¡æ•°
        from continual.metrics import compute_multimodal_transfer_metrics, analyze_task_similarity_transfer
        
        # è·å–ä»»åŠ¡åç§°åˆ—è¡¨
        task_names = [session.get('task_name', 'unknown') for session in train_info["sessions"]]
        
        # âœ¨ åˆ†åˆ«ç”¨ä¸‰ç§æŒ‡æ ‡è®¡ç®—æŒç»­å­¦ä¹ æŒ‡æ ‡
        logger.info("="*80)
        logger.info("ğŸ“Š Computing Continual Learning Metrics with 3 different metrics:")
        logger.info("="*80)
        
        # 1. é»˜è®¤æŒ‡æ ‡ï¼ˆaccï¼‰
        logger.info(f"ğŸ“ˆ Metric 1: Default (acc) - micro_f1 for sentence tasks, chunk_f1 for sequence tasks")
        final_metrics = compute_multimodal_transfer_metrics(cm, k, task_names, matrix_type='acc')
        similarity_analysis = analyze_task_similarity_transfer(cm, task_names, matrix_type='acc')
        if similarity_analysis:
            final_metrics.update(similarity_analysis)
        logger.info(f"  AA={final_metrics.get('AA', 0):.2f}, AIA={final_metrics.get('AIA', 0):.2f}, "
                   f"FM={final_metrics.get('FM', 0):.2f}, BWT={final_metrics.get('BWT', 0):.2f}")
        
        # 2. Chunk F1ï¼ˆä»…å¯¹åºåˆ—ä»»åŠ¡æœ‰æ•ˆï¼Œå¥çº§ä»»åŠ¡å›é€€åˆ°accï¼‰
        logger.info(f"ğŸ“ˆ Metric 2: Chunk-level F1 (for sequence tasks)")
        final_metrics_chunk_f1 = compute_multimodal_transfer_metrics(cm, k, task_names, matrix_type='chunk_f1')
        similarity_analysis_chunk = analyze_task_similarity_transfer(cm, task_names, matrix_type='chunk_f1')
        if similarity_analysis_chunk:
            final_metrics_chunk_f1.update(similarity_analysis_chunk)
        logger.info(f"  AA={final_metrics_chunk_f1.get('AA', 0):.2f}, AIA={final_metrics_chunk_f1.get('AIA', 0):.2f}, "
                   f"FM={final_metrics_chunk_f1.get('FM', 0):.2f}, BWT={final_metrics_chunk_f1.get('BWT', 0):.2f}")
        
        # 3. Token Micro F1 (no O)ï¼ˆä»…å¯¹åºåˆ—ä»»åŠ¡æœ‰æ•ˆï¼Œå¥çº§ä»»åŠ¡å›é€€åˆ°accï¼‰
        logger.info(f"ğŸ“ˆ Metric 3: Token-level Micro F1 (no O, for sequence tasks)")
        final_metrics_token_micro_f1_no_o = compute_multimodal_transfer_metrics(cm, k, task_names, matrix_type='token_micro_f1_no_o')
        similarity_analysis_token = analyze_task_similarity_transfer(cm, task_names, matrix_type='token_micro_f1_no_o')
        if similarity_analysis_token:
            final_metrics_token_micro_f1_no_o.update(similarity_analysis_token)
        logger.info(f"  AA={final_metrics_token_micro_f1_no_o.get('AA', 0):.2f}, AIA={final_metrics_token_micro_f1_no_o.get('AIA', 0):.2f}, "
                   f"FM={final_metrics_token_micro_f1_no_o.get('FM', 0):.2f}, BWT={final_metrics_token_micro_f1_no_o.get('BWT', 0):.2f}")
        
        logger.info("="*80)
    
    # âœ¨ åˆå¹¶è®­ç»ƒæŒ‡æ ‡å’ŒæŒç»­å­¦ä¹ æŒ‡æ ‡ï¼ˆä¸‰ç§æŒ‡æ ‡ï¼‰
    session_info["final_metrics"] = {
        "best_metrics": train_result["best_metrics"],
        "continual_metrics": final_metrics,  # é»˜è®¤æŒ‡æ ‡ï¼ˆaccï¼‰
        "continual_metrics_chunk_f1": final_metrics_chunk_f1,  # Chunk F1
        "continual_metrics_token_micro_f1_no_o": final_metrics_token_micro_f1_no_o  # Token Micro F1 (no O)
    }
    
    # ========== 12.5) ç‰¹å¾èšç±»å¯è§†åŒ– ==========
    if getattr(args, 'enable_feature_visualization', True):  # é»˜è®¤å¼€å¯å¯è§†åŒ–
        try:
            logger.info("="*60)
            logger.info("ğŸ“Š å¼€å§‹ç‰¹å¾èšç±»å¯è§†åŒ–...")
            logger.info("="*60)
            
            # åˆ›å»ºå¯è§†åŒ–ä¿å­˜ç›®å½•
            vis_dir = os.path.join(os.path.dirname(args.output_model_path), 'feature_clustering')
            os.makedirs(vis_dir, exist_ok=True)
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰ˆå¯è§†åŒ–ï¼ˆçœŸå®vsé¢„æµ‹å¯¹æ¯”ï¼‰
            show_predictions = getattr(args, 'vis_show_predictions', True)  # é»˜è®¤æ˜¾ç¤ºé¢„æµ‹å¯¹æ¯”
            
            if show_predictions:
                # ä½¿ç”¨å¢å¼ºç‰ˆï¼šç”ŸæˆçœŸå®æ ‡ç­¾å›¾ + é¢„æµ‹å¯¹æ¯”å›¾
                logger.info("ğŸ“Š ä½¿ç”¨å¢å¼ºç‰ˆå¯è§†åŒ–ï¼ˆåŒ…å«é¢„æµ‹å¯¹æ¯”å›¾ï¼‰")
                visualize_task_enhanced(
                    model=full_model,
                    task_name=args.task_name,
                    session_name=args.session_name,
                    device=device,
                    args=args,
                    save_dir=vis_dir,
                    split='dev',  # ä½¿ç”¨éªŒè¯é›†
                    max_samples=getattr(args, 'vis_max_samples', 2000),
                    show_predictions=True,  # ç”Ÿæˆé¢„æµ‹å¯¹æ¯”å›¾
                    config_name=config_name,  # ä¼ é€’é…ç½®æ–‡ä»¶åï¼Œé¿å…è¦†ç›–
                    plot_dual_metrics=True  # âœ¨ ä¸ºåºåˆ—ä»»åŠ¡ç”Ÿæˆä¸¤ç§æŒ‡æ ‡çš„å›¾
                )
            else:
                # ä½¿ç”¨åŸºç¡€ç‰ˆï¼šä»…ç”ŸæˆçœŸå®æ ‡ç­¾å›¾
                logger.info("ğŸ“Š ä½¿ç”¨åŸºç¡€ç‰ˆå¯è§†åŒ–ï¼ˆä»…çœŸå®æ ‡ç­¾ï¼‰")
                visualize_task_after_training(
                    model=full_model,
                    task_name=args.task_name,
                    session_name=args.session_name,
                    device=device,
                    args=args,
                    config_name=config_name,  # ä¼ é€’é…ç½®æ–‡ä»¶å
                    save_dir=vis_dir,
                    split='dev',  # ä½¿ç”¨éªŒè¯é›†
                    max_samples=getattr(args, 'vis_max_samples', 2000),
                    use_both_methods=getattr(args, 'vis_use_both', False)
                )
            
            # å¦‚æœå·²ç»å­¦ä¹ äº†å¤šä¸ªä»»åŠ¡ï¼Œç»˜åˆ¶æ¼”è¿›å›¾
            if len(train_info["sessions"]) >= 2:
                logger.info("ğŸ“Š ç»˜åˆ¶æŒç»­å­¦ä¹ æ¼”è¿›å›¾ï¼ˆæ‰€æœ‰å·²å­¦ä¹ ä»»åŠ¡ï¼‰...")
                visualize_all_tasks_evolution(
                    save_dir=vis_dir,
                    split='dev',
                    method='tsne',
                    config_name=config_name  # âœ¨ ä¼ é€’config_nameé¿å…è¦†ç›–
                )
            
            logger.info("âœ“ ç‰¹å¾èšç±»å¯è§†åŒ–å®Œæˆ\n")
            
        except Exception as e:
            logger.warning(f"âš ï¸  ç‰¹å¾å¯è§†åŒ–å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")
            import traceback
            logger.debug(traceback.format_exc())
    
    # ========== 13) ä¿å­˜æ¨¡å‹ ==========
    logger.info(f"Saving model to: {args.output_model_path}")
    torch.save(full_model.state_dict(), args.output_model_path)
    
    # ä¿å­˜ä»»åŠ¡å¤´ä¿¡æ¯
    task_heads_path = args.output_model_path.replace('.pt', '_task_heads.pt')
    if hasattr(full_model, 'save_task_heads'):
        full_model.save_task_heads(task_heads_path)
        logger.info(f"Task heads saved to: {task_heads_path}")
    
    logger.info(f"Model saved successfully to: {args.output_model_path}")
    
    # ========== 15) ç»˜åˆ¶è®­ç»ƒæ›²çº¿ ==========
    if getattr(args, 'plot_training_curves', True):  # é»˜è®¤å¯ç”¨
        try:
            logger.info("="*80)
            logger.info("ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
            logger.info("="*80)
            
            # å‡†å¤‡ç»˜å›¾æ•°æ®
            epoch_losses = train_result.get("epoch_losses", [])
            dev_losses = train_result.get("dev_losses", [])
            dev_metrics_history = train_result.get("dev_metrics_history", [])
            
            if epoch_losses and dev_metrics_history:
                # æå–å…³é”®æŒ‡æ ‡
                epochs = list(range(1, len(epoch_losses) + 1))
                # å¦‚æœdev_lossesä¸å­˜åœ¨æˆ–é•¿åº¦ä¸åŒ¹é…ï¼Œç”¨å ä½ç¬¦
                if not dev_losses or len(dev_losses) != len(epoch_losses):
                    dev_losses = [0.0] * len(epochs)
                span_f1_scores = [m.get('acc', 0.0) for m in dev_metrics_history]  # ä¸»æŒ‡æ ‡
                
                metrics_history = {
                    'epochs': epochs,
                    'train_loss': epoch_losses,
                    'dev_loss': dev_losses,  # éªŒè¯lossï¼ˆå·²åœ¨validate_epochä¸­è®¡ç®—ï¼‰
                    'span_f1': span_f1_scores
                }
                
                # ç¡®å®šä¿å­˜è·¯å¾„
                curves_dir = os.path.dirname(args.output_model_path)
                curves_filename = f"{args.session_name}_training_curves.png"
                if config_name:
                    curves_filename = f"{config_name}_{args.session_name}_curves.png"
                curves_path = os.path.join(curves_dir, curves_filename)
                
                # ç»˜åˆ¶æ›²çº¿
                plot_training_curves(
                    metrics_history=metrics_history,
                    save_path=curves_path,
                    task_name=f"{args.task_name.upper()} ({args.session_name})",
                    figsize=(12, 6),
                    dpi=150
                )
                logger.info(f"âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {curves_path}")
            else:
                logger.warning("âš ï¸ è®­ç»ƒå†å²æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡ç»˜å›¾")
        except Exception as e:
            logger.error(f"âš ï¸ ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    save_train_info(train_info, args.train_info_json, logger)
    
    logger.info(f"=== Training completed for task: {args.task_name} ===")
    return train_result["best_metrics"]


def main():
    """ä¸»å‡½æ•°"""
    # ä½¿ç”¨æ–°çš„parseræ¨¡å—
    args = parse_train_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(args=args)
    
    # å¼€å§‹è®­ç»ƒ
    try:
        best_metrics = train(args, logger)
        logger.info(f"Training completed successfully. Best metrics: {best_metrics}")
    except Exception as e:
        logger.error(f"Training failed: {str(e)}")
        raise


if __name__ == "__main__":
    main() 
