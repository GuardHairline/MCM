# modules/training_loop_fixed.py
"""
ä¿®å¤åçš„è®­ç»ƒå¾ªç¯
ä¸»è¦ä¿®å¤ï¼š
1. TAM-CLæŸå¤±é‡å¤è®¡ç®—é—®é¢˜
2. æ¢¯åº¦è£å‰ªä½ç½®é”™è¯¯
3. ç±»åˆ«æƒé‡é—®é¢˜å·²é€šè¿‡label_configè§£å†³
4. æ¢¯åº¦å¤„ç†é¡ºåº
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from typing import Dict, Any, Optional, Tuple
import os
from pathlib import Path
import json
import time
import logging
from collections import Counter

from continual.metrics import ContinualMetrics, compute_metrics_example
from continual.label_embedding_manager import LabelEmbeddingManager
from continual.label_config import get_label_manager
from modules.evaluate import evaluate_single_task
from utils.decode import decode_mate, decode_mner, decode_mabsa
import json
from utils.span_loss import SpanLoss, compute_boundary_loss

logger = logging.getLogger(__name__)


def select_debug_records(debug_records, debug_samples):
    """
    ä»debug_recordsä¸­æŠ½å–å‰50å’Œå50ï¼ˆæˆ–ä¸è¶…è¿‡debug_samplesæ•°é‡ï¼‰ç”¨äºå†™ç›˜ã€‚
    è¿”å›(records, front_count, back_count)ã€‚
    """
    if debug_samples <= 0 or not debug_records:
        return [], 0, 0

    records_to_write = debug_records
    front_written = 0
    back_written = 0

    if len(debug_records) > debug_samples:
        front_n = min(50, debug_samples)
        back_n = min(50, debug_samples - front_n)
        if back_n == 0 and debug_samples > front_n:
            back_n = debug_samples - front_n
        if back_n > 0:
            records_to_write = debug_records[:front_n] + debug_records[-back_n:]
        else:
            records_to_write = debug_records[:front_n]
        front_written, back_written = front_n, back_n
    elif len(debug_records) > 100:
        front_n = min(50, len(debug_records))
        back_n = min(50, len(debug_records) - front_n)
        records_to_write = debug_records[:front_n] + (debug_records[-back_n:] if back_n else [])
        front_written, back_written = front_n, back_n
    else:
        front_written = len(records_to_write)

    return records_to_write, front_written, back_written


def train_epoch(model, train_loader, optimizer, device, args, 
                ewc=None, fisher_selector=None, replay_memory=None,
                lwf=None, si=None, mas=None, gem=None,
                label_embedding_manager: Optional[LabelEmbeddingManager] = None,
                ddas_optimizer=None, scheduler=None, logger_obj=None):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ä»»åŠ¡å¤´ï¼ˆä½¿ç”¨éä¸¥æ ¼æ¨¡å¼ï¼‰
    if hasattr(model, 'set_active_head') and hasattr(args, 'session_name'):
        try:
            model.set_active_head(args.session_name, strict=False)
        except:
            pass  # å¤±è´¥æ—¶ä½¿ç”¨å½“å‰head
    
    # ç¡®ä¿base_modelçš„modeæ­£ç¡®è®¾ç½®
    if hasattr(model, 'base_model') and hasattr(model.base_model, 'mode'):
        current_mode = getattr(args, 'mode', 'multimodal')
        model.base_model.mode = current_mode
        if logger_obj:
            logger_obj.info(f"Set base_model.mode to: {current_mode}")
    
    model.train()
    total_loss = 0.0
    total_samples = 0
    label_counter = Counter()
    ddas_feats = []
    
    # è·å–ä»»åŠ¡ä¿¡æ¯
    task_config = get_label_manager().get_task_config(args.task_name)
    is_seq_task = task_config.task_type.value == "token" if task_config else False
    
    # åˆå§‹åŒ–span lossï¼ˆä»…ç”¨äºåºåˆ—ä»»åŠ¡ï¼‰
    # æ³¨æ„ï¼šå½“ä½¿ç”¨CRFæ—¶ï¼Œç¦ç”¨Span Lossä»¥é¿å…å†²çª
    span_loss_fn = None
    use_crf = getattr(args, 'use_crf', 0)
    use_span_loss = getattr(args, 'use_span_loss', 0)  # æ”¹ä¸ºé»˜è®¤ç¦ç”¨
    
    # CRF å’Œ Span Loss äº’æ–¥ï¼ˆé¿å…å†²çªï¼‰
    if use_crf and use_span_loss:
        if logger_obj:
            logger_obj.warning("âš ï¸ CRF å’Œ Span Loss åŒæ—¶å¯ç”¨ä¼šäº§ç”Ÿå†²çªï¼Œè‡ªåŠ¨ç¦ç”¨ Span Loss")
        use_span_loss = 0
    
    if use_span_loss and is_seq_task:
        span_loss_fn = SpanLoss(
            task_name=args.task_name,
            span_f1_weight=getattr(args, 'span_f1_weight', 0.0),  # F1 lossä¸å¯å¾®ï¼Œæš‚æ—¶ç¦ç”¨
            boundary_weight=getattr(args, 'boundary_weight', 0.2),  # è¾¹ç•Œlossæƒé‡
            transition_weight=getattr(args, 'transition_weight', 0.0)  # è½¬ç§»æƒ©ç½šä¸å¯å¾®ï¼Œæš‚æ—¶ç¦ç”¨
        )
        if logger_obj:
            logger_obj.info(f"âœ“ Span Loss enabled for {args.task_name} (boundary_weight={getattr(args, 'boundary_weight', 0.2)})")

    for batch_idx, batch in enumerate(train_loader):
        # æ•°æ®ç§»åˆ°è®¾å¤‡
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch.get('token_type_ids', None)
        if token_type_ids is not None:
            token_type_ids = token_type_ids.to(device)
        image_tensor = batch['image_tensor'].to(device)
        labels = batch['labels'].to(device)
        
        optimizer.zero_grad()
        
        # ============ å‰å‘ä¼ æ’­ ============
        logits = None
        classification_loss = None
        fused_feat = None  # ç¡®ä¿fused_featè¢«åˆå§‹åŒ–
        
        if args.tam_cl:
            # =========== TAM-CL å‰å‘ =============
            out = model(input_ids, attention_mask, token_type_ids, image_tensor,
                        session_id=args.session_name)
            logits, seq, _ = out if isinstance(out, tuple) else (out, None, None)
            
            # 1. åˆ†ç±»æŸå¤±
            class_weights = get_label_manager().get_class_weights(args.task_name, device)
            if is_seq_task:
                if class_weights is not None:
                    classification_loss = F.cross_entropy(
                        logits.reshape(-1, logits.size(-1)),
                        labels.reshape(-1),
                        weight=class_weights,
                        ignore_index=-100
                    )
                else:
                    classification_loss = F.cross_entropy(
                        logits.reshape(-1, logits.size(-1)),
                        labels.reshape(-1),
                        ignore_index=-100
                    )
            else:
                # å¥çº§ä»»åŠ¡ï¼šåº”ç”¨ç±»åˆ«æƒé‡
                class_weights = get_label_manager().get_class_weights(args.task_name, device)
                if class_weights is not None:
                    classification_loss = F.cross_entropy(logits, labels, weight=class_weights)
                else:
                    classification_loss = F.cross_entropy(logits, labels)
            
            # 2. è®¡ç®—KDæŸå¤±å’Œå¤šæ ·æ€§æŸå¤±
            kd_loss = model.compute_distillation(seq, args.session_name, T=args.lwf_T) if seq is not None else 0.0
            div_loss = model.diversity_loss()
            
            # 3. è®¡ç®—æƒé‡
            lambda_tam = args.old_sessions_count / (args.old_sessions_count + 1) if args.old_sessions_count > 0 else 0.0
            alpha_tam = getattr(args, "tam_alpha", args.lwf_alpha)
            
            # 4. è®¡ç®—betaï¼ˆéœ€è¦detaché¿å…æ¢¯åº¦é—®é¢˜ï¼‰
            if isinstance(kd_loss, torch.Tensor):
                beta_base = 0.1 * ((1 - lambda_tam) * classification_loss.detach() + lambda_tam * alpha_tam * kd_loss.detach())
            else:
                beta_base = 0.1 * (1 - lambda_tam) * classification_loss.detach()
            
            if isinstance(div_loss, torch.Tensor):
                beta_tam = torch.min(div_loss.detach(), beta_base)
            else:
                beta_tam = 0.0
            
            # 5. æœ€ç»ˆæŸå¤±ï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼ï¼‰
            loss = (1 - lambda_tam) * classification_loss
            if isinstance(kd_loss, torch.Tensor):
                loss = loss + lambda_tam * alpha_tam * kd_loss
            if isinstance(beta_tam, torch.Tensor) and isinstance(div_loss, torch.Tensor):
                loss = loss + beta_tam * div_loss
            
            label_counter.update(labels.cpu().numpy())
            
        elif args.clap4clip:
            # CLAP4CLIP æ¨¡å‹
            logits = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                image_tensor=image_tensor,
                task_name=args.session_name
            )
            # CLAP4CLIP: æ ¹æ®ä»»åŠ¡ç±»å‹åº”ç”¨ç±»åˆ«æƒé‡
            is_seq_task = args.task_name in ["mate", "mner", "mabsa"]
            if is_seq_task:
                class_weights = get_label_manager().get_class_weights(args.task_name, device)
                if class_weights is not None:
                    loss = F.cross_entropy(
                        logits.reshape(-1, logits.size(-1)),
                        labels.reshape(-1),
                        weight=class_weights,
                        ignore_index=-100
                    )
                else:
                    loss = F.cross_entropy(
                        logits.reshape(-1, logits.size(-1)),
                        labels.reshape(-1),
                        ignore_index=-100
                    )
            else:
                class_weights = get_label_manager().get_class_weights(args.task_name, device)
                if class_weights is not None:
                    loss = F.cross_entropy(logits, labels, weight=class_weights)
                else:
                    loss = F.cross_entropy(logits, labels)
            label_counter.update(labels.cpu().numpy())
            
        else:
            # =========== æ ‡å‡†æ¨¡å‹å‰å‘ =============
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ··åˆå¤´
            if hasattr(args, 'use_hierarchical_head') and args.use_hierarchical_head:
                token_logits, sent_logits = model(input_ids, attention_mask, token_type_ids, image_tensor)
                
                if is_seq_task:
                    # åºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼šä¸»è¦ä½¿ç”¨tokenå¤´
                    class_weights = get_label_manager().get_class_weights(args.task_name, device)
                    if class_weights is not None:
                        main_loss = F.cross_entropy(
                            token_logits.reshape(-1, token_logits.size(-1)),
                            labels.reshape(-1),
                            weight=class_weights,
                            ignore_index=-100
                        )
                    else:
                        main_loss = F.cross_entropy(
                            token_logits.reshape(-1, token_logits.size(-1)),
                            labels.reshape(-1),
                            ignore_index=-100
                        )
                    
                    # è¾…åŠ©æŸå¤±ï¼šå¥å­çº§é¢„æµ‹
                    sentence_labels = torch.zeros(labels.size(0), dtype=torch.long, device=device)
                    for i in range(labels.size(0)):
                        seq_labels = labels[i]
                        valid_labels = seq_labels[seq_labels != -100]
                        if len(valid_labels) > 0 and (valid_labels != 0).any():
                            sentence_labels[i] = 1
                    
                    # è¾…åŠ©æŸå¤±ï¼šå¥å­çº§åˆ«çš„äºŒåˆ†ç±»ï¼Œä¸ä½¿ç”¨ç±»åˆ«æƒé‡
                    aux_loss = F.cross_entropy(sent_logits, sentence_labels)
                    aux_weight = 0.1
                    loss = main_loss + aux_weight * aux_loss
                    logits = token_logits
                else:
                    # å¥çº§åˆ†ç±»ä»»åŠ¡ï¼šä¸»è¦ä½¿ç”¨å¥å­å¤´ï¼Œåº”ç”¨ç±»åˆ«æƒé‡
                    class_weights = get_label_manager().get_class_weights(args.task_name, device)
                    if class_weights is not None:
                        main_loss = F.cross_entropy(sent_logits, labels, weight=class_weights)
                    else:
                        main_loss = F.cross_entropy(sent_logits, labels)
                    token_labels = labels.unsqueeze(1).expand(-1, token_logits.size(1))
                    aux_loss = F.cross_entropy(
                        token_logits.reshape(-1, token_logits.size(-1)),
                        token_labels.reshape(-1),
                        ignore_index=-100
                    )
                    aux_weight = 0.1
                    loss = main_loss + aux_weight * aux_loss
                    logits = sent_logits
                
                label_counter.update(labels.cpu().numpy())
                
                if args.ddas:
                    pooled_feature = token_logits.mean(dim=1)
            else:
                # åŸæœ‰çš„å•å¤´é€»è¾‘
                if is_seq_task:
                    # è°ƒè¯•ä¿¡æ¯
                    if logger_obj and batch_idx == 0:
                        logger_obj.info(f"DEBUG: Training sequence task, base_model.mode={getattr(model.base_model, 'mode', 'unknown')}")
                    
                    fused_feat = model.base_model(
                        input_ids, attention_mask, token_type_ids, image_tensor,
                        return_sequence=True
                    )
                    
                    # æ£€æŸ¥fused_featæ˜¯å¦ä¸ºNone
                    if fused_feat is None:
                        error_msg = f"ERROR: fused_feat is None! base_model.mode={getattr(model.base_model, 'mode', 'unknown')}, task={args.task_name}"
                        if logger_obj:
                            logger_obj.error(error_msg)
                        raise ValueError(error_msg)
                    
                    # è°ƒç”¨headï¼Œå¯èƒ½è¿”å›logitsæˆ–(nll, logits)
                    # å¯¹äºsequence labelingä»»åŠ¡ï¼Œä¼ å…¥attention_maskä½œä¸ºCRFçš„mask
                    head_output = model.head(fused_feat, labels=labels, mask=attention_mask)
                    
                    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†CRFï¼ˆè¿”å›å…ƒç»„ï¼‰
                    if isinstance(head_output, tuple):
                        # CRFæ¨¡å¼ï¼šè¿”å› (nll, logits)
                        nll, logits = head_output
                        loss = nll  # CRFå·²ç»è®¡ç®—äº†loss
                        if logger_obj and batch_idx % 50 == 0:
                            logger_obj.debug(f"CRF loss (NLL): {loss.item():.4f}")
                    else:
                        # éCRFæ¨¡å¼ï¼šè¿”å› logits
                        logits = head_output
                        class_weights = get_label_manager().get_class_weights(args.task_name, device)
                        if class_weights is not None:
                            loss = F.cross_entropy(
                                logits.reshape(-1, logits.size(-1)),
                                labels.reshape(-1),
                                weight=class_weights,
                                ignore_index=-100
                            )
                        else:
                            loss = F.cross_entropy(
                                logits.reshape(-1, logits.size(-1)),
                                labels.reshape(-1),
                                ignore_index=-100
                            )
                    
                    # âœ¨ æ·»åŠ span lossï¼ˆåºåˆ—ä»»åŠ¡ï¼‰
                    if span_loss_fn is not None:
                        span_loss = span_loss_fn(logits, labels)
                        if isinstance(span_loss, torch.Tensor) and span_loss.requires_grad:
                            loss = loss + span_loss
                            if logger_obj and batch_idx % 50 == 0:
                                logger_obj.debug(f"Span loss: {span_loss.item():.4f}")
                    
                    if args.ddas:
                        pooled_feature = fused_feat.mean(dim=1)
                else:
                    fused_feat = model.base_model(
                        input_ids, attention_mask, token_type_ids, image_tensor,
                        return_sequence=False
                    )
                    logits = model.head(fused_feat)
                    # å¥çº§ä»»åŠ¡ï¼šåº”ç”¨ç±»åˆ«æƒé‡
                    class_weights = get_label_manager().get_class_weights(args.task_name, device)
                    if class_weights is not None:
                        loss = F.cross_entropy(logits, labels, weight=class_weights)
                    else:
                        loss = F.cross_entropy(logits, labels)
                    
                    if args.ddas:
                        pooled_feature = fused_feat
                    
                    label_counter.update(labels.cpu().numpy())
            
            if args.ddas:
                ddas_feats.append(pooled_feature.detach())
        
        # ============ æ·»åŠ æ­£åˆ™åŒ–æŸå¤± ============
        # æ ‡ç­¾ç›¸ä¼¼åº¦æ­£åˆ™åŒ–
        if label_embedding_manager:
            similarity_loss = label_embedding_manager.get_similarity_loss()
            if isinstance(similarity_loss, torch.Tensor) and similarity_loss.requires_grad:
                loss = loss + similarity_loss
        
        # EWC æ­£åˆ™åŒ–
        if ewc is not None:
            ewc_loss = ewc.penalty(model)
            if isinstance(ewc_loss, torch.Tensor) and ewc_loss.requires_grad:
                loss = loss + ewc_loss
        
        # MyMethod æ­£åˆ™åŒ–
        if fisher_selector is not None:
            mymethod_loss = fisher_selector.penalty(model)
            if isinstance(mymethod_loss, torch.Tensor) and mymethod_loss.requires_grad:
                loss = loss + mymethod_loss
        
        # LwF è’¸é¦æŸå¤±
        if lwf is not None and logits is not None:
            inputs = {
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "token_type_ids": token_type_ids,
                "image_tensor": image_tensor
            }
            lwf_loss = lwf.distillation_loss(logits, inputs)
            if isinstance(lwf_loss, torch.Tensor) and lwf_loss.requires_grad:
                loss = loss + lwf_loss
        
        # SI æ­£åˆ™åŒ–
        if si is not None:
            si_loss = si.penalty()
            if isinstance(si_loss, torch.Tensor) and si_loss.requires_grad:
                loss = loss + si_loss
        
        # MAS æ­£åˆ™åŒ–
        if mas is not None:
            mas_loss = mas.penalty()
            if isinstance(mas_loss, torch.Tensor) and mas_loss.requires_grad:
                loss = loss + mas_loss
        
        # Experience Replayï¼ˆä¿®å¤ç‰ˆï¼šåœ¨batchè®­ç»ƒæ—¶é›†æˆï¼‰
        if replay_memory and hasattr(args, 'replay') and args.replay:
            # æ¯éš”å‡ ä¸ªbatchè¿›è¡Œä¸€æ¬¡replay
            if batch_idx % getattr(args, 'replay_frequency', 4) == 0:
                replay_session = replay_memory.sample_replay_session(batch_idx, model, device, args)
                if replay_session is not None:
                    replay_loss = replay_memory.compute_replay_loss(replay_session, model, device)
                    if replay_loss is not None:
                        replay_weight = getattr(args, 'replay_weight', 0.5)
                        loss = loss + replay_weight * replay_loss
                        if logger_obj and batch_idx % 10 == 0:
                            logger_obj.debug(f"Added replay loss from '{replay_session}': {replay_loss.item():.4f}")
        
        # âœ“ MoE Load Balancing Lossï¼ˆä¿®å¤ç‰ˆï¼‰
        if args.moe_adapters and hasattr(model, 'base_model') and hasattr(model.base_model, 'text_adapters'):
            balance_coef = getattr(args, 'moe_balance_coef', 0.01)  # å¯é…ç½®çš„ç³»æ•°
            bal_loss = 0.0
            num_layers = 0
            
            # æ”¶é›†æ‰€æœ‰MoEå±‚çš„load_loss
            for moe_layer in model.base_model.text_adapters + model.base_model.image_adapters:
                if hasattr(moe_layer, 'load_loss') and moe_layer.load_loss is not None:
                    bal_loss += moe_layer.load_loss
                    num_layers += 1
            
            # å¹³å‡å¹¶åŠ æƒ
            if num_layers > 0:
                bal_loss = bal_loss / num_layers  # å¹³å‡
                loss = loss + balance_coef * bal_loss
                
                if logger_obj and batch_idx % 50 == 0:
                    logger_obj.debug(f"MoE Balance Loss: {bal_loss.item():.4f} (coef={balance_coef})")
        
        # ============ åå‘ä¼ æ’­å’Œæ¢¯åº¦å¤„ç† ============
        # æ£€æŸ¥lossæ˜¯å¦æœ‰æ•ˆ
        if torch.isnan(loss) or torch.isinf(loss):
            logger.warning(f"Batch {batch_idx}: Invalid loss detected (NaN or Inf), skipping batch")
            continue
        
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆåœ¨GEMæŠ•å½±ä¹‹å‰ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # GEM æ¢¯åº¦æŠ•å½±ï¼ˆåœ¨æ¢¯åº¦è£å‰ªä¹‹åï¼Œä¼˜åŒ–å™¨æ›´æ–°ä¹‹å‰ï¼‰
        if gem is not None:
            gem.project_gradients()
        
        optimizer.step()
        if scheduler is not None:
            scheduler.step()
        
        # SI æ¢¯åº¦ç´¯ç§¯ï¼ˆåœ¨optimizer.step()ä¹‹åï¼‰
        if si is not None:
            si.accumulate()
        
        # === DDAS è‡ªç¼–ç å™¨è®­ç»ƒ ===
        if args.moe_adapters and args.ddas and ddas_optimizer is not None and ddas_feats:
            ae_inputs = torch.cat(ddas_feats, dim=0)
            ddas_optimizer.zero_grad()
            recon = model.ddas.ae_list[-1](ae_inputs)
            ae_loss = F.mse_loss(recon, ae_inputs)
            ae_loss.backward()
            ddas_optimizer.step()
            ddas_feats.clear()
        
        batch_size = input_ids.size(0)
        total_loss += loss.item() * batch_size
        total_samples += batch_size
        
        if logger_obj and batch_idx % 10 == 0:
            logger_obj.info(f"Batch {batch_idx}/{len(train_loader)}: loss={loss.item():.4f}")
    
    avg_loss = total_loss / total_samples if total_samples > 0 else 0.0
    if logger_obj:
        logger_obj.info(f"Epoch total_loss={total_loss:.4f}, avg_loss={avg_loss:.4f}, total_samples={total_samples}")
    
    return avg_loss


def validate_epoch(model, val_loader, device, args, logger=None):
    """éªŒè¯ä¸€ä¸ªepochï¼ˆè®¡ç®—losså’Œmetricsï¼‰"""
    debug_samples = getattr(args, "debug_samples", 0)
    debug_records = []
    # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ä»»åŠ¡å¤´ï¼ˆä½¿ç”¨éä¸¥æ ¼æ¨¡å¼ï¼‰
    if hasattr(model, 'set_active_head') and hasattr(args, 'session_name'):
        try:
            model.set_active_head(args.session_name, strict=False)
        except:
            pass  # å¤±è´¥æ—¶ä½¿ç”¨å½“å‰head
    
    # ç¡®ä¿base_modelçš„modeæ­£ç¡®è®¾ç½®
    if hasattr(model, 'base_model') and hasattr(model.base_model, 'mode'):
        current_mode = getattr(args, 'mode', 'multimodal')
        model.base_model.mode = current_mode
    
    # CLAP4CLIPæ¨¡å‹éœ€è¦ç‰¹æ®Šå¤„ç†
    if args.clap4clip and hasattr(model, 'set_current_task'):
        model.set_current_task(args.session_name)
    
    # è®¡ç®—éªŒè¯loss
    model.eval()
    total_loss = 0.0
    total_samples = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            # æ•°æ®ç§»åˆ°device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch.get('labels', None)
            
            # å‡†å¤‡å…¶ä»–è¾“å…¥
            token_type_ids = batch.get('token_type_ids', None)
            if token_type_ids is not None:
                token_type_ids = token_type_ids.to(device)
            
            image_tensor = batch.get('image_tensor', None)
            if image_tensor is not None:
                image_tensor = image_tensor.to(device)
            
            if labels is None:
                continue
            labels = labels.to(device)
            
            try:
                task_config = get_label_manager().get_task_config(args.task_name)
                is_seq_task = task_config.task_type.value == "token" if task_config else False
                if is_seq_task:
                    # åºåˆ—ä»»åŠ¡ï¼šæ˜¾å¼è°ƒç”¨ base_model + headï¼Œè·å– CRF/CE lossï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰
                    fused_feat = model.base_model(
                        input_ids, attention_mask, token_type_ids, image_tensor,
                        return_sequence=True
                    )
                    head_out = model.head(fused_feat, labels=labels, mask=attention_mask)
                    if isinstance(head_out, tuple):
                        loss, logits = head_out
                    else:
                        logits = head_out
                        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                        num_labels = logits.size(-1)
                        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))
                else:
                    # å¥çº§/å…¶ä»–ä»»åŠ¡ï¼šä¿æŒåŸé€»è¾‘
                    out = model(input_ids, attention_mask, token_type_ids, image_tensor)
                    if isinstance(out, tuple):
                        if len(out) >= 2 and isinstance(out[0], torch.Tensor) and out[0].dim() == 0:
                            loss = out[0]
                            logits = out[1]
                        else:
                            logits = out[0] if isinstance(out, tuple) else out
                            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                            loss = loss_fct(logits, labels)
                    else:
                        logits = out
                        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                        loss = loss_fct(logits, labels)
                
                if loss is not None:
                    bs = input_ids.size(0)
                    total_loss += loss.item() * bs
                    total_samples += bs
                    
                    if debug_samples and len(debug_records) < debug_samples and logits is not None:
                        preds = logits.argmax(dim=-1) if logits.dim() >= 2 else logits.argmax(dim=-1)
                        for i in range(min(bs, debug_samples - len(debug_records))):
                            # ä»…ä¿ç•™æœ‰æ•ˆtokenï¼ˆæ’é™¤-100ä»¥åŠpaddingä½ç½®ï¼‰
                            valid_mask = labels[i] != -100
                            gold_seq = labels[i][valid_mask].cpu().tolist()
                            if preds.dim() > 1:
                                pred_seq = preds[i][valid_mask].cpu().tolist()
                            else:
                                pred_seq = preds.cpu().tolist()
                            # è§£ç spanï¼ˆ0=Oï¼Œ1/2=B/I-PERï¼Œ3/4=B/I-ORGï¼Œ5/6=B/I-LOCï¼Œ7/8=B/I-MISCï¼‰
                            if args.task_name == "mner":
                                pred_span = list(decode_mner(pred_seq))
                                gold_span = list(decode_mner(gold_seq))
                            elif args.task_name == "mate":
                                pred_span = list(decode_mate(pred_seq))
                                gold_span = list(decode_mate(gold_seq))
                            elif args.task_name == "mabsa":
                                pred_span = list(decode_mabsa(pred_seq))
                                gold_span = list(decode_mabsa(gold_seq))
                            else:
                                pred_span = []
                                gold_span = []
                            debug_records.append({
                                "batch_idx": batch_idx,
                                "sample_idx": i,
                                "valid_len": int(valid_mask.sum().item()),
                                "gold_seq": gold_seq,
                                "pred_seq": pred_seq,
                                "gold_spans": gold_span,
                                "pred_spans": pred_span
                            })
                    
            except Exception as e:
                if logger:
                    logger.warning(f"éªŒè¯æ—¶è®¡ç®—losså¤±è´¥: {e}")
                continue
    
    # è®¡ç®—å¹³å‡loss
    avg_loss = total_loss / total_samples if total_samples > 0 else 0.0
    
    # ä½¿ç”¨ evaluate_single_task è®¡ç®—è¯¦ç»†metrics
    metrics = evaluate_single_task(model, args.task_name, "dev", device, args)
    if debug_samples > 0 and debug_records:
        log_dir = Path(os.path.dirname(args.output_model_path) or ".")
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / f"{args.session_name}_debug_samples.jsonl"

        records_to_write, front_written, back_written = select_debug_records(debug_records, debug_samples)

        with log_file.open("a", encoding="utf-8") as f:
            for rec in records_to_write:
                f.write(json.dumps(rec, ensure_ascii=False) + "\n")
        if logger:
            logger.info(f"âœ… Debug samples written to {log_file} (first {front_written}, last {back_written})")
    
    return avg_loss, metrics


def train_model(model, train_loader, val_loader, optimizer, scheduler, device, args,
                ewc=None, fisher_selector=None, replay_memory=None,
                lwf=None, si=None, mas=None, gem=None,
                label_embedding_manager: Optional[LabelEmbeddingManager] = None,
                logger=None):
    """å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆä¿®å¤ç‰ˆï¼‰"""
    best_val_metric = 0.0  # æ”¹ç”¨accuracyä½œä¸ºæ ‡å‡†
    best_metrics = None
    best_epoch = 0  # è®°å½•æœ€ä½³epoch
    patience = args.patience
    no_improve_count = 0
    
    # æ”¶é›†æ¯ä¸ªepochçš„losså’Œdev metrics
    epoch_losses = []
    dev_losses = []  # è®°å½•éªŒè¯loss
    dev_metrics_history = []
    
    # åˆ›å»ºDDASä¼˜åŒ–å™¨
    ddas_optimizer = None
    if args.ddas and hasattr(model, 'ddas') and model.ddas is not None:
        ddas_optimizer = torch.optim.Adam(model.ddas.parameters(), lr=1e-4)
    
    for epoch in range(args.epochs):
        start_time = time.time()
        
        if logger:
            logger.info(f"{'='*80}")
            logger.info(f"Epoch {epoch+1}/{args.epochs}")
            logger.info(f"{'='*80}")
        
        # è®­ç»ƒ
        train_loss = train_epoch(
            model, train_loader, optimizer, device, args,
            ewc, fisher_selector, replay_memory, lwf, si, mas, gem,
            label_embedding_manager=label_embedding_manager,
            ddas_optimizer=ddas_optimizer,
            scheduler=scheduler,
            logger_obj=logger
        )
                
        # éªŒè¯
        val_loss, metrics = validate_epoch(model, val_loader, device, args, logger)
        
        # è®°å½•losså’Œmetrics
        epoch_losses.append(train_loss)
        dev_losses.append(val_loss)  # è®°å½•éªŒè¯loss
        dev_metrics_history.append(metrics)
        
        # Early stoppingæ£€æŸ¥
        # æ ¹æ®ä»»åŠ¡ç±»å‹ï¼Œaccå­—æ®µå­˜å‚¨çš„æ˜¯ï¼š
        # - åºåˆ—ä»»åŠ¡(MATE/MNER/MABSA): chunk_f1 * 100 (span-level micro F1)
        # - å¥çº§ä»»åŠ¡(MASC): micro_f1 * 100
        current_metric = metrics.get('acc', 0.0)
        if current_metric > best_val_metric:
            best_val_metric = current_metric
            best_metrics = metrics.copy()
            best_epoch = epoch + 1  # è®°å½•æœ€ä½³epochï¼ˆä»1å¼€å§‹ï¼‰
            no_improve_count = 0
            if logger:
                # åˆ¤æ–­ä»»åŠ¡ç±»å‹ä»¥æ˜¾ç¤ºæ­£ç¡®çš„æŒ‡æ ‡åç§°
                task_name = args.task_name
                is_sequence_task = task_name in ["mate", "mner", "mabsa"]
                metric_name = "Chunk F1 (Span-level)" if is_sequence_task else "Micro F1"
                logger.info(f"âœ“ New best {metric_name}: {best_val_metric:.4f} at epoch {best_epoch}")
        else:
            no_improve_count += 1
            if logger:
                logger.info(f"âœ— No improvement for {no_improve_count} epoch(s)")
        
        epoch_time = time.time() - start_time
        
        if logger:
            logger.info(f"Epoch {epoch+1}/{args.epochs} - "
                       f"Train Loss: {train_loss:.4f}, "
                       f"Val Acc: {current_metric:.4f}, "
                       f"Time: {epoch_time:.2f}s")
        
        # Early stopping
        if no_improve_count >= patience:
            if logger:
                logger.info(f"Early stopping triggered after {epoch+1} epochs")
            break
    
    # è®­ç»ƒç»“æŸåè¯„ä¼°dev/test
    final_dev_metrics = evaluate_single_task(model, args.task_name, "dev", device, args)
    final_test_metrics = evaluate_single_task(model, args.task_name, "test", device, args)
    
    if args.ddas and hasattr(model, 'ddas') and model.ddas is not None:
        model.ddas.add_task()
    
    if best_metrics is None:
        best_metrics = final_dev_metrics
        best_epoch = args.epochs  # å¦‚æœæ²¡æœ‰æ›´æ–°è¿‡ï¼Œè¯´æ˜æœ€åä¸€ä¸ªepochæœ€å¥½
    
    # è®°å½•æœ€ä½³devæŒ‡æ ‡çš„è¯¦ç»†ä¿¡æ¯
    task_name = args.task_name
    is_sequence_task = task_name in ["mate", "mner", "mabsa"]
    
    # æ„å»ºæœ€ä½³æŒ‡æ ‡æ‘˜è¦
    best_metric_summary = {
        "best_epoch": best_epoch,
        "best_dev_metric": best_val_metric,
        "metric_type": "chunk_f1 (span-level)" if is_sequence_task else "micro_f1",
        "all_metrics": best_metrics,  # åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å®Œæ•´å­—å…¸
    }
    
    if logger:
        logger.info("="*80)
        logger.info("ğŸ“Š Training Summary")
        logger.info("="*80)
        logger.info(f"Best Epoch: {best_epoch}/{args.epochs}")
        logger.info(f"Best Dev {best_metric_summary['metric_type']}: {best_val_metric:.4f}")
        logger.info(f"Final Dev {best_metric_summary['metric_type']}: {final_dev_metrics.get('acc', 0.0):.4f}")
        logger.info(f"Final Test {best_metric_summary['metric_type']}: {final_test_metrics.get('acc', 0.0):.4f}")
        logger.info("="*80)
    
    return {
        "best_metrics": best_metrics,
        "best_metric_summary": best_metric_summary,  # æ–°å¢ï¼šæœ€ä½³æŒ‡æ ‡æ‘˜è¦
        "epoch_losses": epoch_losses,
        "dev_losses": dev_losses,  # éªŒè¯loss
        "dev_metrics_history": dev_metrics_history,
        "final_dev_metrics": final_dev_metrics,
        "final_test_metrics": final_test_metrics
    }


def update_continual_learning_components(model, train_loader, device, args, 
                                       ewc=None, fisher_selector=None, si=None, mas=None, gem=None,
                                       session_info=None, logger=None):
    """æ›´æ–°æŒç»­å­¦ä¹ ç»„ä»¶"""
    # EWC Fisherä¼°è®¡ï¼ˆä½¿ç”¨æ›´å¤§çš„æ ·æœ¬é‡ï¼‰
    if ewc:
        ewc.estimate_and_save_fisher(train_loader, device=device, sample_size=500)
        if logger:
            logger.info(f"[EWC] Fisher estimated and saved (500 samples)")
    
    # MyMethod Fisherä¼°è®¡ï¼ˆä½¿ç”¨æ›´å¤§çš„æ ·æœ¬é‡ï¼‰
    if fisher_selector:
        fisher_selector.estimate_and_save_fisher(train_loader, device=device, sample_size=500)
        if logger:
            logger.info(f"[MyMethod] Fisher estimated and saved (500 samples)")
    
    # SI Omegaæ›´æ–°
    if si:
        si.update_omega()
        if logger:
            logger.info(f"[SI] Omega updated")
    
    # MAS Importanceè®¡ç®—
    if mas:
        mas.compute_importance(train_loader, device, task_name=args.task_name)
        if logger:
            logger.info(f"[MAS] Importance computed for task '{args.task_name}'")
    
    # GEM Memoryä¿å­˜
    if gem:
        gem.save_memory(args.task_name)
        if logger:
            logger.info(f"[GEM] Memory for task '{args.task_name}' saved")
    
    return session_info

