# continual/ta_pecl/model_wrapper.py
import torch
import torch.nn as nn
import json
import os
from .config import get_expert_config, TASK_NAME_MAP 
from .modules import TA_PECL_Block

class TaskState:
    """
    ä¸€ä¸ªç®€å•çš„é nn.Module ç±»ï¼Œç”¨äºåœ¨çˆ¶å­æ¨¡å—é—´å…±äº«çŠ¶æ€ï¼ˆtask_idï¼‰ã€‚
    å› ä¸ºä¸æ˜¯ nn.Moduleï¼Œæ‰€ä»¥ä¸ä¼šè§¦å‘ PyTorch çš„é€’å½’éå†æ­»å¾ªç¯ã€‚
    """
    def __init__(self):
        self.current_task_id = 0

class TA_PECL_LayerWrapper(nn.Module):
    """
    å•å±‚åŒ…è£…å™¨ï¼šæ‹¦æˆªåŸå§‹ Transformer å±‚çš„è¾“å‡ºï¼Œå¹¶æ³¨å…¥ Adapter ä¿¡å·ã€‚
    """
    def __init__(self, original_layer, adapter, task_state):
        super().__init__()
        self.original_layer = original_layer
        self.adapter = adapter
        self.task_state = task_state  # å¼•ç”¨å…±äº«çŠ¶æ€å¯¹è±¡

    def forward(self, *args, **kwargs):
        # 1. æ‰§è¡ŒåŸå§‹å±‚ (å†»ç»“çŠ¶æ€)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è®©åŸå§‹å±‚è‡ªå·±å¤„ç† attention_mask, relative_pos ç­‰å¤æ‚å‚æ•°
        with torch.no_grad():
            outputs = self.original_layer(*args, **kwargs)
        
        # DeBERTa/BERT çš„è¾“å‡ºé€šå¸¸æ˜¯ tuple: (hidden_states, attention_weights, ...)
        if isinstance(outputs, tuple):
            hidden_states = outputs[0]
        else:
            hidden_states = outputs
            
        # 2. æ‰§è¡Œ Adapter (å¯è®­ç»ƒ)
        # ä»å…±äº«çŠ¶æ€è·å–å½“å‰ä»»åŠ¡ ID
        task_id = self.task_state.current_task_id
        
        adapter_out = self.adapter(hidden_states, task_id)
        
        # 3. æ®‹å·®è¿æ¥ (Residual Connection)
        hidden_states = hidden_states + adapter_out
        
        # 4. æ¢å¤åŸå§‹è¾“å‡ºæ ¼å¼
        if isinstance(outputs, tuple):
            return (hidden_states,) + outputs[1:]
        else:
            return hidden_states
    def reset_expert_stats(self):
        """é‡ç½®æ‰€æœ‰å±‚çš„ä¸“å®¶ç»Ÿè®¡"""
        for layer in self.patched_layers:
            # layer æ˜¯ TA_PECL_LayerWrapper
            # layer.adapter æ˜¯ TA_PECL_Block
            # layer.adapter.router æ˜¯ TaskAwareRouter
            layer.adapter.router.reset_stats()
        print("[TA-PECL] Expert statistics reset.")

    
class TA_PECL_ModelWrapper(nn.Module):
    """
    TA-PECL æ¨¡å‹åŒ…è£…å™¨ï¼š
    ä¸é‡å†™ forwardï¼Œè€Œæ˜¯é€šè¿‡ 'æ‰‹æœ¯' æ›¿æ¢ base_model å†…éƒ¨çš„ Transformer å±‚ã€‚
    è¿™ç§æ–¹æ³•æœ€ç¨³å¥ï¼Œå…¼å®¹ DeBERTa, BERT, RoBERTa ç­‰å¤šç§æ¶æ„ã€‚
    """
    def __init__(self, base_model, args):
        super().__init__()
        self.base_model = base_model
        self.args = args
        
        # é…ç½®
        self.hidden_size = getattr(args, 'hidden_dim', 768)
        self.expert_config = get_expert_config(hidden_size=self.hidden_size)
        self.num_tasks = len(TASK_NAME_MAP)
        self.top_k = getattr(args, 'ta_pecl_top_k', 4)
        
        # [å…³é”®ä¿®å¤] ä½¿ç”¨ç‹¬ç«‹çš„çŠ¶æ€å¯¹è±¡ï¼Œé¿å… nn.Module å¾ªç¯å¼•ç”¨
        self.task_state = TaskState()
        
        output_dir = os.path.dirname(args.output_model_path) if hasattr(args, 'output_model_path') and args.output_model_path else "./checkpoints"
        self.stats_dir = os.path.join(output_dir, "expert_stats")
        os.makedirs(self.stats_dir, exist_ok=True)
        
        # 1. è‡ªåŠ¨å®šä½å¹¶æ›¿æ¢ Transformer å±‚
        self.patched_layers = self._find_and_replace_layers()
        
        # 2. å†»ç»“ä¸»å¹² (é™¤äº†æˆ‘ä»¬åˆšæ³¨å…¥çš„ Adapter)
        self._freeze_backbone()
        
        print(f"\n[TA-PECL] System Initialized Successfully.")
        print(f"          - Strategy: Layer Injection (In-place)")
        print(f"          - Injected Layers: {len(self.patched_layers)}")
        print(f"          - Active Experts: Top-{self.top_k}")

    def _find_and_replace_layers(self):
        """
        é€’å½’æŸ¥æ‰¾ transformer layers å¹¶è¿›è¡Œæ›¿æ¢
        """
        # ç­–ç•¥ 1: é’ˆå¯¹ä½ çš„ BaseMultimodalModel (DeBERTa V3)
        if hasattr(self.base_model, 'text_encoder'):
            # DeBERTa V3 ç»“æ„: text_encoder -> encoder -> layer (ModuleList)
            encoder_module = self.base_model.text_encoder
            if hasattr(encoder_module, 'encoder'):
                 container = encoder_module.encoder
                 if hasattr(container, 'layer'):
                     return self._replace_in_container(container, 'layer')
                 elif hasattr(container, 'layers'):
                     return self._replace_in_container(container, 'layers')
        
        # ç­–ç•¥ 2: æ ‡å‡† HF Model (base_model æœ¬èº«å°±æ˜¯ Transformer)
        if hasattr(self.base_model, 'encoder'):
            container = self.base_model.encoder
            if hasattr(container, 'layer'):
                return self._replace_in_container(container, 'layer')
        
        raise ValueError("TA-PECL Error: Could not locate transformer layers to patch in base_model.")

    def _replace_in_container(self, container, attribute_name):
        """
        åœ¨ ModuleList å®¹å™¨ä¸­æ‰§è¡ŒåŸåœ°æ›¿æ¢
        """
        layers_list = getattr(container, attribute_name) # è·å– ModuleList å¯¹è±¡
        patched_layers = []
        
        for i, original_layer in enumerate(layers_list):
            # é˜²æ­¢é‡å¤åŒ…è£…
            if isinstance(original_layer, TA_PECL_LayerWrapper):
                patched_layers.append(original_layer)
                continue

            # åˆ›å»º Adapter Block
            adapter_block = TA_PECL_Block(
                hidden_size=self.hidden_size, 
                num_tasks=self.num_tasks,
                expert_config=self.expert_config,
                top_k=self.top_k
            )
            
            # åˆ›å»ºåŒ…è£…å±‚ (LayerWrapper)
            # [å…³é”®ä¿®å¤] ä¼ å…¥ task_state è€Œä¸æ˜¯ self
            wrapped_layer = TA_PECL_LayerWrapper(original_layer, adapter_block, self.task_state)
            
            # [å…³é”®] åŸåœ°æ›¿æ¢ï¼
            layers_list[i] = wrapped_layer
            patched_layers.append(wrapped_layer)
            
        return patched_layers

    def _freeze_backbone(self):
        # 1. å…ˆå†»ç»“æ‰€æœ‰å‚æ•°
        for param in self.base_model.parameters():
            param.requires_grad = False
            
        # 2. è§£å†» Adapter å‚æ•°
        # å› ä¸º Adapter ç°åœ¨æ˜¯ base_model çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ patched_layers æ‰¾åˆ°å®ƒä»¬
        count = 0
        for layer in self.patched_layers:
            for param in layer.adapter.parameters():
                param.requires_grad = True
                count += 1
                
        print(f"[TA-PECL] Backbone frozen. {count} Adapter parameter groups unfrozen.")

    def set_task_name(self, task_name):
        """è®¾ç½®å½“å‰ä»»åŠ¡IDï¼ŒLayerWrapper ä¼šé€šè¿‡ task_state è¯»å–å®ƒ"""
        t_name = task_name.lower()
        found = False
        for key, tid in TASK_NAME_MAP.items():
            if key in t_name:
                self.task_state.current_task_id = tid # æ›´æ–°å…±äº«çŠ¶æ€
                found = True
                break
        if not found:
            print(f"[Warning] Unknown task name '{task_name}', defaulting to MASC (id=0).")
            self.task_state.current_task_id = 0

    def forward(self, *args, **kwargs):
        """
        ç›´æ¥å§”æ‰˜ç»™ base_modelã€‚
        ç”±äºæˆ‘ä»¬å·²ç»æ›¿æ¢äº†å†…éƒ¨çš„å±‚ï¼Œbase_model çš„ forward æµç¨‹ä¼šè‡ªåŠ¨ç»è¿‡æˆ‘ä»¬çš„ Adapterã€‚
        """
        # ç¡®ä¿ forward å‰çŠ¶æ€å·²è®¾ç½®ï¼ˆè™½ç„¶é€šå¸¸ç”± set_task_name å¤„ç†ï¼‰
        # è¿™é‡Œåªåšå§”æ‰˜
        return self.base_model(*args, **kwargs)
    # é‡ç½®ç»Ÿè®¡ï¼Œç¡®ä¿ä¸åŒä»»åŠ¡ä¸æ··æ·†
    def reset_expert_stats(self):
        """é‡ç½®æ‰€æœ‰å±‚çš„ä¸“å®¶ç»Ÿè®¡æ•°æ®"""
        for layer in self.patched_layers:
            # layer.adapter.router æ˜¯ TaskAwareRouter
            if hasattr(layer.adapter.router, 'reset_stats'):
                layer.adapter.router.reset_stats()
        # print("[TA-PECL] Expert statistics reset.") # å¯é€‰ï¼šå‡å°‘æ—¥å¿—åˆ·å±

    # æ ¸å¿ƒä¿å­˜é€»è¾‘
    def save_expert_stats(self, session_name, phase="train", epoch=None):
        """
        å°†ä¸“å®¶ç»Ÿè®¡ä¿¡æ¯ä¿å­˜ä¸º JSON æ–‡ä»¶
        Args:
            session_name: å½“å‰ä»»åŠ¡ä¼šè¯åç§° (å¦‚ masc_twitter2015)
            phase: é˜¶æ®µ (train, eval, test)
            epoch: å½“å‰è½®æ•° (å¯é€‰)
        """
        # 1. æ±‡æ€»æ‰€æœ‰å±‚çš„æ•°æ®
        total_samples = 0
        global_counts = None
        global_weights = None
        
        # éå†æ‰€æœ‰å±‚ç´¯åŠ 
        for layer in self.patched_layers:
            router = layer.adapter.router
            if global_counts is None:
                global_counts = router.activation_counts.clone().cpu()
                global_weights = router.accumulated_weights.clone().cpu()
                total_samples = router.total_samples
            else:
                global_counts += router.activation_counts.cpu()
                global_weights += router.accumulated_weights.cpu()
        
        # è½¬æ¢ä¸º Python æ•°å­—
        if torch.is_tensor(total_samples):
            total_samples = total_samples.item()

        if total_samples == 0:
            return

        # 2. æ„å»ºç»Ÿè®¡å­—å…¸
        stats_data = {
            "session_name": session_name,
            "phase": phase,
            "epoch": epoch,
            "total_samples": total_samples,
            "top_k": self.top_k,
            "num_layers": len(self.patched_layers),
            "experts": {}
        }

        # è®¡ç®—æ€»å†³ç­–æ¬¡æ•° (ç”¨äºç®—ç™¾åˆ†æ¯”)
        total_decisions = total_samples * len(self.patched_layers) * self.top_k
        expert_names = list(self.expert_config.keys())

        for idx, name in enumerate(expert_names):
            count = int(global_counts[idx].item())
            weight_sum = global_weights[idx].item()
            
            stats_data["experts"][name] = {
                "activation_count": count,            # æ¿€æ´»æ€»æ¬¡æ•°
                "accumulated_weight": weight_sum,     # æƒé‡æ€»å’Œ
                "active_rate": (count / total_decisions), # æ¿€æ´»å æ¯” (0~1)
                "avg_weight": (weight_sum / count) if count > 0 else 0.0 # è¢«é€‰ä¸­æ—¶çš„å¹³å‡æƒé‡
            }

        # 3. å†™å…¥æ–‡ä»¶
        # æ–‡ä»¶åç¤ºä¾‹: stats_masc_twitter2015_train_final.json
        filename = f"stats_{session_name}_{phase}"
        if epoch is not None:
            filename += f"_ep{epoch}"
        filename += ".json"
        
        save_path = os.path.join(self.stats_dir, filename)
        
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(stats_data, f, indent=4)
            print(f"[TA-PECL] ğŸ“Š Expert stats saved to: {save_path}")
        except Exception as e:
            print(f"[TA-PECL] Failed to save stats: {e}")

    # æ—¥å¿—æ‰“å°æ–¹æ³•ï¼Œæ–¹ä¾¿åœ¨æ§åˆ¶å°å¿«é€ŸæŸ¥çœ‹
    def log_expert_statistics(self, logger, phase="TRAIN"):
        """
        æ±‡æ€»å¹¶æ‰“å°ä¸“å®¶ä½¿ç”¨æƒ…å†µæŠ¥å‘Š
        """
        total_samples = 0
        
        # èšåˆæ‰€æœ‰å±‚çš„ç»Ÿè®¡æ•°æ®
        # global_counts: [num_experts]
        global_counts = None
        global_weights = None
        
        # 1. æ±‡æ€»æ•°æ®
        for layer in self.patched_layers:
            router = layer.adapter.router
            if global_counts is None:
                global_counts = router.activation_counts.clone()
                global_weights = router.accumulated_weights.clone()
                total_samples = router.total_samples
            else:
                global_counts += router.activation_counts
                global_weights += router.accumulated_weights
                # total_samples åœ¨æ‰€æœ‰å±‚åº”è¯¥æ˜¯ä¸€æ ·çš„ï¼Œå–ä¸€ä¸ªå³å¯
        
        if total_samples == 0:
            logger.warning("[TA-PECL] No statistics collected (total_samples=0).")
            return

        # 2. è®¡ç®—ç™¾åˆ†æ¯”
        # æ¿€æ´»ç‡ = æ¿€æ´»æ¬¡æ•° / (æ€»æ ·æœ¬æ•° * å±‚æ•° * TopK) ? 
        # æ›´ç›´è§‚çš„æ˜¯ï¼šæ¯ä¸ªæ ·æœ¬å¹³å‡æ¿€æ´»è¯¥ä¸“å®¶çš„å±‚æ•°æ¯”ä¾‹ï¼Œæˆ–è€…ç®€å•çš„æ€»å æ¯”
        # è¿™é‡Œæˆ‘ä»¬è®¡ç®—ï¼šåœ¨æ‰€æœ‰è·¯ç”±å†³ç­–ä¸­ï¼ˆå±‚æ•°*æ ·æœ¬æ•°*TopKï¼‰ï¼Œè¯¥ä¸“å®¶è¢«é€‰ä¸­çš„æ¦‚ç‡
        
        num_layers = len(self.patched_layers)
        total_decisions = total_samples * num_layers * self.top_k
        
        # 3. æ‰“å°æŠ¥å‘Š
        logger.info("=" * 80)
        logger.info(f"ğŸ“Š TA-PECL Expert Usage Report ({phase}) - Total Samples: {total_samples}")
        logger.info(f"{'Expert Name':<20} | {'Type':<10} | {'Active %':<10} | {'Avg Weight':<10} | {'Count':<10}")
        logger.info("-" * 80)
        
        expert_names = list(self.expert_config.keys())
        
        # æŒ‰ç±»å‹åˆ†ç»„æ’åºä»¥ä¾¿æŸ¥çœ‹ (Task -> Modality -> DEQA -> Flex)
        sorted_indices = sorted(range(len(expert_names)), key=lambda k: expert_names[k])
        
        for idx in sorted_indices:
            name = expert_names[idx]
            count = int(global_counts[idx].item())
            weight_sum = global_weights[idx].item()
            
            # Active %: è¯¥ä¸“å®¶è¢«æ¿€æ´»çš„é¢‘ç‡
            active_pct = (count / total_decisions) * 100
            
            # Avg Weight: è¢«æ¿€æ´»æ—¶çš„å¹³å‡æƒé‡ (é¿å…é™¤ä»¥0)
            avg_weight = (weight_sum / count) if count > 0 else 0.0
            
            # ç¡®å®šç±»å‹
            etype = "Unknown"
            if "flex" in name: etype = "Flexible"
            elif "deqa" in name: etype = "DEQA"
            elif "text" in name or "multi" in name: etype = "Modal"
            else: etype = "Task"
            
            # é«˜äº®æ˜¾ç¤ºè¿‡åº¦æ´»è·ƒçš„ Flex ä¸“å®¶ (ä¾‹å¦‚è¶…è¿‡ 20%)
            highlight = ""
            if etype == "Flexible" and active_pct > 20:
                highlight = "ğŸ”´ (High)"
            elif etype == "Task" and active_pct < 1:
                highlight = "âš ï¸ (Low)"

            logger.info(f"{name:<20} | {etype:<10} | {active_pct:6.2f}%    | {avg_weight:6.4f}     | {count:<10} {highlight}")
            
        logger.info("=" * 80)