#!/usr/bin/env python3
"""
æµ‹è¯•CLAP4CLIPæ¨¡å‹
"""

import sys
import os
import argparse
import torch
import torch.nn.functional as F
from PIL import Image
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from continual.clap4clip import CLAP4CLIP, TaskAdapter, ProbabilisticFinetuning
from continual.clap4clip.clap_utils import (
    create_clap4clip_model,
    compute_clap4clip_loss,
    get_clip_processor,
    preprocess_clap4clip_data
)

def test_clap4clip_basic():
    """æµ‹è¯•CLAP4CLIPåŸºæœ¬åŠŸèƒ½"""
    print("å¼€å§‹æµ‹è¯•CLAP4CLIPåŸºæœ¬åŠŸèƒ½...")
    
    try:
        # åˆ›å»ºæ¨¡å‹
        print("1. åˆ›å»ºCLAP4CLIPæ¨¡å‹...")
        model = CLAP4CLIP(
            text_model_name="openai/clip-vit-base-patch32",
            image_model_name="openai/clip-vit-base-patch32",
            num_labels=3,
            dropout_prob=0.1,
            adapter_size=64,
            finetune_lambda=0.1,
            temperature=0.07
        )
        print("   âœ“ CLAP4CLIPæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ä»»åŠ¡ç®¡ç†
        print("2. æµ‹è¯•ä»»åŠ¡ç®¡ç†...")
        model.add_task("task1", 3)
        model.add_task("task2", 5)
        model.set_current_task("task1")
        print(f"   âœ“ ä»»åŠ¡ç®¡ç†æ­£å¸¸ï¼Œå½“å‰ä»»åŠ¡: {model.current_task}")
        print(f"   âœ“ ä»»åŠ¡å¤´æ•°é‡: {len(model.task_heads)}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        print("3. åˆ›å»ºæµ‹è¯•æ•°æ®...")
        batch_size = 2
        seq_len = 77
        hidden_size = model.projection_dim
        
        input_ids = torch.randint(0, 1000, (batch_size, seq_len))
        attention_mask = torch.ones(batch_size, seq_len)
        image_tensor = torch.randn(batch_size, 3, 224, 224)
        
        print("   âœ“ æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("4. æµ‹è¯•å‰å‘ä¼ æ’­...")
        model.eval()
        with torch.no_grad():
            logits = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                image_tensor=image_tensor,
                task_name="task1"
            )
        print(f"   âœ“ å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {logits.shape}")
        
        # æµ‹è¯•ç‰¹å¾æå–
        print("5. æµ‹è¯•ç‰¹å¾æå–...")
        text_features = model.get_text_features(input_ids, attention_mask)
        image_features = model.get_image_features(image_tensor)
        print(f"   âœ“ æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {text_features.shape}")
        print(f"   âœ“ å›¾åƒç‰¹å¾å½¢çŠ¶: {image_features.shape}")
        
        # æµ‹è¯•å¯¹æ¯”å­¦ä¹ æŸå¤±
        print("6. æµ‹è¯•å¯¹æ¯”å­¦ä¹ æŸå¤±...")
        contrastive_loss = model.compute_contrastive_loss(text_features, image_features)
        print(f"   âœ“ å¯¹æ¯”å­¦ä¹ æŸå¤±: {contrastive_loss.item():.4f}")
        
        print("\nğŸ‰ CLAP4CLIPåŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_probabilistic_finetuning():
    """æµ‹è¯•æ¦‚ç‡å¾®è°ƒæ¨¡å—"""
    print("\nå¼€å§‹æµ‹è¯•æ¦‚ç‡å¾®è°ƒæ¨¡å—...")
    
    try:
        # åˆ›å»ºæ¦‚ç‡å¾®è°ƒæ¨¡å—
        print("1. åˆ›å»ºæ¦‚ç‡å¾®è°ƒæ¨¡å—...")
        dummy_model = type('DummyModel', (), {'hidden_size': 512})()
        pf_module = ProbabilisticFinetuning(
            model=dummy_model,
            num_tasks=3,
            finetune_lambda=0.1,
            temperature=0.07,
            use_uncertainty=True
        )
        print("   âœ“ æ¦‚ç‡å¾®è°ƒæ¨¡å—åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•ç‰¹å¾
        print("2. åˆ›å»ºæµ‹è¯•ç‰¹å¾...")
        batch_size = 4
        hidden_size = 512
        
        text_features = torch.randn(batch_size, hidden_size)
        image_features = torch.randn(batch_size, hidden_size)
        
        print("   âœ“ æµ‹è¯•ç‰¹å¾åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ¦‚ç‡è®¡ç®—
        print("3. æµ‹è¯•æ¦‚ç‡è®¡ç®—...")
        task_probs = pf_module.compute_task_probabilities(text_features, image_features)
        print(f"   âœ“ ä»»åŠ¡æ¦‚ç‡å½¢çŠ¶: {task_probs.shape}")
        print(f"   âœ“ æ¦‚ç‡å’Œ: {task_probs.sum(dim=1)}")
        
        # æµ‹è¯•ä¸ç¡®å®šæ€§ä¼°è®¡
        print("4. æµ‹è¯•ä¸ç¡®å®šæ€§ä¼°è®¡...")
        uncertainty = pf_module.estimate_uncertainty(text_features + image_features)
        print(f"   âœ“ ä¸ç¡®å®šæ€§å½¢çŠ¶: {uncertainty.shape}")
        print(f"   âœ“ ä¸ç¡®å®šæ€§èŒƒå›´: [{uncertainty.min().item():.4f}, {uncertainty.max().item():.4f}]")
        
        # æµ‹è¯•æ¦‚ç‡å¾®è°ƒæ›´æ–°
        print("5. æµ‹è¯•æ¦‚ç‡å¾®è°ƒæ›´æ–°...")
        text_finetuned, image_finetuned, task_probs = pf_module.probabilistic_update(
            text_features, image_features
        )
        print(f"   âœ“ å¾®è°ƒåæ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {text_finetuned.shape}")
        print(f"   âœ“ å¾®è°ƒåå›¾åƒç‰¹å¾å½¢çŠ¶: {image_finetuned.shape}")
        
        print("\nğŸ‰ æ¦‚ç‡å¾®è°ƒæ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_clap4clip_integration():
    """æµ‹è¯•CLAP4CLIPä¸è®­ç»ƒæµç¨‹çš„é›†æˆ"""
    print("\nå¼€å§‹æµ‹è¯•CLAP4CLIPé›†æˆ...")
    
    try:
        # åˆ›å»ºå‚æ•°
        print("1. åˆ›å»ºæµ‹è¯•å‚æ•°...")
        args = argparse.Namespace(
            text_model_name="openai/clip-vit-base-patch32",
            image_model_name="openai/clip-vit-base-patch32",
            num_labels=3,
            dropout_prob=0.1,
            adapter_size=64,
            finetune_lambda=0.1,
            temperature=0.07,
            session_name="test_session",
            clap4clip=True,
            pretrained_model_path=None,
            use_label_embedding=False,
            tam_cl=False,
            moe_adapters=False
        )
        
        print("   âœ“ æµ‹è¯•å‚æ•°åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹åˆ›å»º
        print("2. æµ‹è¯•æ¨¡å‹åˆ›å»º...")
        from modules.train_utils import create_model
        
        device = "cpu"
        model = create_model(args, device, label_embedding_manager=None, logger=None)
        print("   âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("3. æµ‹è¯•é›†æˆå‰å‘ä¼ æ’­...")
        batch_size = 2
        seq_len = 77
        
        input_ids = torch.randint(0, 1000, (batch_size, seq_len))
        attention_mask = torch.ones(batch_size, seq_len)
        image_tensor = torch.randn(batch_size, 3, 224, 224)
        
        model.eval()
        with torch.no_grad():
            logits = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                image_tensor=image_tensor
            )
        print(f"   âœ“ é›†æˆå‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {logits.shape}")
        
        print("\nğŸ‰ CLAP4CLIPé›†æˆæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("CLAP4CLIPæ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    success1 = test_clap4clip_basic()
    
    # æµ‹è¯•æ¦‚ç‡å¾®è°ƒ
    success2 = test_probabilistic_finetuning()
    
    # æµ‹è¯•é›†æˆ
    success3 = test_clap4clip_integration()
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“:")
    print(f"åŸºæœ¬åŠŸèƒ½æµ‹è¯•: {'âœ“ é€šè¿‡' if success1 else 'âœ— å¤±è´¥'}")
    print(f"æ¦‚ç‡å¾®è°ƒæµ‹è¯•: {'âœ“ é€šè¿‡' if success2 else 'âœ— å¤±è´¥'}")
    print(f"é›†æˆæµ‹è¯•: {'âœ“ é€šè¿‡' if success3 else 'âœ— å¤±è´¥'}")
    
    if success1 and success2 and success3:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CLAP4CLIPæ¨¡å‹å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
    
    print("=" * 60) 