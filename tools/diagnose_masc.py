"""
MASCä»»åŠ¡è¯Šæ–­å·¥å…·

ç”¨äºè¯Šæ–­MASCä»»åŠ¡è®­ç»ƒå¤±è´¥çš„åŸå› ï¼ŒåŒ…æ‹¬ï¼š
1. æ•°æ®åˆ†å¸ƒåˆ†æ
2. ç±»åˆ«æƒé‡åˆç†æ€§æ£€æŸ¥
3. è®­ç»ƒè¿‡ç¨‹ä¸­çš„é¢„æµ‹åˆ†å¸ƒ
4. æŸå¤±å‡½æ•°å’Œæ¢¯åº¦åˆ†æ
"""

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
from collections import Counter
import sys
import argparse
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from datasets.get_dataset import get_dataset
from continual.label_config import get_label_manager


def analyze_data_distribution(task_name="masc", dataset_name="twitter2015", split="train"):
    """åˆ†ææ•°æ®åˆ†å¸ƒ"""
    print("\n" + "="*80)
    print(f"ğŸ“Š åˆ†æ {task_name.upper()} - {dataset_name} - {split} æ•°æ®é›†åˆ†å¸ƒ")
    print("="*80)
    
    # åˆ›å»ºå‚æ•°å¯¹è±¡
    # æ ¹æ®ä»»åŠ¡å’Œæ•°æ®é›†ç¡®å®šæ–‡ä»¶è·¯å¾„
    # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
    possible_paths = [
        f"data/{dataset_name}",  # åŸå§‹è·¯å¾„
        f"data/MASC/{dataset_name}",  # MASCå­ç›®å½•
    ]
    
    data_base_path = None
    for path in possible_paths:
        if os.path.exists(f"{path}/{split}.txt") or os.path.exists(f"{path}/{split}__.txt"):
            data_base_path = path
            break
    
    if data_base_path is None:
        data_base_path = possible_paths[0]  # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ª
    
    print(f"ä½¿ç”¨æ•°æ®è·¯å¾„: {data_base_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    text_files = {}
    for split_name in ['train', 'dev', 'test']:
        for suffix in ['', '__']:
            file_path = f"{data_base_path}/{split_name}{suffix}.txt"
            if os.path.exists(file_path):
                text_files[f"{split_name}_text_file"] = file_path
                break
        if f"{split_name}_text_file" not in text_files:
            # é»˜è®¤è·¯å¾„
            text_files[f"{split_name}_text_file"] = f"{data_base_path}/{split_name}.txt"
    
    args = argparse.Namespace(
        task_name=task_name,
        dataset=dataset_name,
        image_dir=f"{data_base_path}/images",
        **text_files,
        text_model_name="microsoft/deberta-v3-base",
        max_seq_length=128,
        deqa=False
    )
    
    try:
        # ç›´æ¥è¯»å–æ–‡æœ¬æ–‡ä»¶æ¥ç»Ÿè®¡æ ‡ç­¾ï¼Œé¿å…åŠ è½½å›¾ç‰‡
        text_file = text_files.get(f"{split}_text_file")
        if not os.path.exists(text_file):
            print(f"âŒ æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {text_file}")
            return None, None
        
        print(f"è¯»å–æ–‡æœ¬æ–‡ä»¶: {text_file}")
        
        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒï¼ˆç›´æ¥ä»æ–‡ä»¶è¯»å–ï¼Œä¸åŠ è½½æ•°æ®é›†ï¼‰
        # MASC æ•°æ®æ ¼å¼ä¸ºæ¯ 4 è¡Œä¸€ä¸ªæ ·æœ¬ï¼š
        # 1) åŸæ–‡ï¼Œå¸¦ $T$ å ä½ç¬¦
        # 2) aspect_term (æ›¿æ¢ $T$ çš„çœŸå®å­—ç¬¦ä¸²)
        # 3) sentiment (å¯èƒ½æ˜¯ -1, 0, 1)
        # 4) image_name (å›¾åƒæ–‡ä»¶å)
        labels_list = []
        with open(text_file, 'r', encoding='utf-8') as f:
            lines = [l.strip() for l in f.readlines()]
        
        # æ£€æŸ¥æ ¼å¼
        if len(lines) % 4 != 0:
            print(f"âŒ æ•°æ®æ ¼å¼é”™è¯¯ï¼šæ–‡ä»¶è¡Œæ•° {len(lines)} ä¸æ˜¯4çš„å€æ•°")
            return None, None
        
        # æ¯4è¡Œä¸ºä¸€ç»„
        for i in range(0, len(lines), 4):
            text_with_T = lines[i]
            aspect_term = lines[i+1]
            sentiment_str = lines[i+2]
            image_name = lines[i+3]
            
            try:
                sentiment = int(sentiment_str)  # -1, 0, 1
                # æ˜ å°„åˆ°æ ‡ç­¾ID: -1->0(NEG), 0->1(NEU), 1->2(POS)
                label_id = sentiment + 1
                labels_list.append(label_id)
            except ValueError:
                print(f"âš ï¸  è­¦å‘Šï¼šæ— æ³•è§£ææƒ…æ„Ÿå€¼ '{sentiment_str}' (è¡Œ {i+3})")
                continue
        
        # è®¡ç®—åˆ†å¸ƒ
        label_counter = Counter(labels_list)
        total = len(labels_list)
        
        if total == 0:
            print("âŒ æ²¡æœ‰è¯»å–åˆ°ä»»ä½•æ ·æœ¬ï¼")
            return None, None
        
        # è·å–æ ‡ç­¾åç§°
        label_manager = get_label_manager()
        task_config = label_manager.get_task_config(task_name)
        label_names = task_config.label_names
        
        print(f"\næ€»æ ·æœ¬æ•°: {total}")
        print("\næ ‡ç­¾åˆ†å¸ƒ:")
        print("-" * 60)
        print(f"{'æ ‡ç­¾ID':<10} {'æ ‡ç­¾å':<15} {'æ ·æœ¬æ•°':<10} {'å æ¯”':<10} {'é¢‘ç‡å€’æ•°':<10}")
        print("-" * 60)
        
        for label_id in sorted(label_counter.keys()):
            count = label_counter[label_id]
            ratio = count / total
            inv_freq = 1.0 / ratio if ratio > 0 else 0
            label_name = label_names[label_id] if label_id < len(label_names) else f"Unknown-{label_id}"
            print(f"{label_id:<10} {label_name:<15} {count:<10} {ratio*100:>6.2f}%   {inv_freq:>8.2f}")
        
        print("-" * 60)
        
        # è®¡ç®—ä¸å¹³è¡¡ç¨‹åº¦
        counts = np.array([label_counter[i] for i in range(len(label_names))])
        imbalance_ratio = counts.max() / counts.min() if counts.min() > 0 else float('inf')
        print(f"\nâš ï¸  ç±»åˆ«ä¸å¹³è¡¡æ¯”: {imbalance_ratio:.2f}x (æœ€å¤šç±»/æœ€å°‘ç±»)")
        
        if imbalance_ratio > 10:
            print("   âŒ ä¸¥é‡ä¸å¹³è¡¡ï¼å»ºè®®ä½¿ç”¨ç±»åˆ«æƒé‡")
        elif imbalance_ratio > 3:
            print("   âš ï¸  ä¸­åº¦ä¸å¹³è¡¡ï¼Œå»ºè®®ä½¿ç”¨ç±»åˆ«æƒé‡")
        else:
            print("   âœ… ç›¸å¯¹å¹³è¡¡")
        
        return label_counter, label_names
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def analyze_class_weights(task_name="masc"):
    """åˆ†æå½“å‰é…ç½®çš„ç±»åˆ«æƒé‡"""
    print("\n" + "="*80)
    print(f"âš–ï¸  åˆ†æ {task_name.upper()} ç±»åˆ«æƒé‡é…ç½®")
    print("="*80)
    
    label_manager = get_label_manager()
    task_config = label_manager.get_task_config(task_name)
    label_names = task_config.label_names
    
    # è·å–ç±»åˆ«æƒé‡
    device = torch.device("cpu")
    class_weights = label_manager.get_class_weights(task_name, device)
    
    if class_weights is None:
        print("âŒ å½“å‰æ²¡æœ‰é…ç½®ç±»åˆ«æƒé‡ï¼")
        return None
    
    print(f"\nå½“å‰ç±»åˆ«æƒé‡:")
    print("-" * 40)
    for i, (name, weight) in enumerate(zip(label_names, class_weights)):
        print(f"{i}. {name:<15}: {weight.item():.2f}")
    print("-" * 40)
    
    # åˆ†ææƒé‡æ¯”ä¾‹
    weight_ratio = class_weights.max() / class_weights.min()
    print(f"\næƒé‡æ¯”ä¾‹: {weight_ratio:.2f}x (æœ€å¤§/æœ€å°)")
    
    if weight_ratio > 20:
        print("âš ï¸  è­¦å‘Š: æƒé‡å·®è·è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š")
    elif weight_ratio < 2:
        print("âš ï¸  è­¦å‘Š: æƒé‡å·®è·å¤ªå°ï¼Œå¯èƒ½æ— æ³•æœ‰æ•ˆç¼“è§£ç±»åˆ«ä¸å¹³è¡¡")
    else:
        print("âœ… æƒé‡æ¯”ä¾‹åˆç†")
    
    return class_weights


def analyze_loss_function(task_name="masc"):
    """åˆ†ææŸå¤±å‡½æ•°çš„è®¡ç®—"""
    print("\n" + "="*80)
    print(f"ğŸ“ åˆ†ææŸå¤±å‡½æ•° - {task_name.upper()}")
    print("="*80)
    
    label_manager = get_label_manager()
    task_config = label_manager.get_task_config(task_name)
    
    print(f"\nä»»åŠ¡ç±»å‹: {task_config.task_type.value}")
    print(f"æ ‡ç­¾æ•°é‡: {task_config.num_labels}")
    print(f"æ ‡ç­¾åç§°: {task_config.label_names}")
    
    # æ£€æŸ¥åˆ†ç±»å™¨ç»“æ„
    print("\nåˆ†ç±»å™¨ç»“æ„:")
    print(f"  â€¢ è¾“å…¥ç»´åº¦: hidden_dim (é€šå¸¸768)")
    print(f"  â€¢ è¾“å‡ºç»´åº¦: {task_config.num_labels}")
    print(f"  â€¢ åˆ†ç±»å™¨ç±»å‹: {'ä¸‰åˆ†ç±»' if task_config.num_labels == 3 else f'{task_config.num_labels}åˆ†ç±»'}")
    
    # æ£€æŸ¥æŸå¤±å‡½æ•°
    device = torch.device("cpu")
    class_weights = label_manager.get_class_weights(task_name, device)
    
    print("\næŸå¤±å‡½æ•°é…ç½®:")
    if class_weights is not None:
        print(f"  âœ… ä½¿ç”¨ç±»åˆ«æƒé‡: F.cross_entropy(logits, labels, weight=class_weights)")
        print(f"  â€¢ æƒé‡å€¼: {[f'{w:.2f}' for w in class_weights]}")
    else:
        print(f"  âŒ æœªä½¿ç”¨ç±»åˆ«æƒé‡: F.cross_entropy(logits, labels)")
        print(f"  âš ï¸  è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹åå‘å¤šæ•°ç±»åˆ«ï¼")
    
    # æ¨¡æ‹ŸæŸå¤±è®¡ç®—
    print("\næ¨¡æ‹ŸæŸå¤±è®¡ç®—ç¤ºä¾‹:")
    print("-" * 60)
    
    # å‡è®¾çš„logitså’Œlabels
    logits = torch.tensor([[2.0, 3.0, 1.5], [1.0, 4.0, 2.0], [2.5, 2.0, 3.5]])  # 3ä¸ªæ ·æœ¬ï¼Œ3ç±»
    labels = torch.tensor([0, 1, 2])  # NEG, NEU, POSå„ä¸€ä¸ª
    
    # ä¸ä½¿ç”¨æƒé‡
    loss_no_weight = F.cross_entropy(logits, labels)
    print(f"ä¸ä½¿ç”¨æƒé‡çš„æŸå¤±: {loss_no_weight.item():.4f}")
    
    # ä½¿ç”¨æƒé‡
    if class_weights is not None:
        loss_with_weight = F.cross_entropy(logits, labels, weight=class_weights)
        print(f"ä½¿ç”¨æƒé‡çš„æŸå¤±: {loss_with_weight.item():.4f}")
        print(f"å·®å¼‚: {abs(loss_with_weight.item() - loss_no_weight.item()):.4f}")
    
    print("-" * 60)


def recommend_class_weights(label_counter, label_names, strategy="balanced"):
    """æ¨èç±»åˆ«æƒé‡
    
    Args:
        label_counter: æ ‡ç­¾è®¡æ•°å™¨
        label_names: æ ‡ç­¾åç§°åˆ—è¡¨
        strategy: æƒé‡ç­–ç•¥ ("balanced", "sqrt", "log", "inverse")
    """
    print("\n" + "="*80)
    print(f"ğŸ’¡ æ¨èç±»åˆ«æƒé‡ (ç­–ç•¥: {strategy})")
    print("="*80)
    
    total = sum(label_counter.values())
    if total == 0:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ ·æœ¬æ•°æ®ï¼Œæ— æ³•æ¨èæƒé‡")
        return
    
    n_classes = len(label_names)
    
    recommended_weights = []
    
    for i in range(n_classes):
        count = label_counter.get(i, 1)  # é¿å…é™¤é›¶
        freq = count / total
        
        if strategy == "balanced":
            # sklearnçš„balancedç­–ç•¥: n_samples / (n_classes * n_samples_per_class)
            weight = total / (n_classes * count)
        elif strategy == "sqrt":
            # å¹³æ–¹æ ¹å€’æ•°
            weight = 1.0 / np.sqrt(freq)
        elif strategy == "log":
            # å¯¹æ•°å€’æ•°
            weight = 1.0 / np.log(freq + 1e-6)
        elif strategy == "inverse":
            # ç®€å•å€’æ•°
            weight = 1.0 / freq
        else:
            weight = 1.0
        
        recommended_weights.append(weight)
    
    # å½’ä¸€åŒ–æƒé‡ï¼ˆå°†æœ€å°æƒé‡è®¾ä¸º1.0ï¼‰
    recommended_weights = np.array(recommended_weights)
    recommended_weights = recommended_weights / recommended_weights.min()
    
    # é™åˆ¶æœ€å¤§æƒé‡æ¯”ä¾‹ï¼ˆé¿å…è¿‡å¤§ï¼‰
    max_ratio = 20.0
    if recommended_weights.max() / recommended_weights.min() > max_ratio:
        recommended_weights = np.clip(recommended_weights, 1.0, max_ratio)
        print(f"âš ï¸  æƒé‡å·²è£å‰ªåˆ°æœ€å¤§æ¯”ä¾‹ {max_ratio}x")
    
    print("\næ¨èæƒé‡:")
    print("-" * 60)
    print(f"{'æ ‡ç­¾':<15} {'æ ·æœ¬æ•°':<10} {'é¢‘ç‡':<10} {'æ¨èæƒé‡':<10}")
    print("-" * 60)
    
    for i, name in enumerate(label_names):
        count = label_counter.get(i, 0)
        freq = count / total
        weight = recommended_weights[i]
        print(f"{name:<15} {count:<10} {freq*100:>6.2f}%   {weight:>8.2f}")
    
    print("-" * 60)
    print(f"\nåœ¨ continual/label_config.py ä¸­ä½¿ç”¨:")
    weight_str = ", ".join([f"{w:.1f}" for w in recommended_weights])
    print(f'    "masc": [{weight_str}],  # {label_names}')
    
    return recommended_weights


def main():
    parser = argparse.ArgumentParser(description="MASCä»»åŠ¡è¯Šæ–­å·¥å…·")
    parser.add_argument("--task", type=str, default="masc", help="ä»»åŠ¡åç§°")
    parser.add_argument("--dataset", type=str, default="twitter2015", help="æ•°æ®é›†åç§°")
    parser.add_argument("--split", type=str, default="train", help="æ•°æ®é›†åˆ’åˆ†")
    parser.add_argument("--recommend", type=str, default="balanced", 
                       choices=["balanced", "sqrt", "log", "inverse"],
                       help="æ¨èæƒé‡ç­–ç•¥")
    
    args = parser.parse_args()
    
    # 1. åˆ†ææ•°æ®åˆ†å¸ƒ
    label_counter, label_names = analyze_data_distribution(args.task, args.dataset, args.split)
    
    if label_counter is None:
        return
    
    # 2. åˆ†æå½“å‰æƒé‡
    analyze_class_weights(args.task)
    
    # 3. åˆ†ææŸå¤±å‡½æ•°
    analyze_loss_function(args.task)
    
    # 4. æ¨èæƒé‡
    if args.recommend:
        recommend_class_weights(label_counter, label_names, args.recommend)
    
    # æ‰“å°è¯Šæ–­å»ºè®®
    print("\n" + "="*80)
    print("ğŸ“‹ MASCè®­ç»ƒå¤±è´¥è¯Šæ–­å»ºè®®")
    print("="*80)
    print("""
ğŸ”´ å¸¸è§é—®é¢˜: MASCè®­ç»ƒæ—¶æ¨¡å‹å…¨éƒ¨é¢„æµ‹NEUæˆ–POSï¼Œæ— æ³•å­¦ä¹ NEG

åŸå› åˆ†æ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡ (NEUå 60%, POSå 30%, NEGä»…å 10%)
2. åˆ†ç±»å™¨æ˜¯æ ‡å‡†çš„3åˆ†ç±»å™¨: Linear(hidden_dim, 3) 
3. æŸå¤±å‡½æ•°å¦‚æœä¸ä½¿ç”¨ç±»åˆ«æƒé‡,ä¼šä¸¥é‡åå‘å¤šæ•°ç±»

è§£å†³æ–¹æ¡ˆ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… æ–¹æ¡ˆ1: è°ƒæ•´ç±»åˆ«æƒé‡ (é¦–é€‰)
   å½“å‰æƒé‡: [5.0, 1.0, 2.0]  # [NEG, NEU, POS]
   
   å»ºè®®å°è¯•:
   a) æ›´æ¿€è¿›çš„æƒé‡: [10.0, 1.0, 3.0]  # å¤§å¹…æå‡NEG
   b) Balancedæƒé‡: ä½¿ç”¨ --recommend balanced æŸ¥çœ‹æ¨èå€¼
   c) åŠ¨æ€è°ƒæ•´: è®­ç»ƒå‰å‡ ä¸ªepochç”¨é«˜æƒé‡,åç»­é€æ¸é™ä½

âœ… æ–¹æ¡ˆ2: é™ä½å­¦ä¹ ç‡
   - å½“å‰å­¦ä¹ ç‡å¦‚æœæ˜¯5e-5,æ”¹ä¸º1e-5æˆ–5e-6
   - æ›´å°çš„å­¦ä¹ ç‡è®©æ¨¡å‹æœ‰æ›´å¤šæ—¶é—´å­¦ä¹ minority class
   - é…åˆå¢åŠ è®­ç»ƒepochs (ä¾‹å¦‚ä»10å¢åŠ åˆ°20)

âœ… æ–¹æ¡ˆ3: å‡å°Batch Size
   - ä»32é™åˆ°16æˆ–8
   - æ›´å°çš„batchè®©æ¨¡å‹æ›´é¢‘ç¹åœ°çœ‹åˆ°NEGæ ·æœ¬
   - æ³¨æ„ç›¸åº”è°ƒæ•´å­¦ä¹ ç‡ (batchå‡åŠ,lrä¹Ÿå‡åŠ)

âœ… æ–¹æ¡ˆ4: ä½¿ç”¨Focal Loss
   åœ¨ modules/training_loop_fixed.py ä¸­æ›¿æ¢æŸå¤±å‡½æ•°:
   ```python
   # ä»£æ›¿ F.cross_entropy
   from torch.nn import functional as F
   
   def focal_loss(logits, labels, alpha=0.25, gamma=2.0):
       ce_loss = F.cross_entropy(logits, labels, reduction='none')
       pt = torch.exp(-ce_loss)
       focal_loss = alpha * (1-pt)**gamma * ce_loss
       return focal_loss.mean()
   ```

âœ… æ–¹æ¡ˆ5: è¿‡é‡‡æ ·NEGç±»
   åœ¨æ•°æ®åŠ è½½æ—¶å¯¹NEGæ ·æœ¬é‡å¤é‡‡æ ·2-3æ¬¡

âš ï¸  æ–¹æ¡ˆ6: æ£€æŸ¥æ•°æ®è´¨é‡
   - ç¡®è®¤NEGæ ·æœ¬çš„æ ‡æ³¨æ˜¯å¦æ­£ç¡®
   - æ£€æŸ¥NEGæ ·æœ¬æ˜¯å¦çœŸçš„ä¸NEU/POSæœ‰æ˜æ˜¾åŒºåˆ«
   - å¯è§†åŒ–ä¸€äº›NEGæ ·æœ¬çœ‹æ˜¯å¦åˆç†

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š ç›‘æ§æŒ‡æ ‡:
   è®­ç»ƒæ—¶ä¸è¦åªçœ‹æ€»ä½“Acc,è¦ç›‘æ§æ¯ä¸ªç±»åˆ«çš„Precision/Recall:
   - NEG Recall: NEGæ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ (ç›®æ ‡>60%)
   - NEG Precision: é¢„æµ‹ä¸ºNEGä¸­çœŸæ­£æ˜¯NEGçš„æ¯”ä¾‹ (ç›®æ ‡>50%)
   - å¦‚æœRecall=0%, è¯´æ˜æ¨¡å‹å®Œå…¨æ²¡å­¦ä¼šNEG
   - å¦‚æœRecall>0%ä½†Precisionå¾ˆä½, è¯´æ˜æ¨¡å‹ä¹±çŒœNEG

ğŸ¯ è®­ç»ƒæŠ€å·§:
   1. å‰5ä¸ªepochç”¨é«˜class weight [10.0, 1.0, 3.0]
   2. ç›‘æ§NEG recall,å¦‚æœ>40%,é™ä½weightåˆ° [7.0, 1.0, 2.5]
   3. æœ€åå‡ ä¸ªepoché™åˆ° [5.0, 1.0, 2.0]
   4. æ•´ä¸ªè¿‡ç¨‹ä½¿ç”¨ä½å­¦ä¹ ç‡ (1e-5) å’Œå°batch (16)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""")


if __name__ == "__main__":
    main()

